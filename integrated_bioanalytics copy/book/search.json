[{"path":"index.html","id":"welcome","chapter":"WELCOME","heading":"WELCOME","text":"Integrated Bioanalytics documents methods analyzing chemical sequence data R well basics scientific writing. maintained Lucas Busta members Busta lab. run analyses described book need run source script set R environment variety packages, custom functions, datasets. don’t R, see “installation” “Data Analysis R” table contents. Run source script pasting executing following R command line (RStudio recommended). Busta Lab (want access full features), define object bustalab = TRUE running source command. trouble running source script, please reach Lucas Busta : bust0037@d.umn.edu. source script:","code":"\nsource(\"https://thebustalab.github.io/phylochemistry/phylochemistry.R\")"},{"path":"overview.html","id":"overview","chapter":"overview","heading":"overview","text":"bioanalytical science, separate, identify, quantify matter - DNA, RNA, proteins, small molecules, even atoms. connect data world around us answer scientific questions, multiple chemical entities must separated, quantified, identified. ability collect analytical data expands, must ability effectively analyze data - whether 10 data points 10,000.book first covers data analysis R. first look tools hypothesis generation, including: () encoding variables visual representations data (ii) summarizing providing overviews large data set. turn evaluating hypothesese data looking statistical tests models. Finally, look communicate results clear effective way. techniques also allow us answer common quesions may data: “samples closely related?”, “analytes driving differences among samples?”, “samples fall definable clusters?”, “variables related?”, “distributions different?”.Let’s get started!","code":""},{"path":"installation.html","id":"installation","chapter":"installation","heading":"installation","text":"","code":""},{"path":"installation.html","id":"r","chapter":"installation","heading":"R","text":"R computing language use run analyses produce high quality plots. already R installed (need least version 4.1.1), can go straight installing RStudio. , follow steps install R:Go https://cran.r-project.org/Go https://cran.r-project.org/Click “Download R <operating system>” (see footnote), depending operating system select “Download R Linux”, “Download R (Mac) OS X”, “Download R Windows”.Click “Download R <operating system>” (see footnote), depending operating system select “Download R Linux”, “Download R (Mac) OS X”, “Download R Windows”.use <notation> quite bit. indicates place insert information, data, something similar corresponds particular situation. example means insert “operating system”, .e. Linux, (Mac) OS X, Windows.Mac: download .pkg file latest release. PC: click “install R first time”, click “Download R <version> Windows”.Mac: download .pkg file latest release. PC: click “install R first time”, click “Download R <version> Windows”.executable finishes downloading (Windows, file .exe extension; Mac, .dmg file .dmg inside .pkg file), open file administrator, follow installation instructions. R install without problems. can click OK windows pop-installation, choose “regular” installation (given choice).executable finishes downloading (Windows, file .exe extension; Mac, .dmg file .dmg inside .pkg file), open file administrator, follow installation instructions. R install without problems. can click OK windows pop-installation, choose “regular” installation (given choice).trouble installing R please google “Install R Mac” “Install R PC” follow one many video tutorials . tried still trouble, please contact .","code":""},{"path":"installation.html","id":"rstudio","chapter":"installation","heading":"RStudio","text":"install R, can install RStudio, essentially convenient way interacting R. people like RStudio prefer interact R directly. fine, many beginning R users find RStudio helpful, recommend . Follow steps install RStudio:Go https://rstudio.com/Go https://rstudio.com/Click “DOWNLOAD” top page.Click “DOWNLOAD” top page.Click “DOWNLOAD” button corresponds RStudio Desktop free Open Source License.Click “DOWNLOAD” button corresponds RStudio Desktop free Open Source License.page may automatically detect operating system using recommend version . , download file (.exe PC .dmg Mac). , scroll “Installers” section download file right . Open file administrator, follow installation instructions. RStudio install without problems. can click OK windows pop-installation, choose “regular” installation (given choice).page may automatically detect operating system using recommend version . , download file (.exe PC .dmg Mac). , scroll “Installers” section download file right . Open file administrator, follow installation instructions. RStudio install without problems. can click OK windows pop-installation, choose “regular” installation (given choice).trouble installing RStudio please google “Install RStudio Mac” “Install RStudio PC” following one many video tutorials . tried still trouble, please contact .","code":""},{"path":"installation.html","id":"verification","chapter":"installation","heading":"verification","text":"Open RStudio clicking appropriate file applications folder, wherever saved computer. Windows, sure run RStudio administrator. see several windows. One Code Editor, one R Console, one Workspace History, one Plots Files window.R Console window > . Type head(Indometh). display first six lines data set describing pharmacokinets indomethacin. one built datasets R - need additional files run test.Next, type plot(Indometh) R Console. plot indomethacin dataset basic way.commands (head(Indometh) plot(Indometh)) worked error messages installation, ready proceed.","code":"\nhead(Indometh)\n## Grouped Data: conc ~ time | Subject\n##   Subject time conc\n## 1       1 0.25 1.50\n## 2       1 0.50 0.94\n## 3       1 0.75 0.78\n## 4       1 1.00 0.48\n## 5       1 1.25 0.37\n## 6       1 2.00 0.19\nplot(Indometh)"},{"path":"installation.html","id":"tex","chapter":"installation","heading":"TeX","text":"class generate high quality reports suitable submission supervisors, academic journals, etc. , need typesetting engine TeX. ways . easiest way using following commands:Mac, may get error “able write path” something like . case probably need open terminal run following two commands:thenThen, Mac PC, need :options : Windows, download install MikTeX. OSX, can download install MacTeX.","code":"\ninstall.packages(c('tinytex', 'rmarkdown'))sudo chown -R \\`whoami\\`:admin /usr/local/bin~/Library/TinyTeX/bin/\\*/tlmgr path add\ntinytex::install_tinytex()"},{"path":"installation.html","id":"phylochemistry","chapter":"installation","heading":"phylochemistry","text":"addition tidyverse, variety packages need, well datasets custom functions. call loaded following.First, attempt load phylochemistry, Windows, sure ’ve opened RStudio administrator (right click, “run administrator”):first time try , likely say: “need install following packages proceeding […] okay phylochemistry installs ?” say “yes”.","code":"\nsource(\"https://thebustalab.github.io/phylochemistry/phylochemistry.R\")"},{"path":"data-visualization-i.html","id":"data-visualization-i","chapter":"data visualization I","heading":"data visualization I","text":"Visualization one fun parts working data. section, jump visualization quickly possible - just prerequisites. Please note data visualization whole field (just google “data visualization” see happens). Data visualization also rife “trendy” visuals, misleading visuals, visuals look cool don’t actually communicate much information. touch topics briefly, spend time practicing represent data intuitive interpretable ways. Let’s get started!","code":""},{"path":"data-visualization-i.html","id":"section","chapter":"data visualization I","heading":"","text":"","code":""},{"path":"data-visualization-i.html","id":"objects","chapter":"data visualization I","heading":"objects","text":"R, data stored objects. can think objects “files” inside R session. phylochemistry provides variety objects us work . Let’s look create object. , can use arrow: <- . arrow take something store inside object. example:Now ’ve got new object called new_object, inside number 1. look ’s inside object, can simply type name object console:Easy! Let’s look one objects comes class code base. dimensions “algae_data” data set?","code":"\nnew_object <- 1\nnew_object \n## [1] 1\nalgae_data\n## # A tibble: 180 × 5\n##    replicate algae_strain harvesting_regime chemical_species\n##        <dbl> <chr>        <chr>             <chr>           \n##  1         1 Tsv1         Heavy             FAs             \n##  2         1 Tsv1         Heavy             saturated_Fas   \n##  3         1 Tsv1         Heavy             omega_3_polyuns…\n##  4         1 Tsv1         Heavy             monounsaturated…\n##  5         1 Tsv1         Heavy             polyunsaturated…\n##  6         1 Tsv1         Heavy             omega_6_polyuns…\n##  7         1 Tsv1         Heavy             lysine          \n##  8         1 Tsv1         Heavy             methionine      \n##  9         1 Tsv1         Heavy             essential_Aas   \n## 10         1 Tsv1         Heavy             non_essential_A…\n## # ℹ 170 more rows\n## # ℹ 1 more variable: abundance <dbl>"},{"path":"data-visualization-i.html","id":"functions","chapter":"data visualization I","heading":"functions","text":"Excellent - ’ve got data. Now need manipulate . need functions:function command tells R perform action!function begins ends parentheses: this_is_a_function()stuff inside parentheses details want function perform action: run_this_analysis(on_this_data)Let’s illustrate example. algae_data pretty big object. next chapter visualization, nice smaller dataset object work . Let’s use another tidyverse command called filter filter algae_data object. need tell filter command filter using “logical predicates” (things like equal : ==, less : <, greater : >, greater---equal-: <=, etc.). Let’s filter algae_data rows chemical_species equal FAs (fatty acids) preserved. look like chemical_species == \"FAs\". go:Cool! Now ’s just showing us 18 rows chemical_species fatty acids (FAs). Let’s write new, smaller dataset new object. use <-, remember?variety ways filter:filter(<data>, <variable> < 18) ## less 18filter(<data>, <variable> <= 18) ## less equal 18filter(<data>, <variable> > 18) ## greater 18filter(<data>, <variable> >= 18) ## greater equal 18filter(<data>, <variable> == 18) ## equals 18filter(<data>, <variable> != 18) ## equal 18filter(<data>, <variable> == 18 | <variable> == 19) ## equal 18 19filter(<data>, <variable> %% c(18, 19, 20)) ## equal 18 19 20","code":"\nfilter(algae_data, chemical_species == \"FAs\")\n## # A tibble: 18 × 5\n##    replicate algae_strain harvesting_regime chemical_species\n##        <dbl> <chr>        <chr>             <chr>           \n##  1         1 Tsv1         Heavy             FAs             \n##  2         2 Tsv1         Heavy             FAs             \n##  3         3 Tsv1         Heavy             FAs             \n##  4         1 Tsv1         Light             FAs             \n##  5         2 Tsv1         Light             FAs             \n##  6         3 Tsv1         Light             FAs             \n##  7         1 Tsv2         Heavy             FAs             \n##  8         2 Tsv2         Heavy             FAs             \n##  9         3 Tsv2         Heavy             FAs             \n## 10         1 Tsv2         Light             FAs             \n## 11         2 Tsv2         Light             FAs             \n## 12         3 Tsv2         Light             FAs             \n## 13         1 Tsv11        Heavy             FAs             \n## 14         2 Tsv11        Heavy             FAs             \n## 15         3 Tsv11        Heavy             FAs             \n## 16         1 Tsv11        Light             FAs             \n## 17         2 Tsv11        Light             FAs             \n## 18         3 Tsv11        Light             FAs             \n## # ℹ 1 more variable: abundance <dbl>\nalgae_data_small <- filter(algae_data, chemical_species == \"FAs\")\nalgae_data_small\n## # A tibble: 18 × 5\n##    replicate algae_strain harvesting_regime chemical_species\n##        <dbl> <chr>        <chr>             <chr>           \n##  1         1 Tsv1         Heavy             FAs             \n##  2         2 Tsv1         Heavy             FAs             \n##  3         3 Tsv1         Heavy             FAs             \n##  4         1 Tsv1         Light             FAs             \n##  5         2 Tsv1         Light             FAs             \n##  6         3 Tsv1         Light             FAs             \n##  7         1 Tsv2         Heavy             FAs             \n##  8         2 Tsv2         Heavy             FAs             \n##  9         3 Tsv2         Heavy             FAs             \n## 10         1 Tsv2         Light             FAs             \n## 11         2 Tsv2         Light             FAs             \n## 12         3 Tsv2         Light             FAs             \n## 13         1 Tsv11        Heavy             FAs             \n## 14         2 Tsv11        Heavy             FAs             \n## 15         3 Tsv11        Heavy             FAs             \n## 16         1 Tsv11        Light             FAs             \n## 17         2 Tsv11        Light             FAs             \n## 18         3 Tsv11        Light             FAs             \n## # ℹ 1 more variable: abundance <dbl>"},{"path":"data-visualization-i.html","id":"ggplot-geoms","chapter":"data visualization I","heading":"ggplot & geoms","text":"Now nice, small table can use practice data visualization. visualization, ’re going use ggplot2 - powerful set commands plot generation.three steps setting ggplot:Define data want use.using ggplot function’s data argument. run line, just shows grey plot space. ? ’s ’ve done told ggplot () want make plot (ii) data used. haven’t explained represent features data using ink.Define variables map onto axes.called aesthetic mapping done aes() function. aes() placed inside ggplot command. Now run , get axes!Use geometric shapes represent variables data.Map variables onto geometric features shapes. define shape used, use geom_* command. options , example, geom_point(), geom_boxplot(), geom_violin(). functions added plot using + sign. can use new line keep code getting wide, just make sure + sign end fo top line. Let’s try :way mapped variables dataset plot axes, can map variables dataset geometric features shapes using represent data. , , use aes() map variables onto geometric features shapes:plot , points bit small, fix ? can modify features shapes adding additional arguments geom_*() functions. change size points created geom_point() function, means need add size = argument. IMPORTANT! Please note map feature shape variable data(color/harvesting regime, ) goes inside aes(). contrast, map feature shape constant, goes outside aes(). ’s example:One powerful aspect ggplot ability quickly change mappings see alternative plots effective bringing trends data. example, modify plot switching harvesting_regime mapped:** Important note: Inside aes() function, map aesthetics (features geom’s shape) variable. Outside aes() function, map aesthetics constants. can see two plots - first one, color inside aes() mapped variable called harvesting_regime, size outside aes() call set constant 5. second plot, situation reversed, size inside aes() function mapped variable harvesting_regime, color outside aes() call mapped constant “black”.can also stack geoms top one another using multiple + signs. also don’t assign mappings geom.can probably guess right now, lots mappings can done, lots different ways look data!","code":"\nggplot(data = algae_data_small)\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance))\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) +\n  geom_point()\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) + \n  geom_point(aes(color = harvesting_regime))\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) + \n  geom_point(aes(color = harvesting_regime), size = 5)\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) +\n  geom_point(aes(size = harvesting_regime), color = \"black\")\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) + \n  geom_violin() +\n  geom_point(aes(color = harvesting_regime), size = 5)\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) +\n  geom_violin(aes(fill = algae_strain)) +\n  geom_point(aes(color = harvesting_regime, size = replicate))\nggplot(data = algae_data_small, aes(x = algae_strain, y = abundance)) +\n  geom_boxplot()"},{"path":"data-visualization-i.html","id":"markdown","chapter":"data visualization I","heading":"markdown","text":"Now able filter data make plots, ready make reports show others data processing visualization . , use R Markdown. can open new markdown document RStudio clicking: File -> New File -> R Markdown. get template document compiles press “knit”.Customize document modifying title, add author: \"your_name\" header. Delete content header, compile . get page blank except title author name.can think markdown document stand-alone R Session. means need load class code base new markdown doument create. can adding “chunk” R code. looks like :can compilie document pdf. can also run R chunks right inside document create figures. notice things compile document:Headings: compile code, “# first analysis” creates header. can create headers various levels increasing number hashtags use front header. example, “## Part 1” create subheading, “### Part 1.1” create sub-subheading, .Headings: compile code, “# first analysis” creates header. can create headers various levels increasing number hashtags use front header. example, “## Part 1” create subheading, “### Part 1.1” create sub-subheading, .Plain text: Plain text R Markdown document creates plan text entry compiled document. can use explain analyses figures, etc.Plain text: Plain text R Markdown document creates plan text entry compiled document. can use explain analyses figures, etc.can modify output code chunk adding arguments header. Useful arguments fig.height, fig.width, fig.cap. Dr. Busta show class.can modify output code chunk adding arguments header. Useful arguments fig.height, fig.width, fig.cap. Dr. Busta show class.","code":""},{"path":"data-visualization-ii.html","id":"data-visualization-ii","chapter":"data visualization II","heading":"data visualization II","text":"","code":""},{"path":"data-visualization-ii.html","id":"section-1","chapter":"data visualization II","heading":"","text":"","code":""},{"path":"data-visualization-ii.html","id":"more-geoms","chapter":"data visualization II","heading":"more geoms","text":"’ve looked filter data map variables data geometric shapes make plots. Let’s look things. examples, ’re going use data set called solvents. examples, ’d like introduce two new geoms. first geom_smooth() used two continuous variables. particularly nice geom_point() stacked top .Also, please aware geom_tile(), nice situations two discrete variables one continuous variable. geom_tile() makes often referred heat maps. Note geom_tile() somewhat similar geom_point(shape = 21), fill color aesthetics control fill color border color, respectively.examples illustrate , degree, correspondence type data interested plotting (number discrete continuous variables) types geoms can effectively used represent data.","code":"\nggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + \n  geom_smooth() +\n  geom_point()\n## `geom_smooth()` using method = 'loess' and formula = 'y ~\n## x'\nggplot(\n  data = filter(algae_data, harvesting_regime == \"Heavy\"),\n  aes(x = algae_strain, y = chemical_species)\n) + \n  geom_tile(aes(fill = abundance), color = \"black\", size = 1)"},{"path":"data-visualization-ii.html","id":"facets","chapter":"data visualization II","heading":"facets","text":"alluded Exercises 1, possible map variables dataset geometric features shapes (.e. geoms). One common way facets. Faceting creates small multiples plot, shows different subset data based categorical variable choice. Let’s check ., can facet horizontal direction:can facet vertical direction:can time:Faceting great way describe variation plot without make geoms complicated. situations need generate lots lots facets, consider facet_wrap instead facet_grid:","code":"\nggplot(data = algae_data, aes(x = algae_strain, y = chemical_species)) + \n  geom_tile(aes(fill = abundance), color = \"black\") + \n  facet_grid(.~replicate)\nggplot(data = algae_data, aes(x = algae_strain, y = chemical_species)) + \n  geom_tile(aes(fill = abundance), color = \"black\") + \n  facet_grid(replicate~.)\nggplot(data = algae_data, aes(x = algae_strain, y = chemical_species)) + \n  geom_tile(aes(fill = abundance), color = \"black\") + \n  facet_grid(harvesting_regime~replicate)\nggplot(data = algae_data, aes(x = replicate, y = algae_strain)) + \n  geom_tile(aes(fill = abundance), color = \"black\") + \n  facet_wrap(chemical_species~.)"},{"path":"data-visualization-ii.html","id":"scales","chapter":"data visualization II","heading":"scales","text":"Every time define aesthetic mapping (e.g. aes(x = algae_strain)), defining new scale added plot. can control scales using scale_* family commands. Consider faceting example . , use geom_tile(aes(fill = abundance)) map abundance variable fill aesthetic tiles. creates scale called fill can adjust using scale_fill_*. case, fill mapped continuous variable fill scale color gradient. Therefore, scale_fill_gradient() command need change . Remember always type ?scale_fill_ console help find relevant help topics provide detail. Another option google: “modify color scale ggplot geom_tile”, undoubtedly turn wealth help.One particularly useful type scale color scales provided RColorBrewer:","code":"\nggplot(data = algae_data, aes(x = algae_strain, y = chemical_species)) + \n  geom_tile(aes(fill = abundance), color = \"black\") + \n  facet_grid(harvesting_regime~replicate) +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  theme_classic()\ndisplay.brewer.all()\nggplot(mtcars) +\n  geom_point(\n    aes(x = mpg, y = factor(cyl), fill = factor(carb)), \n    shape = 21, size = 6\n  ) +\n  scale_fill_brewer(palette = \"Set1\")"},{"path":"data-visualization-ii.html","id":"themes","chapter":"data visualization II","heading":"themes","text":"far ’ve just looked control means data represented plot. also components plot , strictly speaking, data per se, rather non-data ink. controlled using theme() family commands. two ways go .ggplot comes handful built “complete themes”. change appearance plots respect non-data ink. Compare following plots:can also change individual components themes. can bit tricky, ’s explained run ?theme(). Hare example (google provide many, many ).Last, example combining scale_* theme_* previous commands really get plot looking sharp.\nFigure 2.1: Vapor pressure function boiling point. scatter plot trendline showing vapor pressure thirty-two solvents (y-axis) function boiling points (x-axis). point represents boiling point vapor pressure one solvent. Data ‘solvents’ dataset used UMD CHEM5725.\ncases, following diagram illustrates useful way think ggplot() / geom_*() / scale_*() / theme_*() situation. shows use things together achieve sharp-looking plot:","code":"\nggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + \n  geom_smooth() +\n  geom_point() +\n  theme_classic()\n## `geom_smooth()` using method = 'loess' and formula = 'y ~\n## x'\nggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + \n  geom_smooth() +\n  geom_point() +\n  theme_dark()\n## `geom_smooth()` using method = 'loess' and formula = 'y ~\n## x'\nggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + \n  geom_smooth() +\n  geom_point() +\n  theme_void()\n## `geom_smooth()` using method = 'loess' and formula = 'y ~\n## x'\nggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + \n  geom_smooth() +\n  geom_point() +\n  theme(\n    text = element_text(size = 20, color = \"black\")\n  )\n## `geom_smooth()` using method = 'loess' and formula = 'y ~\n## x'\nggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + \n  geom_smooth(color = \"#4daf4a\") +\n  scale_x_continuous(\n    name = \"Boiling Point\", breaks = seq(0,200,25), limits = c(30,210)\n  ) +\n  scale_y_continuous(\n    name = \"Vapor Pressure\", breaks = seq(0,600,50)\n  ) +\n  geom_point(color = \"#377eb8\", size = 4, alpha = 0.6) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(color = \"black\"),\n    text = element_text(size = 16, color = \"black\")\n  )\n## `geom_smooth()` using method = 'loess' and formula = 'y ~\n## x'"},{"path":"data-visualization-ii.html","id":"subplots","chapter":"data visualization II","heading":"subplots","text":"can make subplots using plot_grid() function cowplot package, comes source() command. Let’s see:","code":"\nplot1 <-  ggplot(\n            filter(alaska_lake_data, element_type == \"free\")\n          ) +\n          geom_violin(aes(x = park, y = mg_per_L)) + theme_classic() +\n          ggtitle(\"A\")\n\nplot2 <-  ggplot(\n            filter(alaska_lake_data, element_type == \"bound\")\n          ) +\n          geom_violin(aes(x = park, y = mg_per_L)) + theme_classic() +\n          ggtitle(\"B\")\n\nplot3 <-  ggplot(\n            filter(alaska_lake_data, element == \"C\")\n          ) +\n          geom_violin(aes(x = park, y = mg_per_L)) + theme_classic() +\n          coord_flip() + ggtitle(\"C\")\n\nplot_grid(plot_grid(plot1, plot2), plot3, ncol = 1)"},{"path":"data-visualization-ii.html","id":"section-2","chapter":"data visualization II","heading":"","text":"","code":""},{"path":"data-visualization-ii.html","id":"further-reading","chapter":"data visualization II","heading":"further reading","text":"handy cheat sheet can help identify right geom situation. Please keep cheat sheet mind future plotting needs…additional explanations ggplot2: ggplot2-book.Check incredible geoms easy access using R ggplot2: R Graph Gallery. Use make figures attractive easy interpret!challenge, try implementing awesome color scales: Famous R Color Palettes. Note optimized colorblind individuals optimized continuous hue gradients, etc.list data visualization sins: Friends Don’t Let Friends. interesting things !information data visualization graphics theory, check works Edward Tufte: Edward Tufte. digital text covers similar topics : [Look Data] (https://socviz.co/lookatdata.html).examples award winning data visualization: Information Beautiful Awards Data Vis Inspiration.Additional color palettes: MetBrewer Paletteer.","code":""},{"path":"data-visualization-iii.html","id":"data-visualization-iii","chapter":"data visualization III","heading":"data visualization III","text":"","code":""},{"path":"data-visualization-iii.html","id":"section-3","chapter":"data visualization III","heading":"","text":"","code":""},{"path":"data-visualization-iii.html","id":"advanced-plots","chapter":"data visualization III","heading":"advanced plots","text":"","code":""},{"path":"data-visualization-iii.html","id":"d-scatter-plots","chapter":"data visualization III","heading":"3D scatter plots","text":"phylochemistry contains function help make somewhat decent 3D scatter plots. Let’s look example (see ). , use function points3D. Se give data argument gives vectors data x, y, z axes, along vector uniquely identifies observation. also tell angle z axis want, integer ticks rounded, tick intervals. function returns data can pass ggplot make 3D plot.output points3D contains grid, axes, ticks, plotted using geom_segment. also contains points plotted geom_point, point segments plotted geom_segement. can take output points3D join original data, occurr according sample_unique_ID column. , can also plot point metadata:","code":"\npivot_wider(hawaii_aquifers, names_from = \"analyte\", values_from = \"abundance\") %>%\n  mutate(sample_unique_ID = paste0(aquifer_code, \"_\", well_name)) -> aquifers\n\noutput <- points3D(\n  data = data.frame(\n    x = aquifers$SiO2,\n    y = aquifers$Cl,\n    z = aquifers$Mg,\n    sample_unique_ID = aquifers$sample_unique_ID\n  ),\n  angle = pi/2.4,\n  tick_round = 10,\n  x_tick_interval = 10,\n  y_tick_interval = 20,\n  z_tick_interval = 20\n)\n\nstr(output)\n## List of 6\n##  $ grid          :'data.frame':  14 obs. of  4 variables:\n##   ..$ y   : num [1:14] 0 0 0 0 0 ...\n##   ..$ yend: num [1:14] 96.6 96.6 96.6 96.6 96.6 ...\n##   ..$ x   : num [1:14] 10 20 30 40 50 ...\n##   ..$ xend: num [1:14] 35.9 45.9 55.9 65.9 75.9 ...\n##  $ ticks         :'data.frame':  37 obs. of  4 variables:\n##   ..$ y   : num [1:37] 0 0 0 0 0 0 0 0 0 0 ...\n##   ..$ yend: num [1:37] 1.93 1.93 1.93 1.93 1.93 ...\n##   ..$ x   : num [1:37] 10 20 30 40 50 60 70 80 10 20 ...\n##   ..$ xend: num [1:37] 10.5 20.5 30.5 40.5 50.5 ...\n##  $ labels        :'data.frame':  29 obs. of  3 variables:\n##   ..$ y    : num [1:29] -11.2 -11.2 -11.2 -11.2 -11.2 -11.2 -11.2 -11.2 0 20 ...\n##   ..$ x    : num [1:29] 7.2 17.2 27.2 37.2 47.2 57.2 67.2 77.2 4.4 4.4 ...\n##   ..$ label: num [1:29] 10 20 30 40 50 60 70 80 0 20 ...\n##  $ axes          :'data.frame':  3 obs. of  4 variables:\n##   ..$ x   : num [1:3] 10 10 80\n##   ..$ xend: num [1:3] 80 10 106\n##   ..$ y   : num [1:3] 0 0 0\n##   ..$ yend: num [1:3] 0 280 96.6\n##  $ point_segments:'data.frame':  106 obs. of  4 variables:\n##   ..$ x   : num [1:106] 13 33.1 39.4 53.1 22.5 ...\n##   ..$ xend: num [1:106] 13 33.1 39.4 53.1 22.5 ...\n##   ..$ y   : num [1:106] 27.3 81.6 82.6 109.5 19.5 ...\n##   ..$ yend: num [1:106] 7.34 11.59 12.56 15.45 5.51 ...\n##  $ points        :'data.frame':  106 obs. of  3 variables:\n##   ..$ x               : num [1:106] 13 33.1 39.4 53.1 22.5 ...\n##   ..$ y               : num [1:106] 27.3 81.6 82.6 109.5 19.5 ...\n##   ..$ sample_unique_ID: chr [1:106] \"aquifer_1_Alewa_Heights_Spring\" \"aquifer_1_Beretania_High_Service\" \"aquifer_1_Beretania_Low_Service\" \"aquifer_1_Kuliouou_Well\" ...\noutput$points <- left_join(output$points, aquifers)\n## Joining with `by = join_by(sample_unique_ID)`\n  \nggplot() +\n  geom_segment(\n    data = output$grid, aes(x = x, xend = xend, y = y, yend = yend),\n    color = \"grey80\"\n  ) +\n  geom_segment(data = output$axes, aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_segment(data = output$ticks, aes(x = x, xend = xend, y = y, yend = yend)) +\n  geom_text(\n    data = output$labels, aes(x = x, y = y, label = label),\n    hjust = 0.5\n  ) +\n  geom_segment(\n    data = output$point_segments,\n    aes(x = x, xend = xend, y = y, yend = yend),\n    linetype = \"dotted\", color = \"black\"\n  ) +\n  geom_point(\n    data = output$points, aes(x = x, y = y, fill = aquifer_code),\n    size = 3, shape = 21\n  ) +\n  theme_void() +\n  scale_fill_manual(values = discrete_palette)"},{"path":"data-visualization-iii.html","id":"marginal-summaries","chapter":"data visualization III","heading":"marginal summaries","text":"","code":"\ni2 <- iris %>%\n  mutate(Species2 = rep(c(\"A\",\"B\"), 75))\np <- ggplot(i2, aes(Sepal.Width, Sepal.Length, color = Species)) +\n  geom_point()\n\np + geom_xsidedensity(aes(y=stat(density), xfill = Species), position = \"stack\")+\n  geom_ysidedensity(aes(x=stat(density), yfill = Species2), position = \"stack\") +\n  theme_bw() + \n  facet_grid(Species~Species2, space = \"free\", scales = \"free\") +\n  labs(title = \"FacetGrid\", subtitle = \"Collapsing All Side Panels\") +\n  ggside(collapse = \"all\") +\n  scale_xfill_manual(values = c(\"darkred\",\"darkgreen\",\"darkblue\")) +\n  scale_yfill_manual(values = c(\"black\",\"gold\"))"},{"path":"data-visualization-iii.html","id":"representing-distributions","chapter":"data visualization III","heading":"representing distributions","text":"can also combine geoms create detailed representations distributions:","code":"\nmpg %>% filter(cyl %in% c(4,6,8)) %>%\n  ggplot(aes(x = factor(cyl), y = hwy, fill = factor(cyl))) +\n  ggdist::stat_halfeye(\n    adjust = 0.5, justification = -0.2, .width = 0, point_colour = NA\n  ) +\n  geom_boxplot(width = 0.12, outlier.color = NA, alpha = 0.5) +\n  ggdist::stat_dots(side = \"left\", justification = 1.1, binwidth = .25)"},{"path":"data-visualization-iii.html","id":"venn-digrams","chapter":"data visualization III","heading":"venn digrams","text":"","code":"\ndf <- data.frame(\n  plant1 = sample(c(TRUE, FALSE), 24, replace = TRUE),\n  plant2 = sample(c(TRUE, FALSE), 24, replace = TRUE),\n  plant3 = sample(c(TRUE, FALSE), 24, replace = TRUE),\n  attribute_name = sample(letters, 24, replace = FALSE)\n)\n\nvennAnalysis(df[,1:3]) %>%\n  ggplot() +\n    geom_circle(\n      aes(x0 = x, y0 = y, r = r, fill = category),\n      alpha = 0.4\n    ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_void()"},{"path":"data-visualization-iii.html","id":"ternary-plots","chapter":"data visualization III","heading":"ternary plots","text":"","code":"\nlibrary(ggplot2)\nlibrary(ggtern)\nalaska_lake_data %>%\n  pivot_wider(names_from = \"element\", values_from = \"mg_per_L\") %>%\n  ggtern(aes(\n    x = Ca,\n    y = S,\n    z = Na,\n    color = park,\n    size = pH\n    )) +\n  geom_point() "},{"path":"data-visualization-iii.html","id":"map-data","chapter":"data visualization III","heading":"map data","text":"","code":""},{"path":"data-visualization-iii.html","id":"plotting-boundaries","chapter":"data visualization III","heading":"plotting boundaries","text":"simple way plot maps ggplot. map data comes ggplot2! Let’s look. See data sets included. Options included ggplot : world, world2, usa, state (US), county (US), nz, italy, france. geom_polygon() useful plotting , (least ) seems intuitive geom_map().Cool! can see lat, lon, group, order, region, subregion included. makes plotting easy. Note coord_map() can help preserve aspect ratios:Note can use coord_map() pretty cool things!can use filtering produce maps specific regions.","code":"\nhead(map_data(\"world\"))\n##        long      lat group order region subregion\n## 1 -69.89912 12.45200     1     1  Aruba      <NA>\n## 2 -69.89571 12.42300     1     2  Aruba      <NA>\n## 3 -69.94219 12.43853     1     3  Aruba      <NA>\n## 4 -70.00415 12.50049     1     4  Aruba      <NA>\n## 5 -70.06612 12.54697     1     5  Aruba      <NA>\n## 6 -70.05088 12.59707     1     6  Aruba      <NA>\nhead(map_data(\"state\"))\n##        long      lat group order  region subregion\n## 1 -87.46201 30.38968     1     1 alabama      <NA>\n## 2 -87.48493 30.37249     1     2 alabama      <NA>\n## 3 -87.52503 30.37249     1     3 alabama      <NA>\n## 4 -87.53076 30.33239     1     4 alabama      <NA>\n## 5 -87.57087 30.32665     1     5 alabama      <NA>\n## 6 -87.58806 30.32665     1     6 alabama      <NA>\nhead(map_data(\"county\"))\n##        long      lat group order  region subregion\n## 1 -86.50517 32.34920     1     1 alabama   autauga\n## 2 -86.53382 32.35493     1     2 alabama   autauga\n## 3 -86.54527 32.36639     1     3 alabama   autauga\n## 4 -86.55673 32.37785     1     4 alabama   autauga\n## 5 -86.57966 32.38357     1     5 alabama   autauga\n## 6 -86.59111 32.37785     1     6 alabama   autauga\nhead(map_data(\"france\"))\n##       long      lat group order region subregion\n## 1 2.557093 51.09752     1     1   Nord      <NA>\n## 2 2.579995 51.00298     1     2   Nord      <NA>\n## 3 2.609101 50.98545     1     3   Nord      <NA>\n## 4 2.630782 50.95073     1     4   Nord      <NA>\n## 5 2.625894 50.94116     1     5   Nord      <NA>\n## 6 2.597699 50.91967     1     6   Nord      <NA>\nggplot(map_data(\"world\")) +\n  geom_point(aes(x = long, y = lat, color = group), size = 0.5) +\n  theme_void() +\n  coord_map()\nggplot(map_data(\"world\")) +\n  geom_point(aes(x = long, y = lat, color = group), size = 0.5) +\n  theme_void() +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45)\nggplot() +\n  geom_polygon(\n    data = filter(map_data(\"county\"), region == \"minnesota\"),\n    aes(x = long, y = lat, group = subregion, fill = subregion),\n    color = \"black\"\n  ) +\n  theme_void() +\n  coord_map()"},{"path":"data-visualization-iii.html","id":"maps-with-plots","chapter":"data visualization III","heading":"maps with plots","text":"Please note Great Lakes map_data()!can clean map making different groups geom_path() whenever two consecutive points far apart:Now add data. something simple like plot total abundances size point:something sophisticated like add pie charts point:can also access high resolution shoreline dataset Lake Superior directly source() command lake_superior_shoreline:","code":"\nfilter(map_data(\"lakes\"), region == \"Great Lakes\", subregion == \"Superior\") %>%\n    ggplot() +\n      geom_path(aes(x = long, y = lat)) +\n      coord_map() +\n      theme_minimal()\n# Step 1: Filter and prepare your data for Lake Superior (though yes, that includes Michigan and Huron)\nlake_superior <- map_data(\"lakes\") %>%\n  filter(region == \"Great Lakes\", subregion == \"Superior\") %>%\n  arrange(order)\n\n# Step 2: Calculate distances between consecutive points\nlake_superior <- lake_superior %>%\n  mutate(lag_long = lag(long),\n         lag_lat = lag(lat),\n         dist_to_prev = geosphere::distHaversine(cbind(long, lat), cbind(lag_long, lag_lat))) # distHaversine calculates distances\n\n# Step 3: Define a threshold (e.g., 50 km) and create the \"distance_group\"\nthreshold <- 50000  # 50 km\nlake_superior <- lake_superior %>%\n  mutate(distance_group = cumsum(ifelse(dist_to_prev > threshold | is.na(dist_to_prev), 1, 0)))\n\n# Step 4: Plot the map with `distance_group`\nggplot(lake_superior, aes(x = long, y = lat, group = distance_group)) +\n  geom_path() +\n  coord_map() +\n  theme_minimal()\nlake_superior_PFAS <- readMonolist(\"/Users/bust0037/Documents/Websites/pfas_data_private.csv\")\nlake_superior_PFAS %>%\n  group_by(site, lon, lat) %>%\n  summarize(total = sum(abundance)) -> lake_superior_PFAS_summarized\n\nggplot() +\n  geom_path(\n    data = filter(lake_superior, lat > 46, long < -84),\n    aes(x = long, y = lat, group = distance_group)\n  ) +\n  geom_point(\n    data =  lake_superior_PFAS_summarized,\n    aes(x = lon, y = lat, size = total),\n    color = \"black\"\n  ) +\n  coord_map() +\n  theme_cowplot()\nlake_superior_PFAS <- readMonolist(\"/Users/bust0037/Documents/Websites/pfas_data_private.csv\")\n\ngrouped_by_site <- filter(lake_superior_PFAS, component == \"PFBA\")\nsite_less_than_90lon <- filter(grouped_by_site, lon <= -90)\nsite_more_than_90lon <- filter(grouped_by_site, lon >= -90)\n\nunique_sites <- unique(lake_superior_PFAS$site)\ndataframe_of_pies <- list()\nfor (i in 1:length(unique_sites)) { #i=1\n  this_site <- filter(lake_superior_PFAS, site == unique_sites[i])\n  this_site %>%\n    ggplot()+\n    geom_col(aes(x = 1, y = abundance, fill = class_name), color = \"black\") +\n    coord_polar(theta = \"y\") +\n    theme_void() +\n    scale_fill_brewer(palette = \"Set1\", guide = \"none\") -> this_sites_pie\n  dataframe_of_pies[[i]] <- tibble(x = this_site$lon[1], y = this_site$lat[1], plot = list(this_sites_pie))\n}\ndataframe_of_pies <- do.call(rbind, dataframe_of_pies)\n\nggplot() +\n  geom_path(\n    data = filter(lake_superior, lat > 46, long < -84),\n    aes(x = long, y = lat, group = distance_group)\n  ) +\n  geom_point(\n    data =  lake_superior_PFAS,\n    aes(x = lon, y = lat),\n    color = \"black\"\n  ) +\n  geom_plot(\n    data = dataframe_of_pies, aes(x = x, y = y, label = plot),\n    vp.width = 1/20, hjust = 0.5, vjust = 0.5, alpha = 0.5\n  ) +\n  geom_label_repel(data = site_less_than_90lon, aes(x = lon, y = lat, label = site), size = 2.5,min.segment.length = 0.01) +\n  geom_label(data = site_more_than_90lon, aes(x = lon, y = lat, label = site), size = 2.5) +\n  coord_map() +\n  theme_cowplot()\nshore <- readMonolist(\"/Users/bust0037/Documents/Websites/thebustalab.github.io/phylochemistry/sample_data/lake_superior_shoreline.csv\")\n\nwide_view <- ggplot(shore) +\n    geom_point(aes(y = lat, x = lon), size = 0.01) +\n    coord_map() +\n    theme_minimal()\n\nzoom_view <- ggplot(filter(shore, lat < 47.2, lat > 46.6, lon < -90)) +\n    geom_point(aes(y = lat, x = lon), size = 0.01) +\n    coord_map() +\n    theme_minimal()\n    \nplot_grid(wide_view, zoom_view, nrow = 1, rel_widths = c(1,2))"},{"path":"data-visualization-iii.html","id":"section-4","chapter":"data visualization III","heading":"","text":"","code":""},{"path":"data-visualization-iii.html","id":"further-reading-1","chapter":"data visualization III","heading":"further reading","text":"plotting maps R: datavizplyrFor advanced map plotting: R SpatialFor ternary plots: ggtern","code":""},{"path":"wrangling-and-summaries.html","id":"wrangling-and-summaries","chapter":"wrangling and summaries","heading":"wrangling and summaries","text":"Data wrangling refers process organizing, cleaning , making “raw” data set ready downstream analysis. key piece data analysis process. look different aspects wrangling, including data import, subsetting, pivoting, summarizing data.","code":""},{"path":"wrangling-and-summaries.html","id":"section-5","chapter":"wrangling and summaries","heading":"","text":"","code":""},{"path":"wrangling-and-summaries.html","id":"data-import","chapter":"wrangling and summaries","heading":"data import","text":"analyze data stored computer can indeed import RStudio.easiest way use interactive command readCSV(), function comes phylochemistry source command. run readCSV() console, navigate data hard drive.Another option read data path. , need know “path” data file. essentially street address data computer’s hard drive. Paths look different Mac PC.Mac: /Users/lucasbusta/Documents/sample_data_set.csv (note forward slashes!)PC: C:\\\\Computer\\\\Documents\\\\sample_data_set.csv (note double backward slashes!)can quickly find paths files via following:Mac: Locate file Finder. Right-click file, hold Option key, click “Copy  Pathname”PC: Locate file Windows Explorer. Hold Shift key right-click file. Click “Copy Path”paths, can read data using read_csv command. ’ll run read_csv(\"<path_to_your_data>\"). Note use QUOTES \"\"! necessary. Also make sure path uses appropriate direction slashes operating system.","code":""},{"path":"wrangling-and-summaries.html","id":"subsetting","chapter":"wrangling and summaries","heading":"subsetting","text":"far, always passing whole data sets ggplot plotting. However, suppose wanted get just certain portions dataset, say, specific columns, specific rows? ways :","code":"\n# To look at a single column (the third column)\nhead(alaska_lake_data[,3])\n## # A tibble: 6 × 1\n##   water_temp\n##        <dbl>\n## 1       6.46\n## 2       6.46\n## 3       6.46\n## 4       6.46\n## 5       6.46\n## 6       6.46\n\n# To look at select columns:\nhead(alaska_lake_data[,2:5])\n## # A tibble: 6 × 4\n##   park  water_temp    pH element\n##   <chr>      <dbl> <dbl> <chr>  \n## 1 BELA        6.46  7.69 C      \n## 2 BELA        6.46  7.69 N      \n## 3 BELA        6.46  7.69 P      \n## 4 BELA        6.46  7.69 Cl     \n## 5 BELA        6.46  7.69 S      \n## 6 BELA        6.46  7.69 F\n\n# To look at a single row (the second row)\nhead(alaska_lake_data[2,])\n## # A tibble: 1 × 7\n##   lake  park  water_temp    pH element mg_per_L element_type\n##   <chr> <chr>      <dbl> <dbl> <chr>      <dbl> <chr>       \n## 1 Devi… BELA        6.46  7.69 N          0.028 bound\n\n# To look at select rows:\nhead(alaska_lake_data[2:5,])\n## # A tibble: 4 × 7\n##   lake  park  water_temp    pH element mg_per_L element_type\n##   <chr> <chr>      <dbl> <dbl> <chr>      <dbl> <chr>       \n## 1 Devi… BELA        6.46  7.69 N          0.028 bound       \n## 2 Devi… BELA        6.46  7.69 P          0     bound       \n## 3 Devi… BELA        6.46  7.69 Cl        10.4   free        \n## 4 Devi… BELA        6.46  7.69 S          0.62  free\n\n# To look at just a single column, by name\nhead(alaska_lake_data$pH)\n## [1] 7.69 7.69 7.69 7.69 7.69 7.69\n\n# To look at select columns by name\nhead(select(alaska_lake_data, park, water_temp))\n## # A tibble: 6 × 2\n##   park  water_temp\n##   <chr>      <dbl>\n## 1 BELA        6.46\n## 2 BELA        6.46\n## 3 BELA        6.46\n## 4 BELA        6.46\n## 5 BELA        6.46\n## 6 BELA        6.46"},{"path":"wrangling-and-summaries.html","id":"wide-and-long-data","chapter":"wrangling and summaries","heading":"wide and long data","text":"make data tables hand, ’s often easy make wide-style table like following. , abundances 7 different fatty acids 10 different species tabulated. fatty acid gets row, species, column.format nice filling hand (lab notebook similar), groove ggplot tidyverse functions well. need convert long-style table. done using pivot_longer(). can think function transforming data’s column names (column names) data matrix’s values (case, measurements) variables (.e. columns). can fatty acid dataset using command . , specify data want transform (data = fadb_sample), need tell columns want transform (cols = 2:11), want new variable contains column names called (names_to = \"plant_species\") want new variable contains matrix values called (values_to = \"relative_abundance\"). together now:Brilliant! Now long-style table can used ggplot.","code":"\nhead(fadb_sample)\n## # A tibble: 6 × 11\n##   fatty_acid      Agonandra_brasiliensis Agonandra_silvatica\n##   <chr>                            <dbl>               <dbl>\n## 1 Hexadecanoic a…                    3.4                 1  \n## 2 Octadecanoic a…                    6.2                 0.1\n## 3 Eicosanoic acid                    4.7                 3.5\n## 4 Docosanoic acid                   77.4                 0.4\n## 5 Tetracosanoic …                    1.4                 1  \n## 6 Hexacosanoic a…                    1.9                12.6\n## # ℹ 8 more variables: Agonandra_excelsa <dbl>,\n## #   Heisteria_silvianii <dbl>, Malania_oleifera <dbl>,\n## #   Ximenia_americana <dbl>, Ongokea_gore <dbl>,\n## #   Comandra_pallida <dbl>, Buckleya_distichophylla <dbl>,\n## #   Nuytsia_floribunda <dbl>\npivot_longer(data = fadb_sample, cols = 2:11, names_to = \"plant_species\", values_to = \"relative_abundance\")\n## # A tibble: 70 × 3\n##    fatty_acid        plant_species        relative_abundance\n##    <chr>             <chr>                             <dbl>\n##  1 Hexadecanoic acid Agonandra_brasilien…                3.4\n##  2 Hexadecanoic acid Agonandra_silvatica                 1  \n##  3 Hexadecanoic acid Agonandra_excelsa                   1.2\n##  4 Hexadecanoic acid Heisteria_silvianii                 2.9\n##  5 Hexadecanoic acid Malania_oleifera                    0.7\n##  6 Hexadecanoic acid Ximenia_americana                   3.3\n##  7 Hexadecanoic acid Ongokea_gore                        1  \n##  8 Hexadecanoic acid Comandra_pallida                    2.3\n##  9 Hexadecanoic acid Buckleya_distichoph…                1.6\n## 10 Hexadecanoic acid Nuytsia_floribunda                  3.8\n## # ℹ 60 more rows"},{"path":"wrangling-and-summaries.html","id":"the-pipe","chapter":"wrangling and summaries","heading":"the pipe (%>%)","text":"seen create new objects using <-, filtering plotting data using, example:However, analyses get complex, code can get long hard read. ’re going use pipe %>% help us . Check :Neat! Another way think pipe:pipe become important analyses become sophisticated, happens quickly start working summary statistics, shall now see…","code":"\nggplot(filter(alaska_lake_data, park == \"BELA\"), aes(x = pH, y = lake)) + geom_col()\nalaska_lake_data %>%\n  filter(park == \"BELA\") %>%\n  ggplot(aes(x = pH, y = lake)) + geom_col()"},{"path":"wrangling-and-summaries.html","id":"summary-statistics","chapter":"wrangling and summaries","heading":"summary statistics","text":"far, plotting raw data. well good, always suitable. Often scientific questions answered looking raw data alone, sometimes much raw data plot. , need summary statistics - things like averages, standard deviations, . metrics can computed Excel, programming can time consuming, especially group statistics. Consider example , uses ny_trees dataset. NY Trees dataset contains information nearly half million trees New York City (considerable filtering simplification):300,000 observations 14 variables! ’s 4.2M data points! Now, average standard deviation height diameter tree species within NY borough? values change trees parks versus sidewalk pits?? don’t even know one begin approach questions using traditional spreadsheets. , answer questions ease using two new commands: group_by() summarize(). Let’s get .Say want know (course, visualize) mean standard deviation heights tree species NYC. can see data first columns NY trees dataset , calculate statistics? R, mean can computed mean() standard deviation can calculated sd(). use function summarize() calculate summary statistics. , can calculate average standard deviation trees data set follows:Great! species? need subdivide data species, compute mean standard deviation, recombine results new table. First, use group_by(). Note ny_trees, species indicated column called spc_latin. data grouped, can use summarize() compute statistics.Bam. Mean height tree species. can also count number observations using n():Cool! summarize() powerful though, can many summary statistics :Now can use data plotting. , use new geom, geom_pointrange, takes x y aesthetics, usual, also requires two additional y-ish aesthetics ymin ymax (xmin xmax want vary along x). Also, note aesthetic mappings xmin xmax, can use mathematical expression: mean-stdev mean+stdev, respectivey. case, mean_height - stdev_height mean_height + stdev_height. Let’s see action:Cool! Just like , ’ve found (visualized) average standard deviation tree heights, species, NYC. doesn’t stop . can use group_by() summarize() multiple variables (.e. groups). can examine properties tree species NYC borough. Let’s check :Now summary statistics tree species within borough. different previous plot now additional variable (boroname) summarized dataset. additional variable needs encoded plot. Let’s map boroname x facet tree species, used x. ’ll also manually modify theme element strip.text.y get species names readable position.Excellent! really want go something pretty:Now getting somewhere. looks like really big maple trees (Acer) Queens.","code":"\nhead(ny_trees)\n## # A tibble: 6 × 14\n##   tree_height tree_diameter address        tree_loc pit_type\n##         <dbl>         <dbl> <chr>          <chr>    <chr>   \n## 1        21.1             6 1139 57 STREET Front    Sidewal…\n## 2        59.0             6 2220 BERGEN A… Across   Sidewal…\n## 3        92.4            13 2254 BERGEN A… Across   Sidewal…\n## 4        50.2            15 2332 BERGEN A… Across   Sidewal…\n## 5        95.0            21 2361 EAST   7… Front    Sidewal…\n## 6        67.5            19 2409 EAST   7… Front    Continu…\n## # ℹ 9 more variables: soil_lvl <chr>, status <chr>,\n## #   spc_latin <chr>, spc_common <chr>, trunk_dmg <chr>,\n## #   zipcode <dbl>, boroname <chr>, latitude <dbl>,\n## #   longitude <dbl>\nny_trees %>%\n  summarize(mean_height = mean(tree_height))\n## # A tibble: 1 × 1\n##   mean_height\n##         <dbl>\n## 1        72.6\n\nny_trees %>%\n  summarize(stdev_height = sd(tree_height))\n## # A tibble: 1 × 1\n##   stdev_height\n##          <dbl>\n## 1         28.7\nny_trees %>%\n  group_by(spc_latin) %>%\n  summarize(mean_height = mean(tree_height))\n## # A tibble: 12 × 2\n##    spc_latin              mean_height\n##    <chr>                        <dbl>\n##  1 ACER PLATANOIDES              82.6\n##  2 ACER RUBRUM                  106. \n##  3 ACER SACCHARINUM              65.6\n##  4 FRAXINUS PENNSYLVANICA        60.6\n##  5 GINKGO BILOBA                 90.4\n##  6 GLEDITSIA TRIACANTHOS         53.0\n##  7 PLATANUS ACERIFOLIA           82.0\n##  8 PYRUS CALLERYANA              21.0\n##  9 QUERCUS PALUSTRIS             65.5\n## 10 QUERCUS RUBRA                111. \n## 11 TILIA CORDATA                 98.8\n## 12 ZELKOVA SERRATA              101.\nny_trees %>%\n  group_by(spc_latin) %>%\n  summarize(number_of_individuals = n())\n## # A tibble: 12 × 2\n##    spc_latin              number_of_individuals\n##    <chr>                                  <int>\n##  1 ACER PLATANOIDES                       67260\n##  2 ACER RUBRUM                            11506\n##  3 ACER SACCHARINUM                       13161\n##  4 FRAXINUS PENNSYLVANICA                 16987\n##  5 GINKGO BILOBA                          15672\n##  6 GLEDITSIA TRIACANTHOS                  48707\n##  7 PLATANUS ACERIFOLIA                    80075\n##  8 PYRUS CALLERYANA                       39125\n##  9 QUERCUS PALUSTRIS                      37058\n## 10 QUERCUS RUBRA                          10020\n## 11 TILIA CORDATA                          25970\n## 12 ZELKOVA SERRATA                        13221\nny_trees %>%\n  group_by(spc_latin) %>%\n  summarize(\n    mean_height = mean(tree_height),\n    stdev_height = sd(tree_height)\n  ) -> ny_trees_by_spc_summ\nny_trees_by_spc_summ\n## # A tibble: 12 × 3\n##    spc_latin              mean_height stdev_height\n##    <chr>                        <dbl>        <dbl>\n##  1 ACER PLATANOIDES              82.6        17.6 \n##  2 ACER RUBRUM                  106.         15.7 \n##  3 ACER SACCHARINUM              65.6        16.6 \n##  4 FRAXINUS PENNSYLVANICA        60.6        21.3 \n##  5 GINKGO BILOBA                 90.4        24.5 \n##  6 GLEDITSIA TRIACANTHOS         53.0        13.0 \n##  7 PLATANUS ACERIFOLIA           82.0        16.0 \n##  8 PYRUS CALLERYANA              21.0         5.00\n##  9 QUERCUS PALUSTRIS             65.5         6.48\n## 10 QUERCUS RUBRA                111.         20.7 \n## 11 TILIA CORDATA                 98.8        32.6 \n## 12 ZELKOVA SERRATA              101.         10.7\nny_trees_by_spc_summ %>%\nggplot() +\n  geom_pointrange(\n      aes(\n        y = spc_latin,\n        x = mean_height,\n        xmin = mean_height - stdev_height,\n        xmax = mean_height + stdev_height\n      )\n    )\nny_trees %>%\n  group_by(spc_latin, boroname) %>%\n  summarize(\n    mean_diam = mean(tree_diameter),\n    stdev_diam = sd(tree_diameter)\n  ) -> ny_trees_by_spc_boro_summ\nny_trees_by_spc_boro_summ\n## # A tibble: 48 × 4\n## # Groups:   spc_latin [12]\n##    spc_latin        boroname  mean_diam stdev_diam\n##    <chr>            <chr>         <dbl>      <dbl>\n##  1 ACER PLATANOIDES Bronx         13.9        6.74\n##  2 ACER PLATANOIDES Brooklyn      15.4       14.9 \n##  3 ACER PLATANOIDES Manhattan     11.6        8.45\n##  4 ACER PLATANOIDES Queens        15.1       12.9 \n##  5 ACER RUBRUM      Bronx         11.4        7.88\n##  6 ACER RUBRUM      Brooklyn      10.5        7.41\n##  7 ACER RUBRUM      Manhattan      6.63       4.23\n##  8 ACER RUBRUM      Queens        14.1        8.36\n##  9 ACER SACCHARINUM Bronx         19.7       10.5 \n## 10 ACER SACCHARINUM Brooklyn      22.2       10.1 \n## # ℹ 38 more rows\nny_trees_by_spc_boro_summ %>%\nggplot() +\n  geom_pointrange(\n    aes(\n      y = boroname,\n      x = mean_diam,\n      xmin = mean_diam-stdev_diam,\n      xmax = mean_diam+stdev_diam\n    )\n  ) +\n  facet_grid(spc_latin~.) +\n  theme(\n    strip.text.y = element_text(angle = 0)\n  )\nny_trees_by_spc_boro_summ %>%\nggplot() +\n  geom_pointrange(\n    aes(\n      y = boroname,\n      x = mean_diam,\n      xmin = mean_diam-stdev_diam,\n      xmax = mean_diam+stdev_diam,\n      fill = spc_latin\n    ), color = \"black\", shape = 21\n  ) +\n  labs(\n    y = \"Borough\", \n    x = \"Trunk diameter\"\n    # caption = str_wrap(\"Figure 1: Diameters of trees in New York City. Points correspond to average diameters of each tree species in each borough. Horizontal lines indicate the standard deviation of tree diameters. Points are colored according to tree species.\", width = 80)\n  ) +\n  facet_grid(spc_latin~.) +\n  guides(fill = \"none\") +\n  scale_fill_brewer(palette = \"Paired\") +\n  theme_bw() +\n  theme(\n    strip.text.y = element_text(angle = 0),\n    plot.caption = element_text(hjust = 0.5)\n  )"},{"path":"wrangling-and-summaries.html","id":"ordering","chapter":"wrangling and summaries","heading":"ordering","text":"can also sort order data frame based specific column command arrange(). Let’s quick look. Suppose wanted know lake coldest:suppose wanted know warmest?arrange() work grouped data, particularly useful combination slice(), can show us first n elements group:looks like coldest lakes three parks Devil Mountain Lake, Wild Lake, Desperation Lake!","code":"\narrange(alaska_lake_data, water_temp)\n## # A tibble: 220 × 7\n##    lake             park  water_temp    pH element mg_per_L\n##    <chr>            <chr>      <dbl> <dbl> <chr>      <dbl>\n##  1 Desperation_Lake NOAT        2.95  6.34 C          2.1  \n##  2 Desperation_Lake NOAT        2.95  6.34 N          0.005\n##  3 Desperation_Lake NOAT        2.95  6.34 P          0    \n##  4 Desperation_Lake NOAT        2.95  6.34 Cl         0.2  \n##  5 Desperation_Lake NOAT        2.95  6.34 S          2.73 \n##  6 Desperation_Lake NOAT        2.95  6.34 F          0.01 \n##  7 Desperation_Lake NOAT        2.95  6.34 Br         0    \n##  8 Desperation_Lake NOAT        2.95  6.34 Na         1.11 \n##  9 Desperation_Lake NOAT        2.95  6.34 K          0.16 \n## 10 Desperation_Lake NOAT        2.95  6.34 Ca         5.87 \n## # ℹ 210 more rows\n## # ℹ 1 more variable: element_type <chr>\narrange(alaska_lake_data, desc(water_temp))\n## # A tibble: 220 × 7\n##    lake      park  water_temp    pH element mg_per_L\n##    <chr>     <chr>      <dbl> <dbl> <chr>      <dbl>\n##  1 Lava_Lake BELA        20.2  7.42 C          8.3  \n##  2 Lava_Lake BELA        20.2  7.42 N          0.017\n##  3 Lava_Lake BELA        20.2  7.42 P          0.001\n##  4 Lava_Lake BELA        20.2  7.42 Cl         2.53 \n##  5 Lava_Lake BELA        20.2  7.42 S          0.59 \n##  6 Lava_Lake BELA        20.2  7.42 F          0.04 \n##  7 Lava_Lake BELA        20.2  7.42 Br         0.01 \n##  8 Lava_Lake BELA        20.2  7.42 Na         2.93 \n##  9 Lava_Lake BELA        20.2  7.42 K          0.57 \n## 10 Lava_Lake BELA        20.2  7.42 Ca        11.8  \n## # ℹ 210 more rows\n## # ℹ 1 more variable: element_type <chr>\nalaska_lake_data %>%\n  group_by(park) %>%\n  arrange(water_temp) %>%\n  slice(1)\n## # A tibble: 3 × 7\n## # Groups:   park [3]\n##   lake  park  water_temp    pH element mg_per_L element_type\n##   <chr> <chr>      <dbl> <dbl> <chr>      <dbl> <chr>       \n## 1 Devi… BELA        6.46  7.69 C            3.4 bound       \n## 2 Wild… GAAR        5.5   6.98 C            6.5 bound       \n## 3 Desp… NOAT        2.95  6.34 C            2.1 bound"},{"path":"wrangling-and-summaries.html","id":"mutate","chapter":"wrangling and summaries","heading":"mutate","text":"One last thing exercises… another command called mutate(). like summarize calculates user-defined statistics, creates output per-observation level instead group. means doesn’t make data set smaller, fact makes bigger, creating new row new variables defined inside mutate(). can also take grouped data. really useful calculating percentages within groups. example: within park, percent park’s total dissolved sulfur lake ?percent columns park add 100%, , example, Devil Mountain Lake 32.3% BELA’s dissolved sulfur.","code":"\nalaska_lake_data %>%\n  filter(element == \"S\") %>%\n  group_by(park) %>%\n  select(lake, park, element, mg_per_L) %>%\n  mutate(percent_S = mg_per_L/sum(mg_per_L)*100)\n## # A tibble: 20 × 5\n## # Groups:   park [3]\n##    lake                park  element mg_per_L percent_S\n##    <chr>               <chr> <chr>      <dbl>     <dbl>\n##  1 Devil_Mountain_Lake BELA  S           0.62     32.3 \n##  2 Imuruk_Lake         BELA  S           0.2      10.4 \n##  3 Kuzitrin_Lake       BELA  S           0.29     15.1 \n##  4 Lava_Lake           BELA  S           0.59     30.7 \n##  5 North_Killeak_Lake  BELA  S           0.04      2.08\n##  6 White_Fish_Lake     BELA  S           0.18      9.38\n##  7 Iniakuk_Lake        GAAR  S          12.1      13.2 \n##  8 Kurupa_Lake         GAAR  S          12.4      13.6 \n##  9 Lake_Matcharak      GAAR  S          13.3      14.5 \n## 10 Lake_Selby          GAAR  S           7.92      8.64\n## 11 Nutavukti_Lake      GAAR  S           2.72      2.97\n## 12 Summit_Lake         GAAR  S           3.21      3.50\n## 13 Takahula_Lake       GAAR  S           5.53      6.03\n## 14 Walker_Lake         GAAR  S           5.77      6.30\n## 15 Wild_Lake           GAAR  S          28.7      31.3 \n## 16 Desperation_Lake    NOAT  S           2.73     25.8 \n## 17 Feniak_Lake         NOAT  S           4.93     46.5 \n## 18 Lake_Kangilipak     NOAT  S           0.55      5.19\n## 19 Lake_Narvakrak      NOAT  S           1.38     13.0 \n## 20 Okoklik_Lake        NOAT  S           1.01      9.53"},{"path":"wrangling-and-summaries.html","id":"section-6","chapter":"wrangling and summaries","heading":"","text":"","code":""},{"path":"wrangling-and-summaries.html","id":"further-reading-2","chapter":"wrangling and summaries","heading":"further reading","text":"sure check Tidy Data Tutor: https://tidydatatutor.com/vis.html. easy way visualize ’s going data wrangling!","code":""},{"path":"dimensional-reduction.html","id":"dimensional-reduction","chapter":"dimensional reduction","heading":"dimensional reduction","text":"previous chapters, looked explore data sets visualizing many variables manually identifying trends. Sometimes, encounter data sets many variables, reasonable manually select certain variables create plots manually search trends. cases, need dimensionality reduction - set techniques helps us identify variables driving differences among samples. course, conduct dimensionality reduction using runMatrixAnalyses(), function loaded R Session run source() command.Matrix analyses can bit tricky set . two things can help us : () use template runMatrixAnalyses() (see ) (ii) critical think data terms samples analytes. Let’s consider Alaska lakes data set:can see dataset comprised measurements various analytes (.e. several chemical elements, well water_temp, pH), different samples (.e. lakes). need tell runMatrixAnalyses() function column relates samples analytes structuree","code":"\nalaska_lake_data\n## # A tibble: 220 × 7\n##    lake              park  water_temp    pH element mg_per_L\n##    <chr>             <chr>      <dbl> <dbl> <chr>      <dbl>\n##  1 Devil_Mountain_L… BELA        6.46  7.69 C          3.4  \n##  2 Devil_Mountain_L… BELA        6.46  7.69 N          0.028\n##  3 Devil_Mountain_L… BELA        6.46  7.69 P          0    \n##  4 Devil_Mountain_L… BELA        6.46  7.69 Cl        10.4  \n##  5 Devil_Mountain_L… BELA        6.46  7.69 S          0.62 \n##  6 Devil_Mountain_L… BELA        6.46  7.69 F          0.04 \n##  7 Devil_Mountain_L… BELA        6.46  7.69 Br         0.02 \n##  8 Devil_Mountain_L… BELA        6.46  7.69 Na         8.92 \n##  9 Devil_Mountain_L… BELA        6.46  7.69 K          1.2  \n## 10 Devil_Mountain_L… BELA        6.46  7.69 Ca         5.73 \n## # ℹ 210 more rows\n## # ℹ 1 more variable: element_type <chr>"},{"path":"dimensional-reduction.html","id":"section-7","chapter":"dimensional reduction","heading":"","text":"","code":""},{"path":"dimensional-reduction.html","id":"pca","chapter":"dimensional reduction","heading":"pca","text":"“analytes driving differences among samples?”\n“analytes data set correlated?”","code":""},{"path":"dimensional-reduction.html","id":"theory","chapter":"dimensional reduction","heading":"theory","text":"PCA looks variance high dimensional data set chooses new axes within data set align directions containing highest variance. new axes called principal components. Let’s look example:example , three dimensional space can reduced two dimensional space principal components analysis. New axes (principal components) selected (bold arrows left) become x y axes principal components space (right).can run visualize principal components analyses using runMatrixAnalyses() function example . can see output, command provides sample_IDs, sample information, coordinates sample 2D projection (“PCA plot”) raw data, case wish processing.Let’s plot 2D projection Alaska lakes data:Great! plot can see White Fish Lake North Killeak Lake, BELA park, quite different parks (separated others along dimension 1, .e. first principal component). time, Wild Lake, Iniakuk Lake, Walker Lake, several lakes GAAR park different others (separated others along dimension 2, .e. second principal component).Important question: makes lakes listed different others? Certainly aspect chemistry, since ’s data analysis built upon, determine analyte(s) driving differences among lakes see PCA plot?","code":"\n\nalaska_lake_data_wide <- pivot_wider(alaska_lake_data[,1:6], names_from = \"element\", values_from = \"mg_per_L\")\nalaska_lake_data_wide\n## # A tibble: 20 × 15\n##    lake      park  water_temp    pH     C     N     P     Cl\n##    <chr>     <chr>      <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n##  1 Devil_Mo… BELA        6.46  7.69   3.4 0.028 0      10.4 \n##  2 Imuruk_L… BELA       17.4   6.44   4.7 0.013 0       1.18\n##  3 Kuzitrin… BELA        8.06  7.45   2   0     0       0.67\n##  4 Lava_Lake BELA       20.2   7.42   8.3 0.017 0.001   2.53\n##  5 North_Ki… BELA       11.3   8.04   4.3 0.037 0.001 337.  \n##  6 White_Fi… BELA       12.0   7.82  12.3 0.034 0.006 105.  \n##  7 Iniakuk_… GAAR        9.1   7.01   3.3 0.141 0       0.22\n##  8 Kurupa_L… GAAR        9.3   7.03   2.1 0.043 0       0.13\n##  9 Lake_Mat… GAAR       10.2   6.95   5.1 0     0       1.25\n## 10 Lake_Sel… GAAR       15.1   7.15   4.2 0.107 0       0.11\n## 11 Nutavukt… GAAR       17.6   6.88   4.5 0     0.001   0.18\n## 12 Summit_L… GAAR       11.9   6.45   2.4 0     0.001   0.08\n## 13 Takahula… GAAR        9.9   6.88   2.7 0.014 0       0.23\n## 14 Walker_L… GAAR       15.3   7.22   1.3 0.19  0.001   0.19\n## 15 Wild_Lake GAAR        5.5   6.98   6.5 0.13  0.001   0.31\n## 16 Desperat… NOAT        2.95  6.34   2.1 0.005 0       0.2 \n## 17 Feniak_L… NOAT        4.51  7.24   1.8 0     0       0.21\n## 18 Lake_Kan… NOAT        5.36  6.56   8.5 0.005 0       0.55\n## 19 Lake_Nar… NOAT       18.3   7.31   5.8 0     0       0.76\n## 20 Okoklik_… NOAT        6.46  6.87   7.8 0     0       0.76\n## # ℹ 7 more variables: S <dbl>, F <dbl>, Br <dbl>, Na <dbl>,\n## #   K <dbl>, Ca <dbl>, Mg <dbl>\n\nAK_lakes_pca <- runMatrixAnalyses(\n  data = alaska_lake_data_wide,\n  analysis = \"pca\",\n  columns_w_values_for_single_analyte = colnames(alaska_lake_data_wide)[3:15],\n  columns_w_sample_ID_info = c(\"lake\", \"park\")\n)\nhead(AK_lakes_pca)\n## # A tibble: 6 × 18\n##   sample_unique_ID      lake  park   Dim.1  Dim.2 water_temp\n##   <chr>                 <chr> <chr>  <dbl>  <dbl>      <dbl>\n## 1 Devil_Mountain_Lake_… Devi… BELA   0.229 -0.861       6.46\n## 2 Imuruk_Lake_BELA      Imur… BELA  -1.17  -1.62       17.4 \n## 3 Kuzitrin_Lake_BELA    Kuzi… BELA  -0.918 -1.15        8.06\n## 4 Lava_Lake_BELA        Lava… BELA   0.219 -1.60       20.2 \n## 5 North_Killeak_Lake_B… Nort… BELA   9.46   0.450      11.3 \n## 6 White_Fish_Lake_BELA  Whit… BELA   4.17  -0.972      12.0 \n## # ℹ 12 more variables: pH <dbl>, C <dbl>, N <dbl>, P <dbl>,\n## #   Cl <dbl>, S <dbl>, F <dbl>, Br <dbl>, Na <dbl>,\n## #   K <dbl>, Ca <dbl>, Mg <dbl>\nggplot(data = AK_lakes_pca, aes(x = Dim.1, y = Dim.2)) +\n  geom_point(aes(fill = park), shape = 21, size = 4, alpha = 0.8) +\n  geom_label_repel(aes(label = lake), alpha = 0.5) +\n  theme_classic()"},{"path":"dimensional-reduction.html","id":"ordination-plots","chapter":"dimensional reduction","heading":"ordination plots","text":"Let’s look access information analytes major contributors principal component. important tell analytes associated particular dimensions, extension, analytes associated (markers ) particular groups PCA plot. can determined using ordination plot. Let’s look example. can obtain ordination plot information using runMatrixAnalyses() analysis = \"pca_ord\":can now visualize ordination plot using standard ggplot plotting techniques. Note use geom_label_repel() filter() label certain segments ordination plot. need use geom_label_repel(), use built geom_label(), geom_label_repel() can make labelling segments easier.Great! read ordination plot:considering one analyte’s vector: vector’s projected value axis shows much variance aligned principal component.considering one analyte’s vector: vector’s projected value axis shows much variance aligned principal component.considering two analyte vectors: angle two vectors indicates correlated two variables . point direction, highly correlated. meet 90 degrees, correlated. meet ~180 degrees, negatively correlated. say one analyte “1.9” respect dimension 2 another “-1.9” respect dimension 2. Let’s also say vectors ~“0” respect dimension 1.considering two analyte vectors: angle two vectors indicates correlated two variables . point direction, highly correlated. meet 90 degrees, correlated. meet ~180 degrees, negatively correlated. say one analyte “1.9” respect dimension 2 another “-1.9” respect dimension 2. Let’s also say vectors ~“0” respect dimension 1.ordination plot , can now see abundances K, Cl, Br, Na major contributors variance first principal component (first dimension). abundances elements make White Fish Lake North Killeak Lake different lakes. can also see abundances N, S, Ca major contributors variance second dimension, means elements ar set Wild Lake, Iniakuk Lake, Walker Lake, several lakes GAAR park apart rest lakes data set. slightly easier understand look overlay two plots, often called “biplot”:Note plot ordination data circular layout segments. Sometimes much easier plot (interpret!) alternatives:","code":"## # A tibble: 6 × 3\n##   analyte      Dim.1   Dim.2\n##   <chr>        <dbl>   <dbl>\n## 1 water_temp 0.0750  -0.261 \n## 2 pH         0.686    0.0185\n## 3 C          0.290   -0.242 \n## 4 N          0.00435  0.714 \n## 5 P          0.473   -0.0796\n## 6 Cl         0.953    0.0148\n# AK_lakes_pca_ord <- runMatrixAnalysis(\n#   data = alaska_lake_data,\n#   analysis = c(\"pca_ord\"),\n#   column_w_names_of_multiple_analytes = \"element\",\n#   column_w_values_for_multiple_analytes = \"mg_per_L\",\n#   columns_w_values_for_single_analyte = c(\"water_temp\", \"pH\"),\n#   columns_w_additional_analyte_info = \"element_type\",\n#   columns_w_sample_ID_info = c(\"lake\", \"park\")\n# )\n# head(AK_lakes_pca_ord)\n\nggplot(AK_lakes_pca_ord) +\n  geom_segment(aes(x = 0, y = 0, xend = Dim.1, yend = Dim.2, color = analyte), size = 1) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +\n  geom_label_repel(\n    data = filter(AK_lakes_pca_ord, Dim.1 > 0.9, Dim.2 < 0.1, Dim.2 > -0.1),\n    aes(x = Dim.1, y = Dim.2, label = analyte), xlim = c(1,1.5)\n  ) +\n  geom_label_repel(\n    data = filter(AK_lakes_pca_ord, Dim.2 > 0.5),\n    aes(x = Dim.1, y = Dim.2, label = analyte), direction = \"y\", ylim = c(1,1.5)\n  ) +\n  coord_cartesian(xlim = c(-1,1.5), ylim = c(-1,1.5)) +\n  theme_bw()\n# AK_lakes_pca <- runMatrixAnalysis(\n#   data = alaska_lake_data,\n#   analysis = c(\"pca\"),\n#   column_w_names_of_multiple_analytes = \"element\",\n#   column_w_values_for_multiple_analytes = \"mg_per_L\",\n#   columns_w_values_for_single_analyte = c(\"water_temp\", \"pH\"),\n#   columns_w_additional_analyte_info = \"element_type\",\n#   columns_w_sample_ID_info = c(\"lake\", \"park\"),\n#   scale_variance = TRUE\n# )\n# \n# AK_lakes_pca_ord <- runMatrixAnalysis(\n#   data = alaska_lake_data,\n#   analysis = c(\"pca_ord\"),\n#   column_w_names_of_multiple_analytes = \"element\",\n#   column_w_values_for_multiple_analytes = \"mg_per_L\",\n#   columns_w_values_for_single_analyte = c(\"water_temp\", \"pH\"),\n#   columns_w_additional_analyte_info = \"element_type\",\n#   columns_w_sample_ID_info = c(\"lake\", \"park\")\n# )\n\nggplot() +\n  geom_point(\n    data = AK_lakes_pca, \n    aes(x = Dim.1, y = Dim.2, fill = park), shape = 21, size = 4, alpha = 0.8\n  ) +\n  # geom_label_repel(aes(label = lake), alpha = 0.5) +\n  geom_segment(\n    data = AK_lakes_pca_ord,\n    aes(x = 0, y = 0, xend = Dim.1, yend = Dim.2, color = analyte),\n    size = 1\n  ) +\n  scale_color_manual(values = discrete_palette) +\n  theme_classic()\nAK_lakes_pca_ord %>%\n  ggplot(aes(x = Dim.1, y = analyte)) +\n    geom_point(aes(fill = analyte), shape = 22, size = 3) +\n    scale_fill_manual(values = discrete_palette) +\n    theme_bw()"},{"path":"dimensional-reduction.html","id":"principal-components","chapter":"dimensional reduction","heading":"principal components","text":"also can access information much variance data set explained principal component, can plot using ggplot:Cool! can see first principal component retains nearly 50% variance original dataset, second dimension contains 20%. can derive important notion PCA visualization : scales two axes need distances points x y directions comparable. can accomplished using coord_fixed() addition ggplots.","code":"\nAK_lakes_pca_dim <- runMatrixAnalyses(\n  data = alaska_lake_data_wide,\n  analysis = c(\"pca_dim\"),\n  columns_w_values_for_single_analyte = colnames(alaska_lake_data_wide)[3:15],\n  columns_w_sample_ID_info = c(\"lake\", \"park\")\n)\nhead(AK_lakes_pca_dim)\n## # A tibble: 6 × 2\n##   principal_component percent_variance_explained\n##                 <dbl>                      <dbl>\n## 1                   1                      48.8 \n## 2                   2                      18.6 \n## 3                   3                      11.6 \n## 4                   4                       7.88\n## 5                   5                       4.68\n## 6                   6                       3.33\n\nggplot(\n  data = AK_lakes_pca_dim, \n  aes(x = principal_component, y = percent_variance_explained)\n) +\n  geom_line() +\n  geom_point() +\n  theme_bw()"},{"path":"dimensional-reduction.html","id":"umap-and-tsne","chapter":"dimensional reduction","heading":"umap and tsne","text":"“non-linear dimensionality reduction techniques reveal hidden structure data?”\n“Can identify clusters subtle gradients dataset might missed linear methods?”UMAP (Uniform Manifold Approximation Projection) non-linear dimensionality reduction technique , unlike PCA, can capture local global data structure. especially useful data might clusters manifold structures aren’t well-represented linear combinations features.work wine_quality dataset. wine quality dataset doesn’t come unique sample identifiers, essential matrix analysis. can create identifiers adding new column called ID data frame. code demonstrates :command, dollar sign ($) used access (create) ID column within wine_quality data frame. seq() function generates sequence numbers starting 1 ending number rows dataset (given dim(wine_quality)[1]), increment 1. way, every sample uniquely identified.starting non-linear techniques, can helpful see linear method like PCA clusters samples. run analysis using runMatrixAnalysis(), specifying relevant columns contain continuous variables (columns 4 14), passing along sample identifier along additional information wine type, quality_score, quality_category.PCA plot, point represents wine sample, position determined first two principal components. ’re using quality_score fill points color, different shapes distinguish wine type. serves baseline comparing non-linear methods handle data.can perform UMAP wine quality dataset just easily PCA. code shows run UMAP using runMatrixAnalysis():UMAP plot, point’s coordinates (Dim_1 Dim_2) derived UMAP’s algorithm, strives preserve overall topology data. result, UMAP might reveal clusters continuous gradients related wine quality type aren’t apparent PCA.t‑SNE (t‑Distributed Stochastic Neighbor Embedding) another popular non-linear dimensionality reduction technique. excels revealing clusters can sometimes distort global structure favor preserving local relationships. Although ’re showing full t‑SNE example , can run t‑SNE using runMatrixAnalysis() function specifying analysis = “tsne” (see ). Note tsne fails samples duplicate analyte values, filter duplicates., plot t‑SNE dimensions similar fashion PCA UMAP examples.UMAP t‑SNE provide powerful alternatives PCA data’s structure non-linear. can help uncover hidden patterns clusters focusing preserving local relationships—UMAP maintaining sense global structure, t‑SNE emphasizing neighborhood structure data.","code":"\nwine_quality$ID <- seq(1, dim(wine_quality)[1], 1)\nwq <- runMatrixAnalyses(\n  data = wine_quality,\n  analysis = \"pca\",\n  columns_w_values_for_single_analyte = colnames(wine_quality)[4:14],\n  columns_w_sample_ID_info = c(\"ID\", \"type\", \"quality_score\", \"quality_category\")\n)\nwq %>%\n  arrange(quality_score) %>%\n  ggplot(aes(x = Dim.1, y = Dim.2)) +\n  geom_point(size = 3, aes(shape = type, fill = quality_score)) +\n  scale_shape_manual(values = c(21, 22)) +\n  scale_fill_viridis() +\n  theme_classic()\nrunMatrixAnalyses(\n  data = wine_quality,\n  analysis = \"umap\",\n  columns_w_values_for_single_analyte = colnames(wine_quality)[4:14],\n  columns_w_sample_ID_info = c(\"ID\", \"type\", \"quality_score\", \"quality_category\")\n) %>%\n  arrange(quality_score) %>%\n  ggplot(aes(x = Dim_1, y = Dim_2)) +\n  geom_point(size = 3, aes(shape = type, fill = quality_score)) +\n  scale_shape_manual(values = c(21, 22)) +\n  scale_fill_viridis() +\n  theme_classic()\nwine_quality_deduplicated <- wine_quality[!duplicated(wine_quality[4:14]),]\ntsne_results <- runMatrixAnalyses(\n  data = wine_quality_deduplicated,\n  analysis = \"tsne\",\n  columns_w_values_for_single_analyte = colnames(wine_quality)[4:14],\n  columns_w_sample_ID_info = c(\"ID\", \"type\", \"quality_score\", \"quality_category\")\n)"},{"path":"dimensional-reduction.html","id":"section-8","chapter":"dimensional reduction","heading":"","text":"","code":""},{"path":"dimensional-reduction.html","id":"further-reading-3","chapter":"dimensional reduction","heading":"further reading","text":"PCA Explanation Video: YouTube video provides detailed visually intuitive explanation Principal Component Analysis (PCA), breaking complex concepts clear examples graphics. part curated playlist covers variety topics related data visualization statistical analysis. Watch PCA video.PCA Explanation Video: YouTube video provides detailed visually intuitive explanation Principal Component Analysis (PCA), breaking complex concepts clear examples graphics. part curated playlist covers variety topics related data visualization statistical analysis. Watch PCA video.Understanding UMAP: blog post Pair Code delves fundamentals UMAP, explaining intuition behind algorithm practical applications data analysis. provides accessible overview bridges gap theoretical concepts real-world use cases, making valuable read beginners advanced users. Explore Understanding UMAP Article.Understanding UMAP: blog post Pair Code delves fundamentals UMAP, explaining intuition behind algorithm practical applications data analysis. provides accessible overview bridges gap theoretical concepts real-world use cases, making valuable read beginners advanced users. Explore Understanding UMAP Article.UMAP: Mathematical Details (clearly explained!!!) YouTube video offers detailed explanation mathematical underpinnings UMAP, breaking algorithm clear approachable manner. excellent resource viewers want deepen understanding UMAP works behind scenes.\nWatch UMAP Mathematical Details Video.UMAP: Mathematical Details (clearly explained!!!) YouTube video offers detailed explanation mathematical underpinnings UMAP, breaking algorithm clear approachable manner. excellent resource viewers want deepen understanding UMAP works behind scenes.\nWatch UMAP Mathematical Details Video.","code":""},{"path":"hierarchical-clustering.html","id":"hierarchical-clustering","chapter":"hierarchical clustering","heading":"hierarchical clustering","text":"“samples closely related?”","code":""},{"path":"hierarchical-clustering.html","id":"section-9","chapter":"hierarchical clustering","heading":"","text":"far looking plot raw data, summarize data, reduce data set’s dimensionality. ’s time look identify relationships samples data sets. example: Alaska lakes dataset, lake similar, chemically speaking, Lake Narvakrak? Answering requires calculating numeric distances samples based chemical properties. , first thing need distance matrix:Please note can get distance matrices directly runMatrixAnalyses specifying analysis = \"dist\":can distance matrices though, lots . Let’s start looking example hierarchical clustering. , just need tell runMatrixAnalyses() use analysis = \"hclust\":works! Now can plot cluster diagram ggplot add-called ggtree. ’ve seen ggplot takes “data” argument (.e. ggplot(data = <some_data>) + geom_*() etc.). contrast, ggtree takes argument called tr, though ’re using runMatrixAnalysis() function, can treat two (data tr) , , use: ggtree(tr = <output_from_runMatrixAnalyses>) + geom_*() etc.Note ggtree also comes several great new geoms: geom_tiplab() geom_tippoint(). Let’s try :Cool! Though plot use tweaking… let’s try:nice! Since North Killeak White Fish different others, re-analyze data two removed:","code":"\nalaska_lake_data %>%\n    select(-element_type) %>%\n    pivot_wider(names_from = \"element\", values_from = \"mg_per_L\") -> alaska_lake_data_wide\n\ndist <- runMatrixAnalyses(\n    data = alaska_lake_data_wide,\n    analysis = c(\"dist\"),\n    columns_w_values_for_single_analyte = colnames(alaska_lake_data_wide)[3:15],\n    columns_w_sample_ID_info = c(\"lake\", \"park\")\n)\n## Replacing NAs in your data with mean\n\nas.matrix(dist)[1:3,1:3]\n##                          Devil_Mountain_Lake_BELA\n## Devil_Mountain_Lake_BELA                 0.000000\n## Imuruk_Lake_BELA                         3.672034\n## Kuzitrin_Lake_BELA                       1.663147\n##                          Imuruk_Lake_BELA\n## Devil_Mountain_Lake_BELA         3.672034\n## Imuruk_Lake_BELA                 0.000000\n## Kuzitrin_Lake_BELA               3.062381\n##                          Kuzitrin_Lake_BELA\n## Devil_Mountain_Lake_BELA           1.663147\n## Imuruk_Lake_BELA                   3.062381\n## Kuzitrin_Lake_BELA                 0.000000\nAK_lakes_clustered <- runMatrixAnalyses(\n    data = alaska_lake_data_wide,\n    analysis = c(\"hclust\"),\n    columns_w_values_for_single_analyte = colnames(alaska_lake_data_wide)[3:15],\n    columns_w_sample_ID_info = c(\"lake\", \"park\")\n)\nAK_lakes_clustered\n## # A tibble: 38 × 26\n##    sample_unique_ID   lake  park  parent  node branch.length\n##    <chr>              <chr> <chr>  <int> <int>         <dbl>\n##  1 Devil_Mountain_La… Devi… BELA      23     1         7.81 \n##  2 Imuruk_Lake_BELA   Imur… BELA      25     2         6.01 \n##  3 Kuzitrin_Lake_BELA Kuzi… BELA      24     3         3.27 \n##  4 Lava_Lake_BELA     Lava… BELA      32     4         3.14 \n##  5 North_Killeak_Lak… Nort… BELA      38     5       256.   \n##  6 White_Fish_Lake_B… Whit… BELA      38     6         0.828\n##  7 Iniakuk_Lake_GAAR  Inia… GAAR      36     7         2.59 \n##  8 Kurupa_Lake_GAAR   Kuru… GAAR      30     8         6.00 \n##  9 Lake_Matcharak_GA… Lake… GAAR      35     9         2.09 \n## 10 Lake_Selby_GAAR    Lake… GAAR      29    10         3.49 \n## # ℹ 28 more rows\n## # ℹ 20 more variables: label <chr>, isTip <lgl>, x <dbl>,\n## #   y <dbl>, branch <dbl>, angle <dbl>, bootstrap <lgl>,\n## #   water_temp <dbl>, pH <dbl>, C <dbl>, N <dbl>, P <dbl>,\n## #   Cl <dbl>, S <dbl>, F <dbl>, Br <dbl>, Na <dbl>,\n## #   K <dbl>, Ca <dbl>, Mg <dbl>\nlibrary(ggtree)\nAK_lakes_clustered %>%\nggtree() +\n  geom_tiplab() +\n  geom_tippoint() +\n  theme_classic() +\n  scale_x_continuous(limits = c(0,700))\nAK_lakes_clustered %>%\nggtree() +\n    geom_tiplab(aes(label = lake), offset = 10, align = TRUE) +\n    geom_tippoint(shape = 21, aes(fill = park), size = 4) +\n    scale_x_continuous(limits = c(0,600)) +\n    scale_fill_brewer(palette = \"Set1\") +\n    # theme_classic() +\n    theme(\n      legend.position = c(0.4,0.2)\n    )\nalaska_lake_data_wide %>%\n  filter(!lake %in% c(\"North_Killeak_Lake\",\"White_Fish_Lake\")) -> alaska_lake_data_wide_filtered\n\nrunMatrixAnalyses(\n    data = alaska_lake_data_wide_filtered,\n    analysis = c(\"hclust\"),\n    columns_w_values_for_single_analyte = colnames(alaska_lake_data_wide_filtered)[3:15],\n    columns_w_sample_ID_info = c(\"lake\", \"park\")\n) %>%\nggtree() +\n    geom_tiplab(aes(label = lake), offset = 10, align = TRUE) +\n    geom_tippoint(shape = 21, aes(fill = park), size = 4) +\n    scale_x_continuous(limits = c(0,100)) +\n    scale_fill_brewer(palette = \"Set1\") +\n    # theme_classic() +\n    theme(\n      legend.position = c(0.05,0.9)\n    )\n## Replacing NAs in your data with mean"},{"path":"hierarchical-clustering.html","id":"further-reading-4","chapter":"hierarchical clustering","heading":"further reading","text":"information plotting annotated trees, see: https://yulab-smu.top/treedata-book/chapter10.html.clustering, see: https://ryanwingate.com/intro--machine-learning/unsupervised/hierarchical--density-based-clustering/.","code":""},{"path":"comparing-means.html","id":"comparing-means","chapter":"comparing means","heading":"comparing means","text":"“two things ?”","code":""},{"path":"comparing-means.html","id":"section-10","chapter":"comparing means","heading":"","text":"Often, want know study subjects contain different amounts certain analytes. example, “lake contain potassium lake ?” , need statistical tests. , look comparing mean values analyte abundance situations two samples situations two samples.find many concepts discussed chapter easier think example mind. , suppose analytical chemist Hawaii studying chemistry island’s aquifers. data set hawaii_aquifers. can see output structure data set - 990 measurements 9 different analytes multiple wells draw set 10 aquifers.Importantly, many wells draw aquifer, shown graph .","code":"\nhawaii_aquifers\n## # A tibble: 954 × 6\n##    aquifer_code well_name         longitude latitude analyte\n##    <chr>        <chr>                 <dbl>    <dbl> <chr>  \n##  1 aquifer_1    Alewa_Heights_Sp…        NA       NA SiO2   \n##  2 aquifer_1    Alewa_Heights_Sp…        NA       NA Cl     \n##  3 aquifer_1    Alewa_Heights_Sp…        NA       NA Mg     \n##  4 aquifer_1    Alewa_Heights_Sp…        NA       NA Na     \n##  5 aquifer_1    Alewa_Heights_Sp…        NA       NA K      \n##  6 aquifer_1    Alewa_Heights_Sp…        NA       NA SO4    \n##  7 aquifer_1    Alewa_Heights_Sp…        NA       NA HCO3   \n##  8 aquifer_1    Alewa_Heights_Sp…        NA       NA dissol…\n##  9 aquifer_1    Alewa_Heights_Sp…        NA       NA Ca     \n## 10 aquifer_1    Beretania_High_S…        NA       NA SiO2   \n## # ℹ 944 more rows\n## # ℹ 1 more variable: abundance <dbl>\nunique(hawaii_aquifers$aquifer_code)\n##  [1] \"aquifer_1\"  \"aquifer_2\"  \"aquifer_3\"  \"aquifer_4\" \n##  [5] \"aquifer_5\"  \"aquifer_6\"  \"aquifer_7\"  \"aquifer_8\" \n##  [9] \"aquifer_9\"  \"aquifer_10\"\nhawaii_aquifers %>%\n  select(aquifer_code, well_name) %>%\n  group_by(aquifer_code) %>%\n  summarize(n_wells = length(unique(well_name))) -> aquifers_summarized\n\naquifers_summarized\n## # A tibble: 10 × 2\n##    aquifer_code n_wells\n##    <chr>          <int>\n##  1 aquifer_1         12\n##  2 aquifer_10         7\n##  3 aquifer_2          5\n##  4 aquifer_3          3\n##  5 aquifer_4         16\n##  6 aquifer_5          4\n##  7 aquifer_6         12\n##  8 aquifer_7          9\n##  9 aquifer_8          3\n## 10 aquifer_9         30\n\nggplot(aquifers_summarized) + geom_col(aes(x = n_wells, y = aquifer_code))"},{"path":"comparing-means.html","id":"definitions","chapter":"comparing means","heading":"definitions","text":"populations independent measurements: comparing means, comparing two sets values. important consider values came first place. particular, usually useful think values representatives larger populations. example aquifer data set, can think measurements different wells drawing aquifer independent measurements “population” (.e. aquifer).populations independent measurements: comparing means, comparing two sets values. important consider values came first place. particular, usually useful think values representatives larger populations. example aquifer data set, can think measurements different wells drawing aquifer independent measurements “population” (.e. aquifer).null hypothesis: conduct statistical test, testing null hypothesis. null (think “default”) hypothesis difference bewteen means (hence name “null”). example aquifers, let’s say ’re interested whether two aquifers different abundances potassium - case null hypothesis differ, words, amount potassium.null hypothesis: conduct statistical test, testing null hypothesis. null (think “default”) hypothesis difference bewteen means (hence name “null”). example aquifers, let’s say ’re interested whether two aquifers different abundances potassium - case null hypothesis differ, words, amount potassium.p value: p value represents probability getting data extreme results null hypothesis true. words - p value probability observe differences , fact differences means . continue example: suppose measure potassium levels 10% wells access aquifer find aquifer_1 potassium levels 14 +/- 2 aquifer_2 potassium levels 12 +/- 1. Suppose conduct statistical test get p value 0.04. means , assuming aquifers magneisum levels (.e. assuming null hypothesis true), 4% chance get measured values . words, aquifers potassium abundance, pretty unlikely obtained measurements .p value: p value represents probability getting data extreme results null hypothesis true. words - p value probability observe differences , fact differences means . continue example: suppose measure potassium levels 10% wells access aquifer find aquifer_1 potassium levels 14 +/- 2 aquifer_2 potassium levels 12 +/- 1. Suppose conduct statistical test get p value 0.04. means , assuming aquifers magneisum levels (.e. assuming null hypothesis true), 4% chance get measured values . words, aquifers potassium abundance, pretty unlikely obtained measurements .Please note p value probability detected difference false positive. probability false positive requires additional information order calculated. discussion please see end chapter.","code":""},{"path":"comparing-means.html","id":"test-selection","chapter":"comparing means","heading":"test selection","text":"many different types statistical tests. flow chart illustrating recommended statistical tests used course. can see three regimes tests: variance normality tests (blue), parametric tests (green), non-parametric tests (orange):comparing means, need first determine kind statistical tests can use data. () data can reasonably modelled normal distribution (ii) variances two means similar, can use powerful “parametric” tests (.e. tests likely detect difference means, assuming one exists). one criteria met, need use less powerful “non-parametric” tests.can check data normality similar variances using Shapiro test Levene test. Let’s use hawaii_aquifers data example, let’s consider element potassium:work two means, let’s just look aquifers 1 6:data normally distributed? similar variance? Let’s get first approximation looking plot:Based graphic, ’s hard say! Let’s use statistical test help. want run Shaprio test, looking see group normally distributed (group “aquifer_code”, .e. aquifer_1 aquifer_6). means need group_by(aquifer_code) run test:p-values 0.05! means distributions significantly different normal distribution. variances two means? similar? need Levene test. test, looking within group, rather across groups - means need group_by(aquifer_code) specify y ~ x formula instead:p-value test 0.596! means variances significantly different. shape (normality) variances (equal variances), thing can different means. allows us set null hypothesis calculate probability obtaining different means two sets observations come identical source.","code":"\nK_data <- hawaii_aquifers %>%\n  filter(analyte == \"K\")\n  K_data\n## # A tibble: 106 × 6\n##    aquifer_code well_name         longitude latitude analyte\n##    <chr>        <chr>                 <dbl>    <dbl> <chr>  \n##  1 aquifer_1    Alewa_Heights_Sp…       NA      NA   K      \n##  2 aquifer_1    Beretania_High_S…       NA      NA   K      \n##  3 aquifer_1    Beretania_Low_Se…       NA      NA   K      \n##  4 aquifer_1    Kuliouou_Well         -158.     21.3 K      \n##  5 aquifer_1    Manoa_Well_II         -158.     21.3 K      \n##  6 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  7 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  8 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  9 aquifer_1    Nuuanu_Aerator_W…     -158.     21.4 K      \n## 10 aquifer_1    Palolo_Tunnel         -158.     21.3 K      \n## # ℹ 96 more rows\n## # ℹ 1 more variable: abundance <dbl>\nK_data_1_6 <- K_data %>%\n    filter(aquifer_code %in% c(\"aquifer_1\", \"aquifer_6\"))\n\nK_data_1_6\n## # A tibble: 24 × 6\n##    aquifer_code well_name         longitude latitude analyte\n##    <chr>        <chr>                 <dbl>    <dbl> <chr>  \n##  1 aquifer_1    Alewa_Heights_Sp…       NA      NA   K      \n##  2 aquifer_1    Beretania_High_S…       NA      NA   K      \n##  3 aquifer_1    Beretania_Low_Se…       NA      NA   K      \n##  4 aquifer_1    Kuliouou_Well         -158.     21.3 K      \n##  5 aquifer_1    Manoa_Well_II         -158.     21.3 K      \n##  6 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  7 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  8 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  9 aquifer_1    Nuuanu_Aerator_W…     -158.     21.4 K      \n## 10 aquifer_1    Palolo_Tunnel         -158.     21.3 K      \n## # ℹ 14 more rows\n## # ℹ 1 more variable: abundance <dbl>\n\nggplot(K_data_1_6, aes(x = aquifer_code, y = abundance)) +\n    geom_boxplot() +\n    geom_point()\nK_data_1_6 %>%\n  ggplot(aes(x = abundance)) + \n    geom_histogram(bins = 30) +\n    facet_wrap(~aquifer_code) +\n    geom_density(aes(y = ..density..*10), color = \"blue\")\nK_data_1_6 %>%\n  group_by(aquifer_code) %>% \n  shapiroTest(abundance)\n## # A tibble: 2 × 4\n##   aquifer_code variable statistic     p\n##   <chr>        <chr>        <dbl> <dbl>\n## 1 aquifer_1    values       0.885 0.102\n## 2 aquifer_6    values       0.914 0.239\nK_data_1_6 %>%\n  leveneTest(abundance ~ aquifer_code)\n## # A tibble: 1 × 4\n##     df1   df2 statistic     p\n##   <int> <int>     <dbl> <dbl>\n## 1     1    22     0.289 0.596"},{"path":"comparing-means.html","id":"two-means","chapter":"comparing means","heading":"two means","text":"Now, since data passed test, means can use normal t-test. t-test parametric test. means relies modelling data using normal distribution order make comparisons. also powerful test. means likely detect difference means, assuming one present. Let’s try :p-value 0.012! 0.05, meaning 95% chance two means different. Suppose data passed Shapiro /Levene tests. need use Wilcox test. Wilcox test non-parametric test, means use normal distribution model data order make comparisons. means less powerful test t-test, means less likely detect difference means, assuming one. fun, let’s try one compare p-values two methods:p-value 0.028! higher value given t-test (0.012). Wilcox test less powerful test: less likely detect differences means, assuming exist.","code":"\nK_data_1_6 %>%\n  tTest(abundance ~ aquifer_code)\n## # A tibble: 1 × 8\n##   .y.       group1 group2    n1    n2 statistic    df      p\n## * <chr>     <chr>  <chr>  <int> <int>     <dbl> <dbl>  <dbl>\n## 1 abundance aquif… aquif…    12    12     -2.75  20.5 0.0121\nK_data_1_6 %>%\n  wilcoxTest(abundance ~ aquifer_code)\n## # A tibble: 1 × 7\n##   .y.       group1    group2       n1    n2 statistic      p\n## * <chr>     <chr>     <chr>     <int> <int>     <dbl>  <dbl>\n## 1 abundance aquifer_1 aquifer_6    12    12      33.5 0.0282"},{"path":"comparing-means.html","id":"more-than-two-means","chapter":"comparing means","heading":"more than two means","text":"previous section compared two means. want compare means two study subjects? first step determine tests use. Let’s consider hawaii aquifer data , though time let’s use aquifers, just two:Let’s check visually see group normally distributed see roughly equal variance:, somewhat hard tell visually data normally distributed. seems pretty likely different variances means, let’s check using Shapiro Levene tests. Don’t forget: Shaprio test, looking within group need group_by(), Levene test, looking across groups, need provide y~x formula:Based tests, looks like data aquifer 9 significantly different normal distribution (Shaprio test p < 0.05), variances certainly different one another (Levene test p > 0.05).Let’s assume second data passed tests. means reasonably model data normal distributions use parametric test compare means. means can use ANOVA test differences means.","code":"\nK_data <- hawaii_aquifers %>%\n  filter(analyte == \"K\")\n\nK_data\n## # A tibble: 106 × 6\n##    aquifer_code well_name         longitude latitude analyte\n##    <chr>        <chr>                 <dbl>    <dbl> <chr>  \n##  1 aquifer_1    Alewa_Heights_Sp…       NA      NA   K      \n##  2 aquifer_1    Beretania_High_S…       NA      NA   K      \n##  3 aquifer_1    Beretania_Low_Se…       NA      NA   K      \n##  4 aquifer_1    Kuliouou_Well         -158.     21.3 K      \n##  5 aquifer_1    Manoa_Well_II         -158.     21.3 K      \n##  6 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  7 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  8 aquifer_1    Moanalua_Wells_P…     -158.     21.4 K      \n##  9 aquifer_1    Nuuanu_Aerator_W…     -158.     21.4 K      \n## 10 aquifer_1    Palolo_Tunnel         -158.     21.3 K      \n## # ℹ 96 more rows\n## # ℹ 1 more variable: abundance <dbl>\n\nggplot(data = K_data, aes(y = aquifer_code, x = abundance)) +\n  geom_boxplot() +\n  geom_point(color = \"maroon\", alpha = 0.6, size = 3)\nK_data %>%\n  group_by(aquifer_code) %>%\n  ggplot(aes(x = abundance)) + \n    geom_histogram(bins = 30) +\n    facet_wrap(~aquifer_code) +\n    geom_density(aes(y = ..density..*10), colour = \"blue\")\nK_data %>%\n  group_by(aquifer_code) %>% \n  shapiroTest(abundance)\n## # A tibble: 10 × 4\n##    aquifer_code variable statistic         p\n##    <chr>        <chr>        <dbl>     <dbl>\n##  1 aquifer_1    values       0.885 0.102    \n##  2 aquifer_10   values       0.864 0.163    \n##  3 aquifer_2    values       0.913 0.459    \n##  4 aquifer_3    values       0.893 0.363    \n##  5 aquifer_4    values       0.948 0.421    \n##  6 aquifer_5    values       0.993 0.972    \n##  7 aquifer_6    values       0.914 0.239    \n##  8 aquifer_7    values       0.915 0.355    \n##  9 aquifer_8    values       0.842 0.220    \n## 10 aquifer_9    values       0.790 0.0000214\nK_data %>%\n  leveneTest(abundance ~ aquifer_code)\n## # A tibble: 1 × 4\n##     df1   df2 statistic       p\n##   <int> <int>     <dbl>   <dbl>\n## 1     9    96      2.95 0.00387"},{"path":"comparing-means.html","id":"anova-tukey-tests","chapter":"comparing means","heading":"ANOVA, Tukey tests","text":"use anovaTest function package rstatix. tell us means data statistically different one another. However, differences means, tell us different.pretty small p-value! definitely significant differences among group. , different one another though? , need run Tukey’s Honest Significant Difference test (implemented using tukey_hsd). essentially run t-test pairs study subjects can derive data set (example, aquifer_1 vs. aquifer_2, aquifer_1 vs. aquifer_3, etc.). , correct p-values according number comparisons performed. controls rate type error can expect test. corrected values provided us p.adj column.Using output tukey test, can determine means similar. can using pGroups function:can use output pGroups annotate plot:Excellent! plot shows us, using letters line aquifer, means different. letter shared among labels line two aquifers, means means differ significantly. example, aquifer 2 aquifer 6 “b” labels, means different - aquifers 3 10.","code":"\nK_data %>%\n  anovaTest(abundance ~ aquifer_code)\n## ANOVA Table (type II tests)\n## \n##         Effect DFn DFd     F        p p<.05   ges\n## 1 aquifer_code   9  96 9.486 3.28e-10     * 0.471\nK_data %>%\n  tukey_hsd(abundance ~ aquifer_code)\n## # A tibble: 45 × 9\n##    term         group1   group2 null.value estimate conf.low\n##  * <chr>        <chr>    <chr>       <dbl>    <dbl>    <dbl>\n##  1 aquifer_code aquifer… aquif…          0  0.00357   -2.04 \n##  2 aquifer_code aquifer… aquif…          0  1.44      -0.708\n##  3 aquifer_code aquifer… aquif…          0  0.375     -2.40 \n##  4 aquifer_code aquifer… aquif…          0 -1.15      -2.78 \n##  5 aquifer_code aquifer… aquif…          0 -0.875     -3.36 \n##  6 aquifer_code aquifer… aquif…          0  1.98       0.228\n##  7 aquifer_code aquifer… aquif…          0  2.70       0.801\n##  8 aquifer_code aquifer… aquif…          0 -0.125     -2.90 \n##  9 aquifer_code aquifer… aquif…          0 -0.349     -1.80 \n## 10 aquifer_code aquifer… aquif…          0  1.44      -0.954\n## # ℹ 35 more rows\n## # ℹ 3 more variables: conf.high <dbl>, p.adj <dbl>,\n## #   p.adj.signif <chr>\n\n# K_data %>%\n#   tukeyHSD(abundance ~ aquifer_code)\ngroups_based_on_tukey <- K_data %>%\n  tukey_hsd(abundance ~ aquifer_code) %>%\n  pGroups()\ngroups_based_on_tukey\n##             treatment group spaced_group\n## aquifer_1   aquifer_1    ab         ab  \n## aquifer_10 aquifer_10   abc         abc \n## aquifer_2   aquifer_2   acd         a cd\n## aquifer_3   aquifer_3  abcd         abcd\n## aquifer_4   aquifer_4     b          b  \n## aquifer_5   aquifer_5    ab         ab  \n## aquifer_6   aquifer_6    cd           cd\n## aquifer_7   aquifer_7     d            d\n## aquifer_8   aquifer_8  abcd         abcd\n## aquifer_9   aquifer_9    ab         ab\nggplot(data = K_data, aes(y = aquifer_code, x = abundance)) +\n  geom_boxplot() +\n  geom_point(color = \"maroon\", alpha = 0.6, size = 3) +\n  geom_text(data = groups_based_on_tukey, aes(y = treatment, x = 9, label = group))"},{"path":"comparing-means.html","id":"kruskal-dunn-tests","chapter":"comparing means","heading":"Kruskal, Dunn tests","text":"ANOVA example great, remember - data pass Shapiro Levene tests. means data can modelled normal distribution taht need use non-parametric test. non-parametric alternative ANOVA called Kruskal test. Like Wilcox test, less powerful parametric relative, meaning less likely detected differences, exist. However, since data pass Shapiro/Levene tests, resort Kruskal test. Let’s try :pretty small p-value! higher p-value running ANOVA data (remember, Kruskal test less powerful). Never less, value still well 0.05, meaning means different. , determine different one another? ran ANOVA follow-test (post hoc test) Tukey’s HSD. Kruskal test, post hoc test use Dunn test. Let’s try:gives us adjusted p-values pairwise comparisons. , can use pGroups() give us compact letter display group, can used annotate plot:Note groupings different generated ANOVA/Tukey.","code":"\nK_data %>%\n  kruskalTest(abundance ~ aquifer_code)\n## # A tibble: 1 × 6\n##   .y.           n statistic    df             p method      \n## * <chr>     <int>     <dbl> <int>         <dbl> <chr>       \n## 1 abundance   106      55.9     9 0.00000000807 Kruskal-Wal…\nK_data %>%\n  dunnTest(abundance ~ aquifer_code)\n## # A tibble: 45 × 9\n##    .y.     group1 group2    n1    n2 statistic       p p.adj\n##  * <chr>   <chr>  <chr>  <int> <int>     <dbl>   <dbl> <dbl>\n##  1 abunda… aquif… aquif…    12     7    -0.194 0.846   1    \n##  2 abunda… aquif… aquif…    12     6     2.24  0.0254  0.736\n##  3 abunda… aquif… aquif…    12     3     0.866 0.387   1    \n##  4 abunda… aquif… aquif…    12    17    -2.65  0.00806 0.266\n##  5 abunda… aquif… aquif…    12     4    -1.12  0.263   1    \n##  6 abunda… aquif… aquif…    12    12     2.51  0.0121  0.388\n##  7 abunda… aquif… aquif…    12     9     3.01  0.00257 0.100\n##  8 abunda… aquif… aquif…    12     3     0.143 0.886   1    \n##  9 abunda… aquif… aquif…    12    33    -0.470 0.639   1    \n## 10 abunda… aquif… aquif…     7     6     2.17  0.0296  0.830\n## # ℹ 35 more rows\n## # ℹ 1 more variable: p.adj.signif <chr>\ngroups_based_on_dunn <- K_data %>%\n  dunnTest(abundance ~ aquifer_code) %>%\n  pGroups()\ngroups_based_on_dunn\n##             treatment group spaced_group\n## aquifer_1   aquifer_1  abcd         abcd\n## aquifer_10 aquifer_10  abcd         abcd\n## aquifer_2   aquifer_2   abc         abc \n## aquifer_3   aquifer_3  abcd         abcd\n## aquifer_4   aquifer_4     d            d\n## aquifer_5   aquifer_5   acd         a cd\n## aquifer_6   aquifer_6    ab         ab  \n## aquifer_7   aquifer_7     b          b  \n## aquifer_8   aquifer_8  abcd         abcd\n## aquifer_9   aquifer_9    cd           cd\n\nggplot(data = K_data, aes(y = aquifer_code, x = abundance)) +\n  geom_boxplot() +\n  geom_point(color = \"black\", alpha = 0.4, size = 2) +\n  scale_x_continuous(name = \"Potassium abundance\", breaks = seq(0,10,1)) +\n  scale_y_discrete(name = \"Aquifer code\") +\n  geom_text(data = groups_based_on_dunn, aes(y = treatment, x = 9, label = group)) +\n  theme_bw()"},{"path":"comparing-means.html","id":"pairs-of-means","chapter":"comparing means","heading":"pairs of means","text":"Oftentimes two means compare, rather wanting compare means , want compare pairwise fashion. example, suppose want know aquifers contain different amounts Na Cl. interested testing differences among values Na Cl, rather, want test pairs Na Cl values arising aquifer. say, want compare means facet plot :Fortunately, can use approach similar ’ve learned earlier portions chapter, just minor modifications. Let’s look! start Shapiro Levene tests, usual (note group using two variables using Shapiro test analyte within aquifer considered individual distribution):Looks like distributions significantly different normal! Let’s run levene test anyway. Note particular case Levene test, interested testing whether pair distributions similar variances. need feed Levene test data grouped aquifer_code (tests pair group), need specify y ~ x formula (case abundance ~ analyte):looks like variances pair aquifer 1 significantly different variances. - sure need using non-parametric testing. simple case two means use wilcox_test, may pairs, use pairwise_wilcox_test (note test options various styles controlling multiple comparisons, see: ?pairwise_wilcox_test):Excellent! looks like statistically significant difference means abundances Cl Na aquifer_2 (surprisingly?) aquifer_9 (perhaps due large number observations?).done Shaprio Levene tests revealed significant differences? Well, pairwise_TTest course!Excellent, now see run parametric non-parametric pairwise comparisons. annotate plots output tests? example:","code":"\nhawaii_aquifers %>%\n  filter(analyte %in% c(\"Na\", \"Cl\")) %>%\n  ggplot(aes(x = analyte, y = abundance)) + geom_violin() + geom_point() + facet_grid(.~aquifer_code)\nhawaii_aquifers %>%\n  filter(analyte %in% c(\"Na\", \"Cl\")) %>%\n  group_by(analyte, aquifer_code) %>%\n  shapiroTest(abundance)\n## Note: Found a group with exactly 3 data points where at least two were identical.\n## A tiny bit of noise was added to these data points, because the Shapiro–Wilk test\n## p-value can be artificially low in such edge cases due to test mechanics.\n## Consider carefully whether you want to interpret the results of the test, even with noise added.\n## # A tibble: 20 × 5\n##    analyte aquifer_code variable statistic        p\n##    <chr>   <chr>        <chr>        <dbl>    <dbl>\n##  1 Cl      aquifer_1    values       0.900 1.59e- 1\n##  2 Cl      aquifer_10   values       0.486 1.09e- 5\n##  3 Cl      aquifer_2    values       0.869 2.24e- 1\n##  4 Cl      aquifer_3    values       0.750 2.42e- 6\n##  5 Cl      aquifer_4    values       0.903 7.49e- 2\n##  6 Cl      aquifer_5    values       0.849 2.24e- 1\n##  7 Cl      aquifer_6    values       0.741 2.15e- 3\n##  8 Cl      aquifer_7    values       0.893 2.12e- 1\n##  9 Cl      aquifer_8    values       0.878 3.17e- 1\n## 10 Cl      aquifer_9    values       0.420 2.68e-10\n## 11 Na      aquifer_1    values       0.886 1.06e- 1\n## 12 Na      aquifer_10   values       0.593 2.26e- 4\n## 13 Na      aquifer_2    values       0.884 2.88e- 1\n## 14 Na      aquifer_3    values       0.822 1.69e- 1\n## 15 Na      aquifer_4    values       0.933 2.41e- 1\n## 16 Na      aquifer_5    values       0.827 1.61e- 1\n## 17 Na      aquifer_6    values       0.764 3.80e- 3\n## 18 Na      aquifer_7    values       0.915 3.51e- 1\n## 19 Na      aquifer_8    values       0.855 2.53e- 1\n## 20 Na      aquifer_9    values       0.531 3.97e- 9\nhawaii_aquifers %>%\n  filter(analyte %in% c(\"Na\", \"Cl\")) %>%\n  group_by(aquifer_code) %>%\n  leveneTest(abundance ~ analyte)\n## # A tibble: 10 × 5\n##    aquifer_code   df1   df2 statistic       p\n##    <chr>        <int> <int>     <dbl>   <dbl>\n##  1 aquifer_1        1    22   10.5    0.00375\n##  2 aquifer_10       1    12    0.0535 0.821  \n##  3 aquifer_2        1    10    0.0243 0.879  \n##  4 aquifer_3        1     4    0.320  0.602  \n##  5 aquifer_4        1    32    1.57   0.219  \n##  6 aquifer_5        1     6    2      0.207  \n##  7 aquifer_6        1    22    1.03   0.322  \n##  8 aquifer_7        1    16    1.54   0.232  \n##  9 aquifer_8        1     4    0.515  0.512  \n## 10 aquifer_9        1    64    1.10   0.298\nhawaii_aquifers %>%\n  filter(analyte %in% c(\"Na\", \"Cl\")) %>%\n  group_by(aquifer_code) %>%\n  pairwiseWilcoxTest(abundance~analyte)\n## # A tibble: 10 × 10\n##    aquifer_code .y.      group1 group2    n1    n2 statistic\n##  * <chr>        <chr>    <chr>  <chr>  <int> <int>     <dbl>\n##  1 aquifer_1    abundan… Cl     Na        12    12      99.5\n##  2 aquifer_10   abundan… Cl     Na         7     7      14  \n##  3 aquifer_2    abundan… Cl     Na         6     6      36  \n##  4 aquifer_3    abundan… Cl     Na         3     3       3  \n##  5 aquifer_4    abundan… Cl     Na        17    17     189  \n##  6 aquifer_5    abundan… Cl     Na         4     4      13  \n##  7 aquifer_6    abundan… Cl     Na        12    12      53  \n##  8 aquifer_7    abundan… Cl     Na         9     9      42  \n##  9 aquifer_8    abundan… Cl     Na         3     3       6  \n## 10 aquifer_9    abundan… Cl     Na        33    33     195  \n## # ℹ 3 more variables: p <dbl>, p.adj <dbl>,\n## #   p.adj.signif <chr>\nhawaii_aquifers %>%\n  filter(analyte %in% c(\"Na\", \"Cl\")) %>%\n  group_by(aquifer_code) %>%\n  pairwiseTTest(abundance~analyte) -> test_output\n  test_output\n## # A tibble: 10 × 10\n##    aquifer_code .y.       group1 group2    n1    n2        p\n##  * <chr>        <chr>     <chr>  <chr>  <int> <int>    <dbl>\n##  1 aquifer_1    abundance Cl     Na        12    12  4.69e-2\n##  2 aquifer_10   abundance Cl     Na         7     7  8.82e-1\n##  3 aquifer_2    abundance Cl     Na         6     6  3.75e-5\n##  4 aquifer_3    abundance Cl     Na         3     3  6.83e-1\n##  5 aquifer_4    abundance Cl     Na        17    17  1.03e-1\n##  6 aquifer_5    abundance Cl     Na         4     4  9.75e-2\n##  7 aquifer_6    abundance Cl     Na        12    12  5.66e-1\n##  8 aquifer_7    abundance Cl     Na         9     9  5.21e-1\n##  9 aquifer_8    abundance Cl     Na         3     3  4.28e-1\n## 10 aquifer_9    abundance Cl     Na        33    33  8.96e-1\n## # ℹ 3 more variables: p.signif <chr>, p.adj <dbl>,\n## #   p.adj.signif <chr>\n\nanno <- data.frame(\n  xmin = test_output$group1,\n  xmax = test_output$group2,\n  y_position = c(150, 150, 150, 175, 80, 50, 300, 150, 50, 125),\n  text = test_output$p.signif,\n  text_size = 10,\n  text_vert_offset = 10,\n  text_horiz_offset = 1.5,\n  tip_length_xmin = 5,\n  tip_length_xmax = 5,\n  aquifer_code = test_output$aquifer_code,\n  hjust = 0.5,\n  vjust = 0.5\n)\n\nhawaii_aquifers %>%\n  filter(analyte %in% c(\"Na\", \"Cl\")) %>%\n  ggplot(aes(x = analyte, y = abundance)) +\n  geom_violin(fill = \"gold\", color = \"black\") +\n  geom_point(shape = 21, fill = \"maroon\", color = \"black\") +\n  facet_grid(.~aquifer_code) +\n  geomSignif(data = anno, orientation = \"horizontal\") +\n  scale_x_discrete(name = \"Analyte\") +\n  scale_y_continuous(name = \"Abundance\") +\n  theme_bw() +\n  theme(\n    text = element_text(size = 16)\n    )"},{"path":"comparing-means.html","id":"section-11","chapter":"comparing means","heading":"","text":"","code":""},{"path":"comparing-means.html","id":"further-reading-5","chapter":"comparing means","heading":"further reading","text":"comparing multiple means R: www.datanovia.comFor parametric versus non-parametric tests: Statistics JimFor interpreting p values: [p value wars () Ulrich Dirnagl]Ten common statistical mistakes solutions: Science Forum: Ten common statistical mistakes watch writing reviewing manuscriptHow think small p-values: Reporting p Values, Wolfgang Huber","code":""},{"path":"flat-clustering.html","id":"flat-clustering","chapter":"flat clustering","heading":"flat clustering","text":"“samples fall definable clusters?”","code":""},{"path":"flat-clustering.html","id":"section-12","chapter":"flat clustering","heading":"","text":"","code":""},{"path":"flat-clustering.html","id":"kmeans","chapter":"flat clustering","heading":"kmeans","text":"One questions ’ve asking “samples closely related?”. ’ve answering question using clustering. However, now know run principal components analyses, can use another approach. alternative approach called k-means, can help us decide assign data clusters. generally desirable small number clusters, however, must balanced variance within cluster big. strike balance point, elbow method used. , must first determine maximum within-group variance possible number clusters. illustration shown :One know within-group variances, find “elbow” point - point minimum angle theta - thus picking outcome good balance cluster number within-cluster variance (illustrated B C.)Let’s try k-means using runMatrixAnalysis. example, let’s run PCA projection alaska lakes data set. can set analysis = \"kmeans\". , application load show us threshold value number clusters want. set number clusters close app. context markdown document, simply provide number clusters parameters argument:can plot results color according group kmeans suggested. can also highlight groups using geom_mark_ellipse. Note recommended specify fill label geom_mark_ellipse:","code":"\nalaska_lake_data_pca <- runMatrixAnalysis(\n    data = alaska_lake_data,\n    analysis = c(\"pca\"),\n    column_w_names_of_multiple_analytes = \"element\",\n    column_w_values_for_multiple_analytes = \"mg_per_L\",\n    columns_w_values_for_single_analyte = c(\"water_temp\", \"pH\"),\n    columns_w_additional_analyte_info = \"element_type\",\n    columns_w_sample_ID_info = c(\"lake\", \"park\")\n)\n\nalaska_lake_data_pca_clusters <- runMatrixAnalysis(\n    data = alaska_lake_data_pca,\n    analysis = c(\"kmeans\"),\n    parameters = c(5),\n    column_w_names_of_multiple_analytes = NULL,\n    column_w_values_for_multiple_analytes = NULL,\n    columns_w_values_for_single_analyte = c(\"Dim.1\", \"Dim.2\"),\n    columns_w_sample_ID_info = \"sample_unique_ID\"\n)\n\nalaska_lake_data_pca_clusters <- left_join(alaska_lake_data_pca_clusters, alaska_lake_data_pca) \nalaska_lake_data_pca_clusters$cluster <- factor(alaska_lake_data_pca_clusters$cluster)\nggplot() +\n  geom_point(\n    data = alaska_lake_data_pca_clusters,\n    aes(x = Dim.1, y = Dim.2, fill = cluster), shape = 21, size = 5, alpha = 0.6\n  ) +\n  geom_mark_ellipse(\n    data = alaska_lake_data_pca_clusters,\n    aes(x = Dim.1, y = Dim.2, label = cluster, fill = cluster), alpha = 0.2\n  ) +\n  theme_classic() +\n  coord_cartesian(xlim = c(-7,12), ylim = c(-4,5)) +\n  scale_fill_manual(values = discrete_palette) "},{"path":"flat-clustering.html","id":"dbscan","chapter":"flat clustering","heading":"dbscan","text":"another method define clusters call dbscan. method, points necessarily assigned cluster, define clusters according set parameters, instead simply defining number clusteres, kmeans. interactive mode, runMatrixAnalysis() load interactive means selecting parameters defining dbscan clusters (“k”, “threshold”). context markdown document, simply provide “k” “threshold” parameters argument:can make plot way, please note get geom_mark_ellipse omit ellipse NAs need feed data without NAs:","code":"\nalaska_lake_data_pca <- runMatrixAnalysis(\n    data = alaska_lake_data,\n    analysis = c(\"pca\"),\n    column_w_names_of_multiple_analytes = \"element\",\n    column_w_values_for_multiple_analytes = \"mg_per_L\",\n    columns_w_values_for_single_analyte = c(\"water_temp\", \"pH\"),\n    columns_w_additional_analyte_info = \"element_type\",\n    columns_w_sample_ID_info = c(\"lake\", \"park\")\n) \n\nalaska_lake_data_pca_clusters <- runMatrixAnalysis(\n    data = alaska_lake_data_pca,\n    analysis = c(\"dbscan\"),\n    parameters = c(4, 0.45),\n    column_w_names_of_multiple_analytes = NULL,\n    column_w_values_for_multiple_analytes = NULL,\n    columns_w_values_for_single_analyte = c(\"Dim.1\", \"Dim.2\"),\n    columns_w_sample_ID_info = \"sample_unique_ID\"\n)\n\nalaska_lake_data_pca_clusters <- left_join(alaska_lake_data_pca_clusters, alaska_lake_data_pca)\nalaska_lake_data_pca_clusters$cluster <- factor(alaska_lake_data_pca_clusters$cluster)\nggplot() +\n  geom_point(\n    data = alaska_lake_data_pca_clusters,\n    aes(x = Dim.1, y = Dim.2, fill = cluster), shape = 21, size = 5, alpha = 0.6\n  ) +\n  geom_mark_ellipse(\n    data = drop_na(alaska_lake_data_pca_clusters),\n    aes(x = Dim.1, y = Dim.2, label = cluster, fill = cluster), alpha = 0.2\n  ) +\n  theme_classic() +\n  coord_cartesian(xlim = c(-7,12), ylim = c(-4,5)) +\n  scale_fill_manual(values = discrete_palette) "},{"path":"flat-clustering.html","id":"summarize-by-cluster","chapter":"flat clustering","heading":"summarize by cluster","text":"One important point: using kmeans dbscan, can use clusters groupings summary statistics. example, suppose want see differences abundances certain chemicals among clusters:","code":"\nalaska_lake_data_pca <- runMatrixAnalysis(\n  data = alaska_lake_data,\n  analysis = c(\"pca\"),\n  column_w_names_of_multiple_analytes = \"element\",\n  column_w_values_for_multiple_analytes = \"mg_per_L\",\n  columns_w_values_for_single_analyte = c(\"water_temp\", \"pH\"),\n  columns_w_additional_analyte_info = \"element_type\",\n  columns_w_sample_ID_info = c(\"lake\", \"park\")\n)\n\nalaska_lake_data_pca_clusters <- runMatrixAnalysis(\n  data = alaska_lake_data_pca,\n  analysis = c(\"dbscan\"),\n  parameters = c(4, 0.45),\n  column_w_names_of_multiple_analytes = NULL,\n  column_w_values_for_multiple_analytes = NULL,\n  columns_w_values_for_single_analyte = c(\"Dim.1\", \"Dim.2\"),\n  columns_w_sample_ID_info = \"sample_unique_ID\",\n  columns_w_additional_analyte_info = colnames(alaska_lake_data_pca)[6:18]\n) \n\nalaska_lake_data_pca_clusters <- left_join(alaska_lake_data_pca_clusters, alaska_lake_data_pca)\n\nalaska_lake_data_pca_clusters %>%\n  select(cluster, S, Ca) %>%\n  pivot_longer(cols = c(2,3), names_to = \"analyte\", values_to = \"mg_per_L\") %>%\n  drop_na() %>%\n  group_by(cluster, analyte) -> alaska_lake_data_pca_clusters_clean\n\nplot_2 <- ggplot() + \n  geom_col(\n    data = summarize(\n      alaska_lake_data_pca_clusters_clean,\n      mean = mean(mg_per_L), sd = sd(mg_per_L)\n    ),\n    aes(x = cluster, y = mean, fill = cluster),\n    color = \"black\", alpha = 0.6\n  ) +\n  geom_errorbar(\n    data = summarize(\n      alaska_lake_data_pca_clusters_clean,\n      mean = mean(mg_per_L), sd = sd(mg_per_L)\n    ),\n    aes(x = cluster, ymin = mean-sd, ymax = mean+sd, fill = cluster),\n    color = \"black\", alpha = 0.6, width = 0.5, size = 1\n  ) +\n  facet_grid(.~analyte) + theme_bw() +\n  geom_jitter(\n    data = alaska_lake_data_pca_clusters_clean,\n    aes(x = cluster, y = mg_per_L, fill = cluster), width = 0.05,\n    shape = 21\n  ) +\n  scale_fill_manual(values = discrete_palette) \n\nplot_1<- ggplot() +\n  geom_point(\n    data = alaska_lake_data_pca_clusters,\n    aes(x = Dim.1, y = Dim.2, fill = cluster), shape = 21, size = 5, alpha = 0.6\n  ) +\n  geom_mark_ellipse(\n    data = drop_na(alaska_lake_data_pca_clusters),\n    aes(x = Dim.1, y = Dim.2, label = cluster, fill = cluster), alpha = 0.2\n  ) +\n  theme_classic() + coord_cartesian(xlim = c(-7,12), ylim = c(-4,5)) +\n  scale_fill_manual(values = discrete_palette)\n\nplot_1 + plot_2"},{"path":"flat-clustering.html","id":"section-13","chapter":"flat clustering","heading":"","text":"","code":""},{"path":"flat-clustering.html","id":"further-reading-6","chapter":"flat clustering","heading":"further reading","text":"http://www.sthda.com/english/wiki/wiki.php?id_contents=7940https://ryanwingate.com/intro--machine-learning/unsupervised/hierarchical--density-based-clustering/https://ryanwingate.com/intro--machine-learning/unsupervised/hierarchical--density-based-clustering/hierarchical-4.pnghttps://www.geeksforgeeks.org/dbscan-clustering--r-programming/","code":""},{"path":"numerical-models.html","id":"numerical-models","chapter":"numerical models","heading":"numerical models","text":"","code":""},{"path":"numerical-models.html","id":"model-use","chapter":"numerical models","heading":"model use","text":"Next quest develop abilities analytical data exploration modeling. discuss two main ways use numerical models: inferential uses predictive uses.Inferential uses aim understand quantify relationships variables, focusing significance, direction, strength relationships draw conclusions data. using inferential models care lot exact inner workings model inner workings understand relationships variables.Inferential uses aim understand quantify relationships variables, focusing significance, direction, strength relationships draw conclusions data. using inferential models care lot exact inner workings model inner workings understand relationships variables.Predictive uses designed primary goal forecasting future outcomes behaviors based historical data. using predictive models often care much less exact inner workings model instead care accuracy. words, don’t really care model works long accurate.Predictive uses designed primary goal forecasting future outcomes behaviors based historical data. using predictive models often care much less exact inner workings model instead care accuracy. words, don’t really care model works long accurate.preceding section, explored inferential models, highlighting significance grasping quantitative aspects model’s mechanisms order understand relationships variables. Now look predictive models. Unlike inferential models, predictive models typically complex, often making challenging fully comprehend internal processes. okay though, using predictive models usually care predictive accuracy, willing sacrifice quantitative understanding model’s inner workings achieve higher accuracy.Interestingly, increasingly complex predictive models always higher accuracy. model complex say model ‘overfitted’ means model capturing noise random fluctuations input data using erroneous patterns predictions. hand, model complex enough able capture important true patterns data required make accurate predictions. means build models right level complexity.build model appropriate level complexity usually use process: () separate 80% data call training data, (ii) build series models varying complexity using training data, (iii) use models make predictions remaining 20% data (testing data), (iv) whichever model best predictive accuracy remaining 20% model appropriate level complexity.course, build types models using function called buildModel. use , simply give data, tell sets values want compare. tell want compare, need specify (least) two things:input_variables: input variables (sometimes called “features” “predictors”) model use inputs prediction. Depending model, continuous categorical variables.input_variables: input variables (sometimes called “features” “predictors”) model use inputs prediction. Depending model, continuous categorical variables.output_variable: variable model predict. Depending model, continuous value (regression) category/class (classification).output_variable: variable model predict. Depending model, continuous value (regression) category/class (classification).model building, also need talk handling missing data. missing data data set, need one way forward impute . means need fill missing values something. many ways , use median value column. can using impute function rstatix package. Let’s now:","code":"\nany(is.na(metabolomics_data))\n## [1] TRUE\nmetabolomics_data[] <- lapply(metabolomics_data, function(x) Hmisc::impute(x, median(x, na.rm = TRUE)))\nany(is.na(metabolomics_data))\n## [1] FALSE"},{"path":"numerical-models.html","id":"single-linear-regression","chapter":"numerical models","heading":"single linear regression","text":"start simplest models - linear models. variety ways build linear models R, use buildModel, mentioned . First, use least squares regression model relationship input output variables. Suppose want know abundances ADP AMP related metabolomics dataset:looks like might relationship! Let’s build inferential model examine details relationship:output buildModel consists three things: model_type, model , metric describing certain aspects model /performance. Let’s look model:linear model stored inside special object type inside R called lm. can bit tricky work , way make easier - ’ll look second. , let’s look metrics.shows us r-squared, total residual sum squares, intercept (b y = mx + b), coefficient AMP (.e. slope, m), well things (talk second).can also use function called predictWithModel make predictions using model. Let’s try ADP AMP. give model, tell values want predict . case, want predict abundance ADP value AMP data set. can like :, predictWithModel using model predict AMP values ADP. However, note measured AMP values data set. can compare predicted values measured values see well model . can like :good. Now let’s talk evaluating quality model. need means assessing well line fits data. use residuals - distance points line.can calculate sum squared residuals:Cool! Let’s call “residual sum squares”. … mean model good? don’t know. compare number something. Let’s compare super simple model just defined mean y value input data.pretty bad model, agree. much better linear model flat line model? Let’s create measure distance point point predicted x value model:Cool. Let’s call “total sum squares”, now can compare “residual sum squares”:Alright. R squared value. equal 1 minus ratio “residual sum squares” “total sum squares”. can think R squared value :\n- amount variance response explained dependent variable.\n- much better line best fit describes data flat line.\nNow, let’s put together make pretty:","code":"\nggplot(metabolomics_data) +\n  geom_point(aes(x = ADP, y = AMP))\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous.\nbasic_regression_model <- buildModel2(\n  data = metabolomics_data,\n  model_type = \"linear_regression\",\n  input_variables = \"ADP\",\n  output_variable = \"AMP\"\n)\nnames(basic_regression_model)\n## [1] \"model_type\" \"model\"      \"metrics\"\nbasic_regression_model$model\n## \n## Call:\n## lm(formula = formula, data = data, model = TRUE, x = TRUE, y = TRUE, \n##     qr = TRUE)\n## \n## Coefficients:\n## (Intercept)          ADP  \n##      4.5764       0.6705\nbasic_regression_model$metrics\n##               variable   value std_err        type  p_value\n## 1            r_squared  0.4693      NA   statistic       NA\n## 2    total_sum_squares 38.6346      NA   statistic       NA\n## 3 residual_sum_squares 20.5024      NA   statistic       NA\n## 4          (Intercept)  4.5764  0.9667 coefficient 8.04e-06\n## 5                  ADP  0.6705  0.0747 coefficient 0.00e+00\n##   p_value_adj\n## 1          NA\n## 2          NA\n## 3          NA\n## 4    8.04e-06\n## 5    0.00e+00\nAMP_values_predicted_from_ADP_values <- predictWithModel(\n  data = metabolomics_data,\n  model_type = \"linear_regression\",\n  model = basic_regression_model$model\n)\nhead(AMP_values_predicted_from_ADP_values)\n## # A tibble: 6 × 1\n##   value\n##   <dbl>\n## 1  13.2\n## 2  13.4\n## 3  13.5\n## 4  13.4\n## 5  12.6\n## 6  13.2\nADP_values <- metabolomics_data$ADP\n\npredictions_from_basic_linear_model <- data.frame(\n    ADP_values = ADP_values,\n    AMP_values_predicted_from_ADP_values = AMP_values_predicted_from_ADP_values,\n    AMP_values_measured = metabolomics_data$AMP\n)\ncolnames(predictions_from_basic_linear_model)[2] <- \"AMP_values_predicted_from_ADP_values\"\n\nplot1 <- ggplot() +\n    geom_line(\n        data = predictions_from_basic_linear_model,\n        aes(x = ADP_values, y = AMP_values_predicted_from_ADP_values), color = \"red\"\n    ) +\n    geom_point(\n        data = predictions_from_basic_linear_model,\n        aes(x = ADP_values, y = AMP_values_predicted_from_ADP_values), color = \"red\"\n    ) +\n    geom_point(\n        data = metabolomics_data,\n        aes(x = ADP, y = AMP), color = \"blue\"\n    )\nplot1\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous.\nggplot(predictions_from_basic_linear_model) +\n  geom_point(aes(x = ADP_values, y = AMP_values_measured)) +\n  geom_line(aes(x = ADP_values, y = AMP_values_predicted_from_ADP_values)) +\n  geom_segment(aes(x = ADP_values, y = AMP_values_measured, xend = ADP_values, yend = AMP_values_predicted_from_ADP_values))\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous.\nsum(\n  (predictions_from_basic_linear_model$AMP_values_measured - predictions_from_basic_linear_model$AMP_values_predicted_from_ADP_values)^2\n, na.rm = TRUE)\n## [1] 20.50235\nggplot(metabolomics_data) +\n  geom_point(aes(x = ADP, y = AMP)) +\n  geom_hline(aes(yintercept = mean(AMP, na.rm = TRUE)))\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous.\nggplot(metabolomics_data) +\n  geom_point(aes(x = ADP, y = AMP)) +\n  geom_hline(aes(yintercept = mean(ADP, na.rm = TRUE))) +\n  geom_segment(aes(x = ADP, y = AMP, xend = ADP, yend = mean(ADP, na.rm = TRUE)))\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous.\n\nsum(\n  (metabolomics_data$AMP - mean(metabolomics_data$AMP, na.rm = TRUE))^2\n, na.rm = TRUE)\n## [1] 38.63462\n1-(20.1904/38.63)\n## [1] 0.4773389\ntop <- ggplot() +\n    geom_line(\n        data = predictions_from_basic_linear_model,\n        aes(x = ADP_values, y = AMP_values_predicted_from_ADP_values), color = \"red\"\n    ) +\n    geom_point(\n        data = predictions_from_basic_linear_model,\n        aes(x = ADP_values, y = AMP_values_predicted_from_ADP_values), color = \"red\"\n    ) +\n    geom_point(\n        data = metabolomics_data,\n        aes(x = ADP, y = AMP), color = \"blue\"\n    ) +\n    annotate(geom = \"table\",\n      x = 10,\n      y = 15,\n      label = list(select(basic_regression_model$metrics, variable, value))\n    ) +\n    coord_cartesian(ylim = c(10,16)) +\n    theme_bw()\n\nbottom <- ggplot(predictions_from_basic_linear_model) +\n  geom_col(\n    aes(x = ADP_values, y = AMP_values_measured-AMP_values_predicted_from_ADP_values),\n    width = 0.03, color = \"black\", position = \"dodge\", alpha = 0.5\n  ) +\n  theme_bw()\n\ncowplot::plot_grid(top, bottom, ncol = 1, labels = \"AUTO\", rel_heights = c(2,1))\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous.\n## Don't know how to automatically pick scale for object of\n## type <impute>. Defaulting to continuous."},{"path":"numerical-models.html","id":"multiple-linear-regression","chapter":"numerical models","heading":"multiple linear regression","text":"Cool! Now let’s try multiple linear regression model. simple linear regression model, one predictor variable. Simple multiple linear regression statistical methods used explore relationship one independent variables (predictor variables) dependent variable (outcome variable). Simple linear regression involves one independent variable predict value one dependent variable, utilizing linear equation form y = mx + b. Multiple linear regression extends concept include two independent variables, typical form y = m1x1 + m2x2 + … + b, allowing complex representation relationships among variables. simple linear regression provides straight-line relationship independent dependent variables, multiple linear regression can model multi-dimensional plane variable space, providing nuanced understanding independent variables collectively influence dependent variable. complexity multiple linear regression can offer accurate predictions insights, especially scenarios variables interact interdependent, although also requires careful consideration assumptions potential multicollinearity among independent variables. Let’s try first 30 metabolites data set:","code":"\n\nbasic_regression_model <- buildModel2(\n  data = metabolomics_data,\n  model_type = \"linear_regression\",\n  input_variables = \"ADP\",\n  output_variable = \"AMP\"\n)\n\nmultiple_regression_model <- buildModel2(\n  data = metabolomics_data,\n  model_type = \"linear_regression\",\n  input_variables = colnames(metabolomics_data)[3:33],\n  output_variable = \"AMP\"\n)\n\nggplot() +\n  geom_point(\n    data = metabolomics_data,\n    aes(x = ADP, y = AMP), fill = \"gold\", shape = 21, color = \"black\"\n  ) +\n  geom_line(aes(\n    x = metabolomics_data$ADP,\n    y = mean(metabolomics_data$AMP)\n  ), color = \"grey\") +\n  geom_line(aes(\n    x = metabolomics_data$ADP,\n    y = predictWithModel(\n      data = metabolomics_data,\n      model_type = \"linear_regression\",\n      model = basic_regression_model$model\n    )),\n    color = \"maroon\", size = 1\n  ) +\n  geom_line(aes(\n    x = metabolomics_data$ADP,\n    y = predictWithModel(\n      data = metabolomics_data,\n      model_type = \"linear_regression\",\n      model = multiple_regression_model$model\n    )),\n    color = \"black\", size = 1, alpha = 0.6\n  ) +\n  theme_bw()"},{"path":"numerical-models.html","id":"assessing-regression-models","chapter":"numerical models","heading":"assessing regression models","text":"three aspects regression model check , make sure model isn’t violating assumptions make declaring model valid:Residual-Based Assumptions. checks specifically behavior residuals, differences observed values model predictions. group includes:\nLinearity: Examines residuals show random scatter around zero, indicating linear relationship predictors response variable. Patterns curves plot may indicate model adequately capture true relationship, suggesting potential non-linearity.\nHomoscedasticity (Homogeneity Variance): Looks spread residuals confirm variance consistent across fitted values. words, spread residuals uniform regardless fitted values. assess homoscedasticity, residuals plotted fitted values. horizontal band residuals indicates variance consistent, supporting homoscedasticity assumption. Conversely, patterns funnel shape, residuals spread contract fitted values increase, suggest heteroscedasticity, indicating variance errors changes level independent variables.\nNormality Residuals: Assesses whether residuals follow normal distribution, crucial statistical inference regression. check characteristic, Quantile-Quantile (Q-Q) plot used, residuals plotted theoretical normal distribution. residuals normally distributed, points align closely along straight line. Significant deviations line indicate departures normality, may affect reliability statistical inferences drawn model.\nResidual-Based Assumptions. checks specifically behavior residuals, differences observed values model predictions. group includes:Linearity: Examines residuals show random scatter around zero, indicating linear relationship predictors response variable. Patterns curves plot may indicate model adequately capture true relationship, suggesting potential non-linearity.Linearity: Examines residuals show random scatter around zero, indicating linear relationship predictors response variable. Patterns curves plot may indicate model adequately capture true relationship, suggesting potential non-linearity.Homoscedasticity (Homogeneity Variance): Looks spread residuals confirm variance consistent across fitted values. words, spread residuals uniform regardless fitted values. assess homoscedasticity, residuals plotted fitted values. horizontal band residuals indicates variance consistent, supporting homoscedasticity assumption. Conversely, patterns funnel shape, residuals spread contract fitted values increase, suggest heteroscedasticity, indicating variance errors changes level independent variables.Homoscedasticity (Homogeneity Variance): Looks spread residuals confirm variance consistent across fitted values. words, spread residuals uniform regardless fitted values. assess homoscedasticity, residuals plotted fitted values. horizontal band residuals indicates variance consistent, supporting homoscedasticity assumption. Conversely, patterns funnel shape, residuals spread contract fitted values increase, suggest heteroscedasticity, indicating variance errors changes level independent variables.Normality Residuals: Assesses whether residuals follow normal distribution, crucial statistical inference regression. check characteristic, Quantile-Quantile (Q-Q) plot used, residuals plotted theoretical normal distribution. residuals normally distributed, points align closely along straight line. Significant deviations line indicate departures normality, may affect reliability statistical inferences drawn model.Normality Residuals: Assesses whether residuals follow normal distribution, crucial statistical inference regression. check characteristic, Quantile-Quantile (Q-Q) plot used, residuals plotted theoretical normal distribution. residuals normally distributed, points align closely along straight line. Significant deviations line indicate departures normality, may affect reliability statistical inferences drawn model.Predictor Relationships: check pertains relationships among predictor/input variables , rather relationship response/output variable. case:\nCollinearity: Assesses multicollinearity, occurs predictors highly correlated . can lead inflated variances regression coefficients, making challenging attribute effects individual predictors. check helps ensure predictors independent enough provide clear, interpretable results variable’s influence response.\nPredictor Relationships: check pertains relationships among predictor/input variables , rather relationship response/output variable. case:Collinearity: Assesses multicollinearity, occurs predictors highly correlated . can lead inflated variances regression coefficients, making challenging attribute effects individual predictors. check helps ensure predictors independent enough provide clear, interpretable results variable’s influence response.Model Fit Influence: checks look overall fit model assess specific data points undue influence model’s results. group includes:\nPosterior Predictive Check: checks model predictions align well observed data, indicating good overall fit. directly residual analysis, ’s comprehensive check well model captures data patterns.\nInfluential Observations: Identifies data points might disproportionately affect model. High-leverage points can distort model estimates, ’s important verify single observation overly influential.\nModel Fit Influence: checks look overall fit model assess specific data points undue influence model’s results. group includes:Posterior Predictive Check: checks model predictions align well observed data, indicating good overall fit. directly residual analysis, ’s comprehensive check well model captures data patterns.Posterior Predictive Check: checks model predictions align well observed data, indicating good overall fit. directly residual analysis, ’s comprehensive check well model captures data patterns.Influential Observations: Identifies data points might disproportionately affect model. High-leverage points can distort model estimates, ’s important verify single observation overly influential.Influential Observations: Identifies data points might disproportionately affect model. High-leverage points can distort model estimates, ’s important verify single observation overly influential.can assess using:","code":"\nmultiple_regression_model <- buildModel2(\n  data = metabolomics_data,\n  model_type = \"linear_regression\",\n  input_variables = colnames(metabolomics_data)[3:10],\n  output_variable = \"AMP\"\n)\n\ncheck_model(multiple_regression_model$model)"},{"path":"numerical-models.html","id":"random-forests","chapter":"numerical models","heading":"random forests","text":"Random forests collections decision trees can used predicting categorical variables (.e. ‘classification’ task) predicting numerical variables (‘regression’ task). Random forests built constructing multiple decision trees, using randomly selected subset training data, ensure diversity among trees. node tree, number input variables randomly chosen candidates splitting data, introducing randomness beyond data sampling. Among variables randomly selected candidates splitting node, one chosen node based criterion, maximizing purity tree’s output, minimizing mean squared error regression tasks, guiding construction robust ensemble model. forest’s final prediction derived either averaging outputs (regression) majority vote (classification).can use buildModel() function make random forest model. need specify data, model type, input output variables, case random forest model, also need provide list optimization parameters: n_vars_tried_at_split, n_trees, min_n_leaves. information parameters:n_vars_at_split (often called “mtry” implementations): parameter specifies number variables randomly sampled candidate features split point construction tree. main idea behind selecting subset features (variables) introduce randomness model, helps making model robust less likely overfit training data. trying different numbers features, model can find right level complexity, leading generalized predictions. smaller value n_vars_at_split increases randomness forest, potentially increasing bias decreasing variance. Conversely, larger mtry value makes model resemble bagged ensemble decision trees, potentially reducing bias increasing variance.n_vars_at_split (often called “mtry” implementations): parameter specifies number variables randomly sampled candidate features split point construction tree. main idea behind selecting subset features (variables) introduce randomness model, helps making model robust less likely overfit training data. trying different numbers features, model can find right level complexity, leading generalized predictions. smaller value n_vars_at_split increases randomness forest, potentially increasing bias decreasing variance. Conversely, larger mtry value makes model resemble bagged ensemble decision trees, potentially reducing bias increasing variance.n_trees (often referred “num.trees” “n_estimators” implementations): parameter defines number trees grown random forest. individual tree predicts outcome based subset features considers, final prediction typically mode (classification tasks) average (regression tasks) individual tree predictions. Increasing number trees generally improves model’s performance averages predictions, tends reduce overfitting makes model stable. However, beyond certain point, adding trees offers diminishing returns terms performance improvement can significantly increase computational cost memory usage without substantial gains.n_trees (often referred “num.trees” “n_estimators” implementations): parameter defines number trees grown random forest. individual tree predicts outcome based subset features considers, final prediction typically mode (classification tasks) average (regression tasks) individual tree predictions. Increasing number trees generally improves model’s performance averages predictions, tends reduce overfitting makes model stable. However, beyond certain point, adding trees offers diminishing returns terms performance improvement can significantly increase computational cost memory usage without substantial gains.min_n_leaves (often referred “min_n” implementations, default value 1): parameter sets minimum number samples must present node split . Increasing value makes tree random forest less complex reducing depth trees, leading larger, generalized leaf nodes. can help prevent overfitting ensuring trees grow deep specific training data. carefully tuning parameter, can strike balance model’s ability capture underlying patterns data generalization unseen data.min_n_leaves (often referred “min_n” implementations, default value 1): parameter sets minimum number samples must present node split . Increasing value makes tree random forest less complex reducing depth trees, leading larger, generalized leaf nodes. can help prevent overfitting ensuring trees grow deep specific training data. carefully tuning parameter, can strike balance model’s ability capture underlying patterns data generalization unseen data.buildModel() configured allow explore number settings n_vars_at_split n_trees, pick combination highest predictive accuracy. function:data specifies dataset used model training, metabolomics_data.model_type defines type model build, “random_forest_regression” indicating random forest model regression tasks.input_variables selects features predictors model, using columns 3 33 metabolomics_data predictors.output_variable target variable prediction, case, “AMP”.optimization_parameters argument takes list define grid parameters optimization, including n_vars_tried_at_split, n_trees, min_leaf_size. seq() function generates sequences numbers used create ranges parameter:n_vars_tried_at_split = seq(1,24,3) generates sequence number variables tried split, starting 1, ending 24, steps 3 (e.g., 1, 4, 7, …, 24).n_trees = seq(1,40,2) creates sequence number trees forest, 1 40 steps 2.min_leaf_size = seq(1,3,1) defines minimal size leaf nodes, ranging 1 3 steps 1.setup creates grid parameter combinations combination n_vars_tried_at_split, n_trees, min_leaf_size defines unique random forest model. function test combination within grid identify model performs best according given evaluation criterion, effectively searching defined parameter space optimize random forest’s performance. approach allows systematic exploration different configurations affect model’s ability predict output variable, enabling selection effective model configuration based dataset task hand.code builds random forest model. ’s output provides model key components indicating performance configuration model. ’s breakdown part output:$model_type tells us type model .$model shows configuration best random forest model created.$metrics provides detailed results model performance across different combinations random forest parameters n_vars_tried_at_split (number variables randomly sampled candidates split) n_trees (number trees forest). combination, shows:\n- n_vars_tried_at_split n_trees: specific values used model configuration.\n- .metric: performance metric used, ’s accuracy, measures often model correctly predicts patient status.\n- .estimator: Indicates type averaging used metric, ’s binary binary classification tasks.\n- mean: average accuracy across cross-validation folds.\n- fold_cross_validation: Indicates number folds used cross-validation, ’s 3 models.\n- std_err: standard error mean accuracy, providing idea variability model performance.\n- .config: unique identifier model configuration tested.can thus inspect performance model based specific parameters used configuration. can help us understand exploring right parameter space - good values n_vars_tried_at_split n_trees? case regression, performance metric reported RMSE: root mean squared error. want value small! smaller values metric indicate better model.can easily use model make predictions using predictWithModel() function:addition regression modeling, random forests can also used classification modeling. classification modeling, trying predict categorical outcome variable set predictor variables. example, might want predict whether patient disease based metabolomics data. set model_type “random_forest_classification” instead “random_forest_regression”. Let’s try now:Cool! best settings lead model 90% accuracy! can also make predictions unknown data model:","code":"\nrandom_forest_model <- buildModel2(\n    data = metabolomics_data,\n    model_type = \"random_forest_regression\",\n    input_variables = colnames(metabolomics_data)[3:33],\n    output_variable = \"AMP\",\n    optimization_parameters = list(\n      n_vars_tried_at_split = seq(1,24,3),\n      n_trees = seq(1,40,2),\n      min_leaf_size = seq(1,3,1)\n    )\n)\n\nnames(random_forest_model)\n## [1] \"metrics\" \"model\"\nrandom_forest_model$metrics %>%\n    ggplot(aes(x = mtry, y = trees, fill = mean)) +\n    facet_grid(.~min_n) +\n    scale_fill_viridis(direction = -1) +\n    geom_tile() +\n    theme_bw()\nggplot() +\n  geom_point(\n    data = metabolomics_data,\n    aes(x = ADP, y = AMP), fill = \"gold\", shape = 21, color = \"black\"\n  ) +\n  geom_line(aes(\n    x = metabolomics_data$ADP,\n    y = mean(metabolomics_data$AMP)\n  ), color = \"grey\") +\n  geom_line(aes(\n    x = metabolomics_data$ADP,\n    y = predictWithModel(\n      data = metabolomics_data,\n      model_type = \"random_forest_regression\",\n      model = random_forest_model$model\n    )),\n    color = \"maroon\", size = 1\n  ) +\n  theme_bw()\nset.seed(123)\nunknown <- metabolomics_data[35:40,]\n\nrfc <- buildModel2(\n  data = metabolomics_data[c(1:34, 41:93),],\n  model_type = \"random_forest_classification\",\n  input_variables = colnames(metabolomics_data)[3:126],\n  output_variable = \"patient_status\",\n  optimization_parameters = list(\n    n_vars_tried_at_split = seq(5,50,5),\n    n_trees = seq(10,100,10),\n    min_leaf_size = seq(1,3,1)\n  )\n)\nrfc$metrics %>% arrange(desc(mean))\nrfc$metrics %>%\n    ggplot(aes(x = n_vars_tried_at_split, y = n_trees, fill = mean)) +\n    facet_grid(.~min_n) +\n    scale_fill_viridis(direction = -1) +\n    geom_tile() +\n    theme_bw()\npredictions <- predictWithModel(\n  data = unknown,\n  model_type = \"random_forest_classification\",\n  model = rfc$model\n)\n\ndata.frame(\n  real_status = metabolomics_data[35:40,]$patient_status,\n  predicted_status = predictions\n)"},{"path":"numerical-models.html","id":"section-14","chapter":"numerical models","heading":"","text":"","code":""},{"path":"numerical-models.html","id":"further-reading-7","chapter":"numerical models","heading":"further reading","text":"assessing regression models: performance R package. performance package helps check well statistical models work providing simple tools evaluate things like fit quality performance. offers easy ways spot problems models, like ’re complex fitting data properly, works different types models, including mixed-effects Bayesian ones.assessing regression models: performance R package. performance package helps check well statistical models work providing simple tools evaluate things like fit quality performance. offers easy ways spot problems models, like ’re complex fitting data properly, works different types models, including mixed-effects Bayesian ones.common machine learning tasks. Machine learning involves using algorithms learn data, key tasks including classification, regression, clustering. Classification categorizes data, recognizing images animals, regression predicts continuous values like sales forecasts, clustering groups data based similarities without prior labels.common machine learning tasks. Machine learning involves using algorithms learn data, key tasks including classification, regression, clustering. Classification categorizes data, recognizing images animals, regression predicts continuous values like sales forecasts, clustering groups data based similarities without prior labels.Classification random forests:\n- http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/\n- https://hansjoerg./2020/02/09/tidymodels--machine-learning/\n- https://towardsdatascience.com/dials-tune--parsnip-tidymodels-way--create--tune-model-parameters-c97ba31d6173","code":""},{"path":"embedding-models.html","id":"embedding-models","chapter":"embedding models","heading":"embedding models","text":"run analyses chapter, need four things.Please ensure computer can run following R script. may prompt install additional R packages.Please create account obtain API key https://pubmed.ncbi.nlm.nih.gov/ (Login > Account Settings > API Key Management)Please create account obtain API key https://huggingface.co (Login > Settings > Access Tokens, configure access token/key “Make calls serverless Inference API” “Make calls Inference Endpoints”)Please create account obtain API key https://biolm.ai/ (Login > Account > API Tokens)Please create account (may also need create NVIDIA cloud account prompted) obtain API key https://build.nvidia.com/. (get API key, go : https://build.nvidia.com/meta/esm2-650m, switch “input” python click “Get API Key” > Generate Key)Keep API keys (long sequences numbers letters, like password) handy use analyses.last chapter, looked models use numerical data understand relationships different aspects data set (inferential model use) models make predictions based numerical data (predictive model use). chapter, explore set models called language models transform non-numerical data (written text protein sequences) numerical domain, enabling non-numerical data analyzed using techniques already covered. Language models algorithms trained large amounts text (, case protein language models, many sequences) can perform variety tasks related training data. particular, focus embedding models, convert language data numerical data. embedding numerical representation data captures essential features lower-dimensional space different domain. context language models, embeddings transform text, words sentences, vectors numbers, enabling machine learning models statistical methods process analyze data effectively.basic form embedding model neural network called autoencoder. Autoencoders consist two main parts: encoder decoder. encoder takes input data compresses lower-dimensional representation, called embedding. decoder reconstructs original input embedding, output decoder compared original input. model (encoder decoder) iteratively optimized objective minimizing loss function measures difference original input reconstruction, resulting embedding model creates meaningful embeddings capture important aspects original input.","code":"\nsource(\"https://thebustalab.github.io/phylochemistry/language_model_analysis.R\")\n## Loading language model packages...\n## Loading language model functions...\n## Done with language model loading!"},{"path":"embedding-models.html","id":"pre-reading","chapter":"embedding models","heading":"pre-reading","text":"Please read following:Text Embeddings: Comprehensive Guide. article, “Text Embeddings: Comprehensive Guide”, Mariya Mansurova explores evolution, applications, visualization text embeddings. Beginning early methods like Bag Words TF-IDF, traces embeddings advanced capture semantic meaning, highlighting significant milestones word2vec transformer-based models like BERT Sentence-BERT. Mansurova explains embeddings transform text vectors computers can analyze tasks like clustering, classification, anomaly detection. provides practical examples using tools like OpenAI’s embedding models dimensionality reduction techniques, making article -depth resource theoretical hands-understanding text embeddings.Text Embeddings: Comprehensive Guide. article, “Text Embeddings: Comprehensive Guide”, Mariya Mansurova explores evolution, applications, visualization text embeddings. Beginning early methods like Bag Words TF-IDF, traces embeddings advanced capture semantic meaning, highlighting significant milestones word2vec transformer-based models like BERT Sentence-BERT. Mansurova explains embeddings transform text vectors computers can analyze tasks like clustering, classification, anomaly detection. provides practical examples using tools like OpenAI’s embedding models dimensionality reduction techniques, making article -depth resource theoretical hands-understanding text embeddings.ESM3: Simulating 500 million years evolution language model. 2024 blog article “ESM3: Simulating 500 million years evolution language model” EvolutionaryScale introduces ESM3, revolutionary language model trained billions protein sequences. article explores ESM3 marks major advancement computational biology enabling researchers reason protein sequences, structures, functions. massive datasets powerful computational resources, ESM3 can generate entirely new proteins, including esmGFP, green fluorescent protein differs significantly known natural variants. article highlights model’s potential transform fields like medicine, synthetic biology, environmental sustainability making protein design programmable. Please note “Open Model” section blog, highlights applications ESM models natural sciences.ESM3: Simulating 500 million years evolution language model. 2024 blog article “ESM3: Simulating 500 million years evolution language model” EvolutionaryScale introduces ESM3, revolutionary language model trained billions protein sequences. article explores ESM3 marks major advancement computational biology enabling researchers reason protein sequences, structures, functions. massive datasets powerful computational resources, ESM3 can generate entirely new proteins, including esmGFP, green fluorescent protein differs significantly known natural variants. article highlights model’s potential transform fields like medicine, synthetic biology, environmental sustainability making protein design programmable. Please note “Open Model” section blog, highlights applications ESM models natural sciences.","code":""},{"path":"embedding-models.html","id":"text-embeddings","chapter":"embedding models","heading":"text embeddings","text":", create text embeddings using publication data PubMed. Text embeddings numerical representations text preserve important information allow us apply mathematical statistical analyses textual data. , use series functions obtain titles abstracts PubMed, create embeddings titles, analyze using principal component analysis.First, use searchPubMed function extract relevant publications PubMed based specific search terms. function interacts PubMed website via tool called API. API, Application Programming Interface, set rules allows different software programs communicate . case, API allows code access data PubMed database directly, without needing manually search website. API key unique identifier allows authenticate using API. acts like password, giving permission access API services. , reading API key local file. can obtain signing NCBI account https://pubmed.ncbi.nlm.nih.gov/. API key, pass searchPubMed function along search terms. using “beta-amyrin synthase,” “friedelin synthase,” “Sorghum bicolor,” “cuticular wax biosynthesis.” also specify want results sorted according relevance (opposed sorting date) want three results per term (top three relevant hits) returned:output , can see ’ve retrieved records various publications, containing information title, journal, search term used. gives us dataset can analyze gain insights relationships different research topics.Next, use embedText function create embeddings titles extracted publications. Just like PubMed, Hugging Face API requires API key, acts unique identifier grants access services. can obtain API key signing https://huggingface.co following instructions generate key. API key, need specify using embedText function. example , reading key local file convenience.set embedText function, provide dataset containing text want embed (case, search_results, output PubMed search ), column text (title), Hugging Face API key. function generate numerical embeddings publication titles. default, embeddings generated using pre-trained embedding language model called ‘BAAI/bge-small-en-v1.5’, available Hugging Face API https://api-inference.huggingface.co/models/BAAI/bge-small-en-v1.5. model designed create compact, informative numerical representations text, making suitable wide range downstream tasks, clustering similarity analysis. like know model capabilities, can visit Hugging Face website https://huggingface.co, find detailed documentation additional resources.output embedText function data frame 384 appended columns represent embedding variables. embeddings capture features publication title. embeddings like bar codes:examine relationships publication titles, perform PCA text embeddings. use runMatrixAnalysis function, specifying PCA analysis type indicating columns contain embedding values. visualize results using scatter plot, point representing publication title, colored search term corresponds . grep function used search column names search_results data frame contain word ‘embed’. identifies selects columns hold embedding values, used columns values single analytes PCA enable visualization . ’ve seen lots PCA plots course explorations, note one different represents relationships meaning text passages (!) opposed relationships samples made many measurements numerical attributes.can also use embeddings examine data full sentences rather just lists terms, descriptions odors beer_components dataset:","code":"\nsearch_results <- searchPubMed(\n  search_terms = c(\"beta-amyrin synthase\", \"friedelin synthase\", \"sorghum bicolor\", \"cuticular wax biosynthesis\"),\n  pubmed_api_key = readLines(\"/Users/bust0037/Documents/Websites/pubmed_api_key.txt\"),\n  retmax_per_term = 3,\n  sort = \"relevance\"\n)\ncolnames(search_results)\n## [1] \"entry_number\" \"term\"         \"date\"        \n## [4] \"journal\"      \"title\"        \"doi\"         \n## [7] \"abstract\"\nselect(search_results, term, title)\n## # A tibble: 12 × 2\n##    term                       title                         \n##    <chr>                      <chr>                         \n##  1 beta-amyrin synthase       β-Amyrin synthase from Conyza…\n##  2 beta-amyrin synthase       Ginsenosides in Panax genus a…\n##  3 beta-amyrin synthase       β-Amyrin synthase (EsBAS) and…\n##  4 friedelin synthase         Friedelin in Maytenus ilicifo…\n##  5 friedelin synthase         Friedelin Synthase from Mayte…\n##  6 friedelin synthase         Functional characterization o…\n##  7 sorghum bicolor            Sorghum (Sorghum bicolor).    \n##  8 sorghum bicolor            Progress in Optimization of A…\n##  9 sorghum bicolor            Sorghum: A Multipurpose Crop. \n## 10 cuticular wax biosynthesis Regulatory mechanisms underly…\n## 11 cuticular wax biosynthesis Cuticular wax in wheat: biosy…\n## 12 cuticular wax biosynthesis Update on Cuticular Wax Biosy…search_results_embedded <- embedText(\n  df = search_results,\n  column_name = \"title\",\n  hf_api_key = readLines(\"/Users/bust0037/Documents/Websites/hf_api_key.txt\")\n)\n## \n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |==================================================| 100%\nsearch_results_embedded[1:3,1:10]\n## # A tibble: 3 × 10\n##   entry_number term  date       journal title doi   abstract\n##          <dbl> <chr> <date>     <chr>   <chr> <chr> <chr>   \n## 1            1 beta… 2019-11-20 FEBS o… β-Am… 10.1… Conyza …\n## 2            2 beta… 2024-04-03 Acta p… Gins… 10.1… Ginseno…\n## 3            3 beta… 2017-03-16 Phytoc… β-Am… 10.1… Siberia…\n## # ℹ 3 more variables: embedding_1 <dbl>, embedding_2 <dbl>,\n## #   embedding_3 <dbl>\nsearch_results_embedded %>%\n  pivot_longer(\n    cols = grep(\"embed\",colnames(search_results_embedded)),\n    names_to = \"embedding_variable\",\n    values_to = \"value\"\n  ) %>%\n  ggplot() +\n    geom_tile(aes(x = embedding_variable, y = factor(entry_number), fill = value)) +\n    scale_y_discrete(name = \"article\") +\n    scale_fill_gradient(low = \"white\", high = \"black\") +\n    theme(\n      axis.text.x = element_blank(),\n      axis.ticks.x = element_blank()\n    )\nrunMatrixAnalysis(\n  data = search_results_embedded,\n  analysis = \"pca\",\n  columns_w_values_for_single_analyte = colnames(search_results_embedded)[grep(\"embed\", colnames(search_results_embedded))],\n  columns_w_sample_ID_info = c(\"title\", \"journal\", \"term\")\n) %>%\n  ggplot() +\n    geom_label_repel(\n      aes(x = Dim.1, y = Dim.2, label = str_wrap(title, width = 35)),\n      size = 2, min.segment.length = 0.5, force = 50\n    ) +  \n    geom_point(aes(x = Dim.1, y = Dim.2, fill = term), shape = 21, size = 5, alpha = 0.7) +\n    scale_fill_brewer(palette = \"Set1\") +\n    scale_x_continuous(expand = c(0,1)) +\n    scale_y_continuous(expand = c(0,5)) +\n    theme_minimal()n <- 31\n\nodor <- data.frame(\n  sample = seq(1,n,1),\n  odor = dropNA(unique(beer_components$analyte_odor))[sample(1:96, n)]\n)\n\nout <- embedText(\n  odor, column_name = \"odor\",\n  hf_api_key = readLines(\"/Users/bust0037/Documents/Websites/hf_api_key.txt\")\n)\n## \n  |                                                        \n  |                                                  |   0%\n  |                                                        \n  |=========================                         |  50%\n  |                                                        \n  |==================================================| 100%\n\nrunMatrixAnalysis(\n  data = out,\n  analysis = \"pca\",\n  columns_w_values_for_single_analyte = colnames(out)[grep(\"embed\", colnames(out))],\n  columns_w_sample_ID_info = c(\"sample\", \"odor\")\n) -> pca_out\n\npca_out$color <- rgb(\n  scales::rescale(pca_out$Dim.1, to = c(0, 1)),\n  0,\n  scales::rescale(pca_out$Dim.2, to = c(0, 1))\n)\n\nggplot(pca_out) +\n  geom_label_repel(\n    aes(x = Dim.1, y = Dim.2, label = str_wrap(odor, width = 35)),\n    size = 2, min.segment.length = 0.5, force = 25\n  ) +  \n  geom_point(aes(x = Dim.1, y = Dim.2), fill = pca_out$color, shape = 21, size = 3, alpha = 0.7) +\n  # scale_x_continuous(expand = c(1,0)) +\n  # scale_y_continuous(expand = c(1,0)) +\n  theme_minimal()"},{"path":"embedding-models.html","id":"protein-embeddings","chapter":"embedding models","heading":"protein embeddings","text":"Autoencoders can trained accept various types inputs, text (shown ), images, audio, videos, sensor data, sequence-based information like peptides DNA. Protein language models convert protein sequences numerical representations can used variety downstream tasks, structure prediction function annotation. Protein language models, like text counterparts, trained large datasets protein sequences learn meaningful patterns relationships within sequence data.Protein language models offer several advantages traditional approaches, multiple sequence alignments (MSAs). One major disadvantage MSAs computationally expensive become increasingly slow number sequences grows. language models also computationally demanding, primarily resource-intensive training phase, whereas applying trained language model much faster. Additionally, protein language models can capture local global sequence features, allowing identify complex relationships span across different parts sequence. Furthermore, unlike MSAs, rely evolutionary information, protein language models can applied proteins without homologous sequences, making suitable analyzing sequences little evolutionary data available. flexibility broadens scope proteins can effectively studied using models.Beyond benefits described , protein language models additional, highly important capability: ability capture information connections elements input, even elements distant sequence. capability achieved use model architecture called transformer, sophisticated version autoencoder. example, amino acids far apart primary sequence may close 3D, folded protein structure. Proximate amino acids 3D space can play crucial roles protein stability, enzyme catalysis, binding interactions, depending spatial arrangement interactions residues. Embedding models transformer architecture can effectively capture functionally important relationships.adding mechanism called “attention mechanism” autoencoder, can create simple form transformer. attention mechanism works within encoder decoder, allowing element input (e.g., amino acid) compare every element, generating attention scores weigh much attention one amino acid give another. mechanism helps capture local long-range dependencies protein sequences, enabling model focus important areas regardless position sequence. Attention beneficial captures interactions distant amino acids, weighs relationships account protein folding interactions, adjusts focus across sequences varying lengths, captures different types relationships like hydrophobic interactions secondary structures, provides contextualized embeddings reflect broader sequence environment rather just local motifs. attention mechanisms, check reading section chapter.section, explore generate embeddings protein sequences using pre-trained protein language model demonstrate embeddings can used analyze visualize protein data effectively. First, need data. can use OSC_sequences object provided source() code, though can also use searchNCBI() function retrieve sequences. example:sequences, can embed function embedAminoAcids(). example . Note need provide either biolm API key NVIDIA api key, specify platform wish use. also need provide amino acid sequences AAStringSet object. use NVIDIA platform, model esm2-650m used (note: esm2 truncates sequences longer 1022 AA length). use bioLM, can pick number models.Nice! ’ve bot embeddings, can run PCA analysis visualize 2D space:","code":"\nncbi_results <- searchNCBI(search_term = \"oxidosqualene cyclase\", retmax = 100)\nncbi_results\n## AAStringSet object of length 100:\n##       width seq                         names               \n##   [1]   575 MPPSALTESVGT...KDITGDGEGGRP WP_283887978.1 pr...\n##   [2]   428 MNIRRSVAALSL...GFLISGRKRRQL WP_283886809.1 pr...\n##   [3]   428 MNIRRSVAALSL...GFLLSGRKRRQL WP_251772202.1 pr...\n##   [4]   575 MPPSALTESVGT...KDITGDGEGGRP WP_251770008.1 pr...\n##   [5]   468 MTESVGTAIVAG...TRHPDGSWSTGE WP_237509046.1 pr...\n##   ...   ... ...\n##  [96]   489 MFFKYSPIDIED...QSVRPQTFEYVP WP_072873004.1 pr...\n##  [97]   399 MKTVARDALDCR...KRALLQAAGTPP WP_444608723.1 pr...\n##  [98]   399 MKTVARDALDCR...KRALLQAAGTPP WP_444574685.1 pr...\n##  [99]   735 MTEGTHLRRRGG...YPGSPLAGKVKL XP_076603171.1 la...\n## [100]   530 MELKSEVSESLV...TLQVPRRSFLKL WP_308894994.1 pr...\nembedded_OSCs <- embedAminoAcids(\n  amino_acid_stringset = OSC_sequences,\n  biolm_api_key = readLines(\"/Users/bust0037/Documents/Websites/biolm_api_key.txt\"),\n  nvidia_api_key = readLines(\"/Users/bust0037/Documents/Websites/nvidia_api_key.txt\"),\n  platform = \"biolm\"\n)\nembedded_OSCs$product <- tolower(gsub(\".*_\", \"\", embedded_OSCs$name))\nembedded_OSCs <- select(embedded_OSCs, name, product, everything())\nembedded_OSCs[1:3,1:4]\nrunMatrixAnalysis(\n  data = embedded_OSCs,\n  analysis = \"pca\",\n  columns_w_values_for_single_analyte = colnames(embedded_OSCs)[3:dim(embedded_OSCs)[2]],\n  columns_w_sample_ID_info = c(\"name\", \"product\")\n) %>%\n  ggplot() +\n    geom_jitter(\n      aes(x = Dim.1, y = Dim.2, fill = product),\n      shape = 21, size = 5, height = 2, width = 2, alpha = 0.6\n    ) +\n    theme_minimal()"},{"path":"embedding-models.html","id":"section-15","chapter":"embedding models","heading":"","text":"","code":""},{"path":"embedding-models.html","id":"further-reading-8","chapter":"embedding models","heading":"further reading","text":"creating knowledge graphs LLMs. blog post explains create knowledge graphs text using OpenAI functions combined LangChain Neo4j. highlights large language models (LLMs) made information extraction accessible, providing step--step instructions setting pipeline extract structured information construct graph unstructured data.creating knowledge graphs LLMs. blog post explains create knowledge graphs text using OpenAI functions combined LangChain Neo4j. highlights large language models (LLMs) made information extraction accessible, providing step--step instructions setting pipeline extract structured information construct graph unstructured data.creating RAG systems LLMs. article provides technical overview implementing complex Retrieval Augmented Generation (RAG) systems, focusing key concepts like chunking, query augmentation, document hierarchies, knowledge graphs. highlights challenges data retrieval, multi-hop reasoning, query planning, also discussing opportunities improve RAG infrastructure accurate efficient information extraction.creating RAG systems LLMs. article provides technical overview implementing complex Retrieval Augmented Generation (RAG) systems, focusing key concepts like chunking, query augmentation, document hierarchies, knowledge graphs. highlights challenges data retrieval, multi-hop reasoning, query planning, also discussing opportunities improve RAG infrastructure accurate efficient information extraction.using protein embeddings biochemical research. study presents machine learning pipeline successfully identifies characterizes terpene synthases (TPSs), challenging task due limited availability labeled protein sequences. combining curated TPS dataset, advanced structural domain segmentation, language model techniques, authors discovered novel TPSs, including first active enzymes Archaea, significantly improving accuracy substrate prediction across TPS classes.using protein embeddings biochemical research. study presents machine learning pipeline successfully identifies characterizes terpene synthases (TPSs), challenging task due limited availability labeled protein sequences. combining curated TPS dataset, advanced structural domain segmentation, language model techniques, authors discovered novel TPSs, including first active enzymes Archaea, significantly improving accuracy substrate prediction across TPS classes.attention mechanims transformers explained. Financial Times article explains development workings large language models (LLMs), emphasizing foundation transformer model created Google researchers 2017. models use self-attention mechanisms understand context, allowing respond subtle relationships elements input, even elements far one another linear input sequence.attention mechanims transformers explained. Financial Times article explains development workings large language models (LLMs), emphasizing foundation transformer model created Google researchers 2017. models use self-attention mechanisms understand context, allowing respond subtle relationships elements input, even elements far one another linear input sequence.types protein language models. 3D Protein Structure Prediction deepmind / alphafold2-multimer: Predicts 3D structure protein complexes amino acid sequences. deepmind / alphafold2: Predicts 3D structure single proteins amino acid sequences. meta / esmfold: Predicts 3D structure proteins based amino acid sequences. Protein Embedding Generation meta / esm2-650m: Generates protein embeddings amino acid sequences. Protein Sequence Design ipd / proteinmpnn: Predicts amino acid sequences given protein backbone structures. Generative Protein Design ipd / rfdiffusion: generative model designing protein backbones, particularly protein binder design. Molecule-Protein Interaction Prediction mit / diffdock: Predicts 3D interactions molecules proteins (docking simulations).types protein language models. 3D Protein Structure Prediction deepmind / alphafold2-multimer: Predicts 3D structure protein complexes amino acid sequences. deepmind / alphafold2: Predicts 3D structure single proteins amino acid sequences. meta / esmfold: Predicts 3D structure proteins based amino acid sequences. Protein Embedding Generation meta / esm2-650m: Generates protein embeddings amino acid sequences. Protein Sequence Design ipd / proteinmpnn: Predicts amino acid sequences given protein backbone structures. Generative Protein Design ipd / rfdiffusion: generative model designing protein backbones, particularly protein binder design. Molecule-Protein Interaction Prediction mit / diffdock: Predicts 3D interactions molecules proteins (docking simulations).","code":""},{"path":"loading-analyzegcmsdata.html","id":"loading-analyzegcmsdata","chapter":"loading analyzeGCMSdata","heading":"loading analyzeGCMSdata","text":"page explains load simple application integrating analyzing GC-MS data. app, can analyze .CDF files. CDF files contain essentially data GC-MS run, can exported GC-MS systems using software provided manufacturer. run application, use following guidelines:Create new folder hard drive place CDF file(s) folder. doesn’t matter name folder , must contain special characters (including space  name). example, CDF file called “sorghum_bicolor.CDF”, might create folder called gc_data hard drive, place “sorghum_bicolor.CDF” file folder.Create new folder hard drive place CDF file(s) folder. doesn’t matter name folder , must contain special characters (including space  name). example, CDF file called “sorghum_bicolor.CDF”, might create folder called gc_data hard drive, place “sorghum_bicolor.CDF” file folder.R RStudio, run source command shown . need every time re-open R RStudio. command load analyzeGCMSdata function R RStudio environment. Note need connected internet work.R RStudio, run source command shown . need every time re-open R RStudio. command load analyzeGCMSdata function R RStudio environment. Note need connected internet work.command run successfully see something like:\nR RStudio, run analyzeGCMSdata command folder contains CDF file. run command CDF file , work. example, CDF file called “sorghum_bicolor.CDF”, inside folder called gc_data, run following:Mac, use single forward slashes. example:PC, may need use double back slashes. example:first time open CDF datafile, may take load. new RShiny window opens, press shift+q load chromatogram(s).","code":"\nsource(\"https://thebustalab.github.io/phylochemistry/gcms.R\")\nanalyzeGCMSdata(\"/Volumes/My_Drive/gc_data\")\nanalyzeGCMSdata(\"C:\\\\Users\\\\My_Profile\\\\gc_data\")"},{"path":"using-analyzegcmsdata.html","id":"using-analyzegcmsdata","chapter":"using analyzeGCMSdata","heading":"using analyzeGCMSdata","text":"","code":""},{"path":"using-analyzegcmsdata.html","id":"basic-usage-of-analytegcmsdata","chapter":"using analyzeGCMSdata","heading":"basic usage of analyteGCMSdata","text":"reference, key commands used operate integration app. information covered overview video.control chromatogram window:shift + q = updateshift + = add selected peakshift + r = remove selected peakshift + g = add global peak\nshift + z = save tableTo control mass spectrum window:shift+1 = extract mass spectra highlighted chromatogram region, plot average mass spectrum panel 1.shift+2 = refresh mass spectrum panel 1. used zooming region mass spectrum highlighted. spectrum needs first extracted possible.shift+3 = extract mass spectra highlighted chromatogram region, subtract average mass spectrum panel 1.shift+4 = search current spectrum panel 1 library mass spectra (available load via phylochemistry).shift+5 = save current spectrum panel 1 csv file (available load via phylochemistry).","code":""},{"path":"using-analyzegcmsdata.html","id":"advanced-usage-of-analyzegcmsdata","chapter":"using analyzeGCMSdata","heading":"advanced usage of analyzeGCMSdata","text":"can ask analyzeGCMSdata extract single ion chromatograms wish. Just specify list ions argument. Note specifying “0” corresponds total ion chromatogram must included first item list. ’s example:return interface shows chromatograms total ion count ion 218.point, note new set files data-containing folder. one *.CDF.csv file CDF file folder. contains matrix mass measurements entire sample - abundance m/z value scan. also chromatograms.csv file. list chromatograms (total ion + whatever single ions specified). can useful creating plots chromatograms via ggplot.","code":"\nanalyzeGCMSdata(\"/Volumes/My_Drive/gc_data\", ions = c(\"0\", \"218\"))"},{"path":"cdf-export.html","id":"cdf-export","chapter":"CDF export","heading":"CDF export","text":"GC-MS computer, open Enhanced Data AnalysisFile > Export Data .AIA Format, Create New Directory (“OK”) > Desktop (create folder name remember)Select datafiles wish analyze process , saving output folder just createdCopy .D files samples wish analyze folderMove folder personal computerCreate one folder sample, put corresponding .CDF file folder.","code":"\n# reads <- readFasta(\"https://drive.google.com/file/d/1r6E0U5LyYwjWenxy9yqh5QQ2mq1umWOW/view?usp=sharing\")\n\n# # post <- readFasta(\"/Users/bust0037/Desktop/ragtag.scaffold.fasta\")\n# n_chroms <- 18\n\n# pb <- progress::progress_bar$new(total = n_chroms)\n\n# out <- list()\n\n# for (i in 1:n_chroms) {\n\n#   pb$tick()\n\n#   dat <- strsplit(substr(as.character(post[i]), 1, 50000000), \"\")[[1]]\n  \n#   b <- rle(dat)\n\n#   # Create a data frame\n#   dt <- data.frame(number = b$values, lengths = b$lengths, scaff = i)\n#   # Get the end\n#   dt$end <- cumsum(dt$lengths)\n#   # Get the start\n#   dt$start <- dt$end - dt$lengths + 1\n\n#   # Select columns\n#   dt <- dt[, c(\"number\", \"start\", \"end\", \"scaff\")]\n#   # Sort rows\n#   dt <- dt[order(dt$number), ]\n\n#   dt %>%\n#     filter(number == \"N\") -> N_dat\n\n#   out[[i]] <- N_dat\n\n# }\n\n# out <- do.call(rbind, out)\n\n\n# chroms <- data.frame(\n#   lengths = post@ranges@width[1:n_chroms],\n#   scaff = seq(1,n_chroms,1)\n# )\n\n# ggplot() +\n#   statebins:::geom_rrect(data = chroms, aes(xmin = 0, xmax = lengths, ymin = -1, ymax = 1, fill = scaff), color = \"black\") +\n#   geom_rect(data = out, aes(xmin = start, xmax = end, ymin = -0.95, ymax = 0.95), color = \"white\", fill = \"white\", size = 0.08) +\n#   facet_grid(scaff~.) +\n#   scale_fill_viridis(end = 0.8) +\n#   theme_classic()\n\n# ggplot() +\n#   geom_rect(data = filter(chroms, scaff == 1 | scaff == 2), aes(xmin = 0, xmax = lengths, ymin = -1, ymax = 1, fill = scaff), color = \"black\") +\n#   geom_rect(data = filter(out, scaff == 1 | scaff == 2), aes(xmin = start, xmax = end, ymin = -0.95, ymax = 0.95), color = \"white\", fill = \"white\", size = 0.08) +\n#   facet_grid(scaff~.) +\n#   scale_y_continuous(limits = c(-2,2)) +\n#   scale_fill_viridis(end = 0.8) +\n#   theme_classic() +\n#   coord_polar()"},{"path":"homology.html","id":"homology","chapter":"homology","heading":"homology","text":"","code":""},{"path":"homology.html","id":"ncbi-blast","chapter":"homology","heading":"NCBI blast","text":"blastNCBI() function provides quick, programmatic way search NCBI database custom sequences. inputting DNA sequence, desired BLAST program (blastn megablast), selecting database like “core_nt,” can initiate remote BLAST search directly NCBI’s servers. function handles submission, waits search complete, retrieves results analysis. output tibble details significant hit, allowing analyze identity percentages, gaps, scores matches accessible format. function useful researchers needing automated access NCBI BLAST results directly R without needing manually manage BLAST requests wait browser. ’s examples, including approach function ’re working StringSet object:","code":"\nblastNCBI(\n    query_sequence = \"AAGCTTGGAAATATTAAGTGAACAGGGAATAGAAAGGATACAACAAAAGGGAAGAACTTAGAGCA\",\n    program = \"blastn\",\n    database = \"core_nt\"\n)\n\nseqs <- DNAStringSet(\"AAGCTTGGAAATATTAAGTGAACAGGGAATAGAAAGGATACAACAAAAGGGAAGAACTTAGAGCA\")\nblastNCBI(\n    query_sequence = as.character(seqs[1]),\n    program = \"blastn\",\n    database = \"core_nt\"\n)"},{"path":"homology.html","id":"local-blast","chapter":"homology","heading":"local blast","text":"NCBI, can search various sequence collections one queries. However, often want search custom library, multiple libraries. example, maybe downloaded genomes interest want run blast searches . polyBlast() designed . polyBlast() relies BLAST+ program available NCBI BLAST+. Download program point function executable via blast_module_directory_path argument. can search multiple sequence libraries using multiple queries, usual blast configurations (blastp, blastn, tblastn, etc.) available. Please note searches protein sequences translated DNA sequences 5–10-fold sensitive DNA:DNA sequence comparison.Let’s check polyBlast() looking example. example, need set things:“named_subjects_list”: named list sequence collections (often transcriptomes) search (one fasta collection, often one collection species accession).“query_in_path”: One queries, listed single fasta file.“sequences_of_interest_directory_path”: path directory BLAST hits written individual files (useful later ).“blast_module_directory_path”: path folder BLAST+ executable.“blast_mode”: format XYblastZ X subject type (transcriptome proteome, use “n” nucleotide, “p” amino acids). Y whether subjects translated (“t” translate, “n” translation, choose “t” subjects searched ORFs, every three base pairs just translated verbatim). Z format query/queries (“n” nucleotide, “p” amino acids). Allowed formats: nnblastn, ntblastp, pnblastp.“e_value_cutoff”: Hits e-value cutoff returned. Default = 1.“queries_in_outout”: TRUE/FALSE, queries included output? want build tree BLAST hits want queries tree, set TRUE.“monolist_out_path”: path want summary file BLAST hits written.things, can set search (see ). two main outputs search: list hits (“monolist_out”, written “monolist_out_path”), hits , written individual files “sequences_of_interest_directory_path”. two things can used downstream analyses, alignments. function return object.","code":"\nthe_transcriptomes <- c(\n  \"/path_to/the_transcriptomes_or_proteomes/Nicotiana_glauca.fa\",\n  \"/path_to/the_transcriptomes_or_proteomes/Nicotiana_tabacum.fa\",\n  \"/path_to/the_transcriptomes_or_proteomes/Nicotiana_benthamiana.fa\"\n)\n\nnames(the_transcriptomes) <- c(\n  \"Nicotiana_glauca\",\n  \"Nicotiana_tabacum\",\n  \"Nicotiana_benthamiana\"\n)\n\npolyBlast(\n  named_subjects_list = the_transcriptomes,\n  query_in_path = \"/path_to/sequences_you_want_to_find_in_the_transcriptomes.fa\",\n  sequences_of_interest_directory_path = \"/path_to/a_folder_for_hit_sequences/\",\n  blast_module_directory_path = \"/path_to/the_blast_module/\",\n  blast_mode = c(\"nnblastn\", \"ntblastp\", \"pnblastp\", \"dc-megablast\"), \n  e_value_cutoff = 1,\n  queries_in_output = TRUE,\n  monolist_out_path = \"/path_to/a_csv_file_that_will_list_all_blast_hits.csv\"\n)"},{"path":"homology.html","id":"hmmer","chapter":"homology","heading":"hmmer","text":"HMM, stands Hidden Markov Model, statistical model often used various applications involving sequences, including speech recognition, natural language processing, bioinformatics. context bioinformatics, HMMs frequently applied sequence similarity searching, notably analysis protein DNA sequences. talk using HMMs sequence similarity searching, ’re often referring identifying conserved patterns domains within biological sequences. conserved regions can indicative functional structural properties molecule. One advantages using HMMs traditional sequence similarity searching tools like BLAST HMMs can sensitive detecting distant homologues. take account position-specific variability within protein family, opposed just looking stretches similar sequence.’s general idea HMMs used sequence similarity searching:Build library HMM domains: bioinformatics, typical application construction library HMM domains. HMMs built multiple sequence alignments family related proteins genes. alignments help highlight conserved variable positions sequence family. alignment, HMM can ‘trained’ data. training process estimates probabilities different events, like particular amino acid (case proteins) appearing specific position.Build library HMM domains: bioinformatics, typical application construction library HMM domains. HMMs built multiple sequence alignments family related proteins genes. alignments help highlight conserved variable positions sequence family. alignment, HMM can ‘trained’ data. training process estimates probabilities different events, like particular amino acid (case proteins) appearing specific position.Predict domains unknown sequences: training, can use HMMs score sequences. sequence scores certain threshold, suggests sequence may member protein gene family represented HMM. can search databases uncharacterized sequences using HMM. Sequences database get high score HMM potential new members family, thus might share similar functional structural properties.Predict domains unknown sequences: training, can use HMMs score sequences. sequence scores certain threshold, suggests sequence may member protein gene family represented HMM. can search databases uncharacterized sequences using HMM. Sequences database get high score HMM potential new members family, thus might share similar functional structural properties.can implement two steps using buildDomainLibrary() function predictDomains() function. See :","code":"\nbuildDomainLibrary(\n    alignment_in_paths = c(\n        \"/project_data/shared/kalanchoe_transporters/alignments/subset_cluster_1_amin_seqs_aligned.fa\",\n        \"/project_data/shared/kalanchoe_transporters/alignments/subset_cluster_2_amin_seqs_aligned.fa\"\n    ),\n    domain_library_out_path = \"/project_data/shared/kalanchoe_transporters/test.hmm\"\n)\n\npredictDomains(\n    fasta_in_path = \"/project_data/shared/kalanchoe_transporters/alignments/subset_cluster_3_amin_seqs.fa\",\n    domain_library_in_path = \"/project_data/shared/kalanchoe_transporters/test.hmm\"\n)"},{"path":"homology.html","id":"d-similarity","chapter":"homology","heading":"3D similarity","text":"foldseek easy-search /project_data/shared/kalanchoe_phylogeny/protein_structures/structures/AHF22083.1.pdb /project_data/shared/kalanchoe_phylogeny/protein_structures/structures/ result.m8 /project_data/shared/kalanchoe_phylogeny/protein_structures/tmp --exhaustive-search 1","code":""},{"path":"homology.html","id":"interpreting-homology-data","chapter":"homology","heading":"interpreting homology data","text":"bitscore v evalue - low evalue (< 10^-250) R just assigns value zero resolution lost. just use bitscore!“30% identity rule--thumb” conservative. Statistically significant (E < 10−6 – 10−3) protein homologs can share less 20% identity. E-values bit scores (bits > 50) far sensitive reliable percent identity inferring homology.expect value (E-value) can changed order limit number hits significant ones. lower E-value, better hit. E-value dependent length query sequence size database. example, alignment obtaining E-value 0.05 means 5 100 chance occurring chance alone. E-values dependent query sequence length database size. Short identical sequence may high E-value may regarded “false positive” hits. often seen one searches short primer regions, small domain regions etc. default threshold E-value BLAST web page 10, default polyBlast 1. Increasing value likely generate hits. rules thumb can used loose guidelines:E-value < 10e-100 Identical sequences. get long alignments across entire query hit sequence.10e-100 < E-value < 10e-50 Almost identical sequences. long stretch query protein matched database.10e-50 < E-value < 10e-10 Closely related sequences, domain match similar.10e-10 < E-value < 1 true homologue gray area.E-value > 1 Proteins likely relatedE-value > 10 Hits likely junk unless query sequence short.reference: https://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/650/_E_value.htmlreference: Pearson W. R. (2013). introduction sequence similarity (“homology”) searching. Current protocols bioinformatics, Chapter 3, Unit3.1. https://doi.org/10.1002/0471250953.bi0301s42.E-values Bit-scores: Pfam-based around hidden Markov model (HMM) searches, provided HMMER3 package. HMMER3, like BLAST, E-values (expectation values) calculated. E-value number hits expected score equal better value chance alone. good E-value much less 1. value 1 expected just chance. principle, need decide significance match E-value.E-values dependent size database searched, use second system -house maintaining Pfam models, based bit score (see ), independent size database searched. Pfam family, set bit score gathering (GA) threshold hand, sequences scoring threshold appear full alignment. works bit score 20 equates E-value approximately 0.1, score 25 approximately 0.01. gathering threshold “trusted cutoff” (TC) “noise cutoff” (NC) recorded automatically. TC score next highest scoring match GA, NC score sequence next GA, .e. highest scoring sequence included full alignment.Sequence versus domain scores: ’s additional wrinkle scoring system. HMMER3 calculates two kinds scores, first sequence whole second domain(s) sequence. “sequence score” total score sequence aligned model (HMM); “domain score” score single domain — two scores virtually identical one domain present sequence. multiple occurrences domain sequence individual match may quite weak, sequence score sum individual domain scores, since finding multiple instances domain increases confidence sequence belongs protein family, .e. truly matches model.Sequence versus domain scores: ’s additional wrinkle scoring system. HMMER3 calculates two kinds scores, first sequence whole second domain(s) sequence. “sequence score” total score sequence aligned model (HMM); “domain score” score single domain — two scores virtually identical one domain present sequence. multiple occurrences domain sequence individual match may quite weak, sequence score sum individual domain scores, since finding multiple instances domain increases confidence sequence belongs protein family, .e. truly matches model.Meaning bit-score non-mathematicians: bit score 0 means likelihood match emitted model equal emitted Null model (chance). bit score 1 means match twice likely emitted model Null. bit score 2 means match 4 times likely emitted model Null. , bit score 20 means match 2 power 20 times likely emitted model Null.Meaning bit-score non-mathematicians: bit score 0 means likelihood match emitted model equal emitted Null model (chance). bit score 1 means match twice likely emitted model Null. bit score 2 means match 4 times likely emitted model Null. , bit score 20 means match 2 power 20 times likely emitted model Null.","code":""},{"path":"alignments.html","id":"alignments","chapter":"alignments","heading":"alignments","text":"","code":""},{"path":"alignments.html","id":"alignsequences","chapter":"alignments","heading":"alignSequences","text":", course, many tools aligning sequences. alignSequences(), alignment tool phylochemistry, designed versatile (can nucleotide, amino acid, codon alignments, ), able quily align different subsets collections sequences. three steps make work, bit work, worth end. list ingredients. used polyBlast(), polyBlast() created ingredients . Following list example. function return object, output fasta containing alignment alignment_out_path.“monolist”: data.frame contains list sequences aligned. first column accession number refers fasta file “sequences_of_interest_directory_path”.“monolist”: data.frame contains list sequences aligned. first column accession number refers fasta file “sequences_of_interest_directory_path”.“subset”: monolist .csv also needs contain least one “subset_*” column. simple implementation column called “subset_all” contains TRUE entry row. means accessions aligned. possible create additonal logical/boolean columns specify argument, cause subset collection sequences aligned.“subset”: monolist .csv also needs contain least one “subset_*” column. simple implementation column called “subset_all” contains TRUE entry row. means accessions aligned. possible create additonal logical/boolean columns specify argument, cause subset collection sequences aligned.“alignment_out_path”: path directory contain output alignment.“alignment_out_path”: path directory contain output alignment.“sequences_of_interest_directory_path”: path directory contains one fasta file accessions monolist.“sequences_of_interest_directory_path”: path directory contains one fasta file accessions monolist.“input_sequence_type”: options “nucl” “amin” specifying type sequence aligned.“input_sequence_type”: options “nucl” “amin” specifying type sequence aligned.“mode”: options “nucl_align”, basic nucleotide alignment, “amin_align”, basic amino acid alignment, “codon_align”, codon alignment, “fragment_align”, align sequences base fragment.“mode”: options “nucl_align”, basic nucleotide alignment, “amin_align”, basic amino acid alignment, “codon_align”, codon alignment, “fragment_align”, align sequences base fragment.“base_fragment”: path fasta file containing base fragment subjects aligned.“base_fragment”: path fasta file containing base fragment subjects aligned.","code":"\nalignSequences(\n  monolist = readMonolist(\"/path_to/a_csv_file_that_will_list_all_blast_hits.csv\"), \n  subset = \"subset_all\", \n  alignment_directory_path = \"/path_to/a_folder_for_alignments/\", \n  sequences_of_interest_directory_path = \"/path_to/a_folder_for_hit_sequences/\",\n  input_sequence_type = \"amin\", \n  mode = \"amin_alignment\",\n  base_fragment = NULL\n)"},{"path":"alignments.html","id":"analyzealignment","chapter":"alignments","heading":"analyzeAlignment","text":"","code":""},{"path":"phylogenies.html","id":"phylogenies","chapter":"phylogenies","heading":"phylogenies","text":"","code":""},{"path":"phylogenies.html","id":"buildtree","chapter":"phylogenies","heading":"buildTree","text":"function swiss army knife tree building. takes input alignments existing phylogenies derive phylogeny interest, can use neighbor-joining maximum liklihood methods (model optimization), can run bootstrap replicates, can calculate ancestral sequence states. illustrate, let’s look examples:","code":""},{"path":"phylogenies.html","id":"newick-input","chapter":"phylogenies","heading":"newick input","text":"Let’s use Busta lab’s plant phylogeny [derived Qian et al., 2016] build phylogeny five species .Cool! got phylogeny. happens want build phylogeny species isn’t scaffold? example, want build phylogeny includes Arabidopsis neglecta? can include name list members:Note buildTree informs us: “Scaffold newick tip Arabidopsis_thaliana substituted Arabidopsis_neglecta”. means Arabidopsis neglecta grafted onto tip originally occupied Arabidopsis thaliana. behaviour useful operating large phylogenetic scale (.e. exact phylogeny topology critical family level). However, person interested using existing newick tree scaffold phylogeny genus-level topology critical, beware! scaffold may appropriate see message. operating genus level, probably want use sequence data build phylogeny anyway. let’s look :","code":"\ntree <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold = \"https://thebustalab.github.io/data/plant_phylogeny.newick\",\n  members = c(\"Sorghum_bicolor\", \"Zea_mays\", \"Setaria_viridis\", \"Arabidopsis_thaliana\", \"Amborella_trichopoda\")\n)\n## Pro tip: most tree read/write functions reset node numbers. Fortify your tree and save it as a csv file to preserve node numbering.\n\ntree\n## \n## Phylogenetic tree with 5 tips and 4 internal nodes.\n## \n## Tip labels:\n##   Amborella_trichopoda, Zea_mays, Sorghum_bicolor, Setaria_viridis, Arabidopsis_thaliana\n## Node labels:\n##   , , , \n## \n## Rooted; includes branch length(s).\n\nplot(tree)\ntree <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold_in_path = \"https://thebustalab.github.io/data/plant_phylogeny.newick\",\n  members = c(\"Sorghum_bicolor\", \"Zea_mays\", \"Setaria_viridis\", \"Arabidopsis_neglecta\", \"Amborella_trichopoda\")\n)\n## IMPORTANT: Some species substitutions or removals were made as part of buildTree. Run build_tree_substitutions() to see them all.\n## Pro tip: most tree read/write functions reset node numbers. Fortify your tree and save it as a csv file to preserve node numbering.\n\ntree\n## \n## Phylogenetic tree with 5 tips and 4 internal nodes.\n## \n## Tip labels:\n##   Amborella_trichopoda, Zea_mays, Sorghum_bicolor, Setaria_viridis, Arabidopsis_neglecta\n## Node labels:\n##   , , , \n## \n## Rooted; includes branch length(s).\n\nplot(tree)"},{"path":"phylogenies.html","id":"alignment-input","chapter":"phylogenies","heading":"alignment input","text":"Arguments case :“scaffold_type”: “amin_alignment” “nucl_alignment” amino acids nucleotides.“scaffold_in_path”: path fasta file contains alignment want build tree.“ml”: Logical, TRUE want use maximum liklihood, FALSE , case neighbor joining ne used.“model_test”: say TRUE “ml”, buildTree test different maximum liklihood models use “best” one?“bootstrap”: TRUE FALSE, whether want bootstrap values nodes.“ancestral_states”: TRUE FALSE, buildTree() compute ancestral sequence node?“root”: NULL, name accession form root tree.","code":"\nbuildTree(\n  scaffold_type = \"amin_alignment\",\n  scaffold_in_path = \"/path_to/a_folder_for_alignments/all_amin_seqs.fa\",\n  ml = FALSE, \n  model_test = FALSE,\n  bootstrap = FALSE,\n  ancestral_states = FALSE,\n  root = NULL\n)"},{"path":"phylogenies.html","id":"plotting-trees","chapter":"phylogenies","heading":"plotting trees","text":"several approaches plotting trees. simple one using base plot function:Though can get messy lots tip labels:One solution use ggtree, default doesn’t show tip labels. plot can , ggtree bunch useful things, recommend :Another convenient fucntion ggplot’s fortify. convert phylo object data frame:ggtree can still plot dataframe, allows metadata stored human readable format using mutating joins (explained ). metadata can plotted standard ggplot geoms, dataframes can also conveniently saved .csv files:","code":"\ntest_tree_small <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold_in_path = \"https://thebustalab.github.io/data/plant_phylogeny.newick\",\n  members = c(\"Sorghum_bicolor\", \"Zea_mays\", \"Setaria_viridis\")\n)\n## Pro tip: most tree read/write functions reset node numbers. Fortify your tree and save it as a csv file to preserve node numbering.\n\nplot(test_tree_small)\nset.seed(122)\ntest_tree_big <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold_in_path = \"https://thebustalab.github.io/data/plant_phylogeny.newick\",\n  members = plant_species$Genus_species[abs(floor(rnorm(60)*100000))]\n)\n\nplot(test_tree_big)\nggtree(test_tree_big)\ntest_tree_big_fortified <- fortify(test_tree_big)\ntest_tree_big_fortified\n## # A tbl_tree abstraction: 101 × 9\n## # which can be converted to treedata or phylo \n## # via as.treedata or as.phylo\n##    parent  node branch.length label isTip     x     y branch\n##     <int> <int>         <dbl> <chr> <lgl> <dbl> <dbl>  <dbl>\n##  1     54     1          83.0 Wolf… TRUE   188.     1   147.\n##  2     54     2          83.0 Spat… TRUE   188.     2   147.\n##  3     55     3         138.  Dios… TRUE   188.     3   120.\n##  4     58     4          42.7 Bulb… TRUE   188.     5   167.\n##  5     58     5          42.7 Ober… TRUE   188.     6   167.\n##  6     59     6          32.0 Poma… TRUE   188.     7   172.\n##  7     59     7          32.0 Teli… TRUE   188.     8   172.\n##  8     56     8         135.  Cala… TRUE   188.     4   121.\n##  9     61     9         147.  Pepe… TRUE   188.     9   115.\n## 10     62    10         121.  Endl… TRUE   188.    10   128.\n## # ℹ 91 more rows\n## # ℹ 1 more variable: angle <dbl>\n\n## Note that \"plant_species\" comes with the phylochemistry source.\n\ntest_tree_big_fortified_w_data <- left_join(test_tree_big_fortified, plant_species, by = c(\"label\" = \"Genus_species\"))\n\ntest_tree_big_fortified_w_data\n## # A tbl_tree abstraction: 101 × 14\n## # which can be converted to treedata or phylo \n## # via as.treedata or as.phylo\n##    parent  node branch.length label isTip     x     y branch\n##     <int> <int>         <dbl> <chr> <lgl> <dbl> <dbl>  <dbl>\n##  1     54     1          83.0 Wolf… TRUE   188.     1   147.\n##  2     54     2          83.0 Spat… TRUE   188.     2   147.\n##  3     55     3         138.  Dios… TRUE   188.     3   120.\n##  4     58     4          42.7 Bulb… TRUE   188.     5   167.\n##  5     58     5          42.7 Ober… TRUE   188.     6   167.\n##  6     59     6          32.0 Poma… TRUE   188.     7   172.\n##  7     59     7          32.0 Teli… TRUE   188.     8   172.\n##  8     56     8         135.  Cala… TRUE   188.     4   121.\n##  9     61     9         147.  Pepe… TRUE   188.     9   115.\n## 10     62    10         121.  Endl… TRUE   188.    10   128.\n## # ℹ 91 more rows\n## # ℹ 6 more variables: angle <dbl>, Phylum <chr>,\n## #   Order <chr>, Family <chr>, Genus <chr>, species <chr>\n\nggtree(test_tree_big_fortified_w_data) + \n  geom_point(\n    data = filter(test_tree_big_fortified_w_data, isTip == TRUE),\n    aes(x = x, y = y, fill = Order), size = 3, shape = 21, color = \"black\") +\n  geom_text(\n    data = filter(test_tree_big_fortified_w_data, isTip == TRUE),\n    aes(x = x, y = y, label = y), size = 2, color = \"white\") +\n  geom_tiplab(aes(label = label), offset = 10, size = 2) +\n  theme_void() +\n  scale_fill_manual(values = discrete_palette) +\n  coord_cartesian(xlim = c(0,280)) +\n  theme(\n    legend.position = c(0.15, 0.75)\n  )"},{"path":"phylogenies.html","id":"collapsetree","chapter":"phylogenies","heading":"collapseTree","text":"Sometimes want view tree higher level taxonomical organization, higher level. can done easily using collapseTree function. takes two arguments: un-fortified tree (tree), two-column data frame (associations). first column data frame tip labels tree, second column higher level organization tip belongs. function prune tree one member higher level organization included output. example, let’s look tree previous section family level:","code":"\ncollapseTree(\n  tree = test_tree_big,\n  associations = data.frame(\n    tip.label = test_tree_big$tip.label,\n    family = plant_species$Family[match(test_tree_big$tip.label, plant_species$Genus_species)]\n  )\n) -> test_tree_big_families\n## Branch lengths have been set to one.\n\nggtree(test_tree_big_families) + geom_tiplab() + coord_cartesian(xlim = c(0,300))"},{"path":"phylogenies.html","id":"trees-and-traits","chapter":"phylogenies","heading":"trees and traits","text":"plot traits alongside tree, can use ggtree combination ggplot. example. First, make tree:Next join tree data:Now can plot tree:IMPORTANT! plot traits, need reorder whatever shared axis (case, y axis) matches order tree. case, need reorder species names match order tree. can using reorder function, takes two arguments: thing reordered, thing reordered . case, want reorder species names y coordinate tree. can using y column data frame created fortified tree. can plot traits:Finally, can plot two plots together using plot_grid. important manually inspect tree tips y axis text make sure everything lines . don’t want plotting abundance one species y axis another species. case, everything looks good:manual inspection complete, can make new version plot y axis text removed trait plot can reduce margin left side trait plot make look nicer:","code":"\nchemical_bloom_tree <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold_in_path = \"http://thebustalab.github.io/data/angiosperms.newick\",\n  members = unique(chemical_blooms$label)\n)\n## IMPORTANT: Some species substitutions or removals were made as part of buildTree. Run build_tree_substitutions() to see them all.\n## Pro tip: most tree read/write functions reset node numbers. Fortify your tree and save it as a csv file to preserve node numbering.\ndata <- left_join(fortify(chemical_bloom_tree), chemical_blooms)\n## Joining with `by = join_by(label)`\nhead(data)\n## # A tibble: 6 × 18\n##   parent  node branch.length label  isTip     x     y branch\n##    <int> <int>         <dbl> <chr>  <lgl> <dbl> <dbl>  <dbl>\n## 1     80     1         290.  Ginkg… TRUE   352.     1   207.\n## 2     81     2         267.  Picea… TRUE   352.     2   219.\n## 3     81     3         267.  Cupre… TRUE   352.     3   219.\n## 4     84     4         135.  Eryth… TRUE   352.     5   285.\n## 5     86     5          16.2 Iris_… TRUE   352.     6   344.\n## 6     86     6          16.2 Iris_… TRUE   352.     7   344.\n## # ℹ 10 more variables: angle <dbl>, Alkanes <dbl>,\n## #   Sec_Alcohols <dbl>, Others <dbl>, Fatty_acids <dbl>,\n## #   Alcohols <dbl>, Triterpenoids <dbl>, Ketones <dbl>,\n## #   Other_compounds <dbl>, Aldehydes <dbl>\ntree_plot <- ggtree(data) +\n  geom_tiplab(\n    align = TRUE, hjust = 1, offset = 350,\n    geom = \"label\", label.size = 0, size = 3\n  ) +\n  scale_x_continuous(limits = c(0,750))\ntrait_plot <- ggplot(\n    data = pivot_longer(\n      dplyr::filter(data, isTip == TRUE),\n      cols = 10:18, names_to = \"compound\", values_to = \"abundance\"\n    ),\n    aes(x = compound, y = reorder(label, y), size = abundance)\n  ) +\n  geom_point() +\n  scale_y_discrete(name = \"\") +\n  theme(\n    plot.margin = unit(c(1,1,1,1), \"cm\")\n  )\nplot_grid(\n  tree_plot,\n  trait_plot,\n  nrow = 1, align = \"h\", axis = \"tb\"\n)\ntree_plot <- ggtree(data) +\n  geom_tiplab(\n    align = TRUE, hjust = 1, offset = 350,\n    geom = \"label\", label.size = 0, size = 3\n  ) +\n  scale_x_continuous(limits = c(0,750))\n\ntrait_plot <- ggplot(\n    data = pivot_longer(\n      filter(data, isTip == TRUE),\n      cols = 10:18, names_to = \"compound\", values_to = \"abundance\"\n    ),\n    aes(x = compound, y = reorder(label, y), size = abundance)\n  ) +\n  geom_point() +\n  scale_y_discrete(name = \"\") +\n  theme(\n    axis.text.y = element_blank(),\n    plot.margin = unit(c(1,1,1,-1.5), \"cm\")\n  )\n\nplot_grid(\n  tree_plot,\n  trait_plot,\n  nrow = 1, align = \"h\", axis = \"tb\"\n)"},{"path":"phylogenetic-analyses.html","id":"phylogenetic-analyses","chapter":"phylogenetic analyses","heading":"phylogenetic analyses","text":"use wrapper function run phylogenetic comparative analyses. ItFor , structural requirements: () tree needs phylo object (ii) traits need data.frame row species column variable, (iii) first column data.frame needs names species must exactly match tip labels tree (though don’t order), example:","code":"\nchemical_bloom_tree <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold_in_path = \"http://thebustalab.github.io/data/angiosperms.newick\",\n  members = unique(chemical_blooms$label)\n)\n## IMPORTANT: Some species substitutions or removals were made as part of buildTree. Run build_tree_substitutions() to see them all.\n## Pro tip: most tree read/write functions reset node numbers. Fortify your tree and save it as a csv file to preserve node numbering.\n\nrunPhylogeneticAnalyses(\n    traits = pivot_longer(chemical_blooms[,1:4], cols = c(3:4), names_to = \"trait\", values_to = \"value\"),\n    column_w_names_of_tiplabels = \"label\",\n    column_w_names_of_traits = \"trait\",\n    column_w_values_for_traits = \"value\",\n    tree = chemical_bloom_tree\n)\n## Joining with `by = join_by(trait)`\n## # A tibble: 310 × 17\n##    parent  node branch.length label isTip     x     y branch\n##     <int> <dbl>         <dbl> <chr> <lgl> <dbl> <dbl>  <dbl>\n##  1     80     1         290.  Gink… TRUE   352.     1   207.\n##  2     80     1         290.  Gink… TRUE   352.     1   207.\n##  3     81     2         267.  Pice… TRUE   352.     2   219.\n##  4     81     2         267.  Pice… TRUE   352.     2   219.\n##  5     81     3         267.  Cupr… TRUE   352.     3   219.\n##  6     81     3         267.  Cupr… TRUE   352.     3   219.\n##  7     84     4         135.  Eryt… TRUE   352.     5   285.\n##  8     84     4         135.  Eryt… TRUE   352.     5   285.\n##  9     86     5          16.2 Iris… TRUE   352.     6   344.\n## 10     86     5          16.2 Iris… TRUE   352.     6   344.\n## # ℹ 300 more rows\n## # ℹ 9 more variables: angle <dbl>, trait <chr>,\n## #   value <dbl>, trait_type <chr>,\n## #   phylogenetic_signal_k_value <dbl>,\n## #   phylogenetic_signal_k_p_value <dbl>,\n## #   phylogenetic_signal_lambda_value <dbl>,\n## #   phylogenetic_signal_lambda_p_value <dbl>, pic <dbl>\nchemical_bloom_tree <- buildTree(\n  scaffold_type = \"newick\",\n  scaffold_in_path = \"http://thebustalab.github.io/data/angiosperms.newick\",\n  members = unique(chemical_blooms$label)\n)\n## IMPORTANT: Some species substitutions or removals were made as part of buildTree. Run build_tree_substitutions() to see them all.\n## Pro tip: most tree read/write functions reset node numbers. Fortify your tree and save it as a csv file to preserve node numbering."},{"path":"phylogenetic-analyses.html","id":"phylogeneticsignal","chapter":"phylogenetic analyses","heading":"phylogeneticSignal","text":"Phylogenetic signal measure degree related species share similar trait values. used determine whether trait evolved manner consistent species’ evolutionary history. phylochemistry provides phylogeneticSignal function, can used calculate phylogenetic signal given set traits phylogenetic tree. example:","code":"\nphylogeneticSignal(\n  traits = pivot_longer(chemical_blooms, cols = c(2:10), names_to = \"compound\", values_to = \"value\"),\n  column_w_names_of_tiplabels = \"label\",\n  column_w_names_of_traits = \"compound\",\n  column_w_values_for_traits = \"value\",\n  tree = chemical_bloom_tree\n)\n##             trait trait_type n_species number_of_levels\n## 1         Alkanes continuous        78               NA\n## 2    Sec_Alcohols continuous        78               NA\n## 3          Others continuous        78               NA\n## 4     Fatty_acids continuous        78               NA\n## 5        Alcohols continuous        78               NA\n## 6   Triterpenoids continuous        78               NA\n## 7         Ketones continuous        78               NA\n## 8 Other_compounds continuous        78               NA\n## 9       Aldehydes continuous        78               NA\n##   evolutionary_transitions_observed\n## 1                                NA\n## 2                                NA\n## 3                                NA\n## 4                                NA\n## 5                                NA\n## 6                                NA\n## 7                                NA\n## 8                                NA\n## 9                                NA\n##   median_evolutionary_transitions_in_randomization\n## 1                                               NA\n## 2                                               NA\n## 3                                               NA\n## 4                                               NA\n## 5                                               NA\n## 6                                               NA\n## 7                                               NA\n## 8                                               NA\n## 9                                               NA\n##   minimum_evolutionary_transitions_in_randomization\n## 1                                                NA\n## 2                                                NA\n## 3                                                NA\n## 4                                                NA\n## 5                                                NA\n## 6                                                NA\n## 7                                                NA\n## 8                                                NA\n## 9                                                NA\n##   evolutionary_transitions_in_randomization\n## 1                                        NA\n## 2                                        NA\n## 3                                        NA\n## 4                                        NA\n## 5                                        NA\n## 6                                        NA\n## 7                                        NA\n## 8                                        NA\n## 9                                        NA\n##   phylogenetic_signal_k_value phylogenetic_signal_k_p_value\n## 1                 0.066016688                         0.167\n## 2                 2.045611108                         0.001\n## 3                 0.029719595                         0.711\n## 4                 0.069056092                         0.292\n## 5                 0.053761730                         0.354\n## 6                 0.566806846                         0.001\n## 7                 0.239488396                         0.030\n## 8                 0.018420367                         0.868\n## 9                 0.008613879                         0.928\n##   phylogenetic_signal_lambda_value\n## 1                           0.0001\n## 2                           0.9999\n## 3                           0.0001\n## 4                           0.0001\n## 5                           0.0001\n## 6                           0.9999\n## 7                           0.7874\n## 8                           0.0001\n## 9                           0.0001\n##   phylogenetic_signal_lambda_p_value\n## 1                              1.000\n## 2                              0.000\n## 3                              1.000\n## 4                              1.000\n## 5                              1.000\n## 6                              0.000\n## 7                              0.045\n## 8                              1.000\n## 9                              1.000"},{"path":"phylogenetic-analyses.html","id":"independentcontrasts","chapter":"phylogenetic analyses","heading":"independentContrasts","text":"Phylogenetic independent contrasts method analyzing relationship two traits taking account evolutionary history species studied. method involves transforming data “independent contrasts” remove effects shared ancestry, allowing accurate analysis relationship traits. phylochemistry provides independentContrasts function calculate phylogenetic independent contrasts given set traits phylogenetic tree. example calculating independent contrasts example dataset, followed generating linear model based contrasts.","code":"\ncontrasts <- independentContrasts(\n  traits = pivot_longer(chemical_blooms, cols = c(2:10), names_to = \"compound\", values_to = \"value\"),\n  column_w_names_of_tiplabels = \"label\",\n  column_w_names_of_traits = \"compound\",\n  column_w_values_for_traits = \"value\",\n  tree = chemical_bloom_tree\n)\n\n# buildLinearModel(\n#   data = contrasts,\n#   formula = \"Fatty_acids = Alkanes + 0\"\n# ) -> model\n# \n# ggplot(model$data) +\n#   geom_point(aes(x = input_x, y = input_y)) +\n#   geom_line(aes(x = model_x, model_y))"},{"path":"phylogenetic-analyses.html","id":"ancestraltraits","chapter":"phylogenetic analyses","heading":"ancestralTraits","text":"Ancestral trait reconstruction method infer characteristics (“traits”) ancestral organisms based traits modern descendants. examining traits present-day species using phylogenetic trees, can estimate “reconstruct” traits common ancestors. method can applied various types traits, including continuously varying discrete traits. Ancestral trait reconstruction helps us gain insights evolutionary processes historical transitions led current biodiversity. phylochemistry provides function ancestralTraits perform operations. Note ancestralTraits different buildTrees “ancestral_states”. “ancestral_states” estimates ancestral sequence states phylogeny nodes, ancestralTraits estimate traits ancestor, given traits extant species present leaves phylogeny. example. Please note ancestralTraits accepts data long-style data frame.addition providing ancestral state estimations, also function plotting estimations phylogeny: geom_ancestral_pie. example. Note cols vector column numbers correspond traits interest. pie_size size pie chart plotted node. geom_ancestral_pie relies columns input called trait value, output ancestralTraits. Note passing object ggtree() duplicate node names, need use distinct function remove duplicates, otherwise geom_ancestral_pie get confused place pies.","code":"\nanc_traits_tree <- ancestralTraits(\n  traits = pivot_longer(chemical_blooms, cols = -1),\n  column_w_names_of_tiplabels = \"label\",\n  column_w_names_of_traits = \"name\",\n  column_w_values_for_traits = \"value\",\n  tree = chemical_bloom_tree\n)\nhead(anc_traits_tree)\n## # A tibble: 6 × 11\n##   parent  node branch.length label  isTip     x     y branch\n##    <int> <dbl>         <dbl> <chr>  <lgl> <dbl> <dbl>  <dbl>\n## 1     80     1          290. Ginkg… TRUE   352.     1   207.\n## 2     80     1          290. Ginkg… TRUE   352.     1   207.\n## 3     80     1          290. Ginkg… TRUE   352.     1   207.\n## 4     80     1          290. Ginkg… TRUE   352.     1   207.\n## 5     80     1          290. Ginkg… TRUE   352.     1   207.\n## 6     80     1          290. Ginkg… TRUE   352.     1   207.\n## # ℹ 3 more variables: angle <dbl>, trait <chr>, value <dbl>\nggtree(\n  distinct(anc_traits_tree, node, .keep_all = TRUE)\n) +\n  geom_ancestral_pie(\n    data = filter(anc_traits_tree, isTip == FALSE),\n    pie_size = 0.1, pie_alpha = 1\n  ) +\n  geom_tiplab(offset = 20, align = TRUE) +\n  scale_x_continuous(limits = c(0,650)) +\n  theme_void()"},{"path":"comparative-genomics.html","id":"comparative-genomics","chapter":"comparative genomics","heading":"comparative genomics","text":"GENESPACE: syntenic pan-genome annotations eukaryotes","code":""},{"path":"comparative-genomics.html","id":"loading-gff-files","chapter":"comparative genomics","heading":"4.1 loading GFF files","text":"","code":""},{"path":"figures-captions.html","id":"figures-captions","chapter":"figures & captions","heading":"figures & captions","text":"","code":""},{"path":"figures-captions.html","id":"section-16","chapter":"figures & captions","heading":"","text":"","code":""},{"path":"figures-captions.html","id":"figures","chapter":"figures & captions","heading":"figures","text":"One first components preparing scientific manuscript creating high quality figures. Considering following figures:General Appearance:Create plots clean, professional, easy view distance. Ensure axes tick labels clear, non-overlapping, utilize available space efficiently enhanced readability precision. Use appealing (color blind-friendly) color palette differentiate data points categories. Tailor axes labels descriptive, select appropriate theme complements data maintains professionalism.Representing Data:Appropriate Geoms Annotations: Choose geoms best represent data help viewer evaluate hypothesis make desired comparison. Include raw data points possible detailed data distribution understanding. Consider apply statistical transformations like smoothing lines histograms appropriate provide deeper insights data. Consider using facets visualizing multiple categories groups, allowing easier comparison maintaining consistent scale layout. Adhere specific standards conventions relevant field, including representation data, error bars, statistical significance markers.","code":""},{"path":"figures-captions.html","id":"advanced-figure-elements","chapter":"figures & captions","heading":"advanced figure elements","text":"","code":""},{"path":"figures-captions.html","id":"insets","chapter":"figures & captions","heading":"insets","text":"zoomed insetsZoom certain plot regionsplot insetsimage insets","code":"\np <- ggplot(mpg, aes(displ, hwy, colour = factor(cyl))) +\n  geom_point() \n\ndata.tb <- \n  tibble(x = 7, y = 44, \n         plot = list(p + \n                       coord_cartesian(xlim = c(4.9, 6.2), \n                                       ylim = c(13, 21)) +\n                       labs(x = NULL, y = NULL) +\n                       theme_bw(8) +\n                       scale_colour_discrete(guide = \"none\")))\n\nggplot(mpg, aes(displ, hwy, colour = factor(cyl))) +\n  geom_plot(data = data.tb, aes(x, y, label = plot)) +\n  annotate(geom = \"rect\", \n           xmin = 4.9, xmax = 6.2, ymin = 13, ymax = 21,\n           linetype = \"dotted\", fill = NA, colour = \"black\") +\n  geom_point() \np <- ggplot(mpg, aes(factor(cyl), hwy, fill = factor(cyl))) +\n  stat_summary(geom = \"col\", fun = mean, width = 2/3) +\n  labs(x = \"Number of cylinders\", y = NULL, title = \"Means\") +\n  scale_fill_discrete(guide = \"none\")\n\ndata.tb <- tibble(x = 7, y = 44, \n                  plot = list(p +\n                                theme_bw(8)))\n\nggplot(mpg, aes(displ, hwy, colour = factor(cyl))) +\n  geom_plot(data = data.tb, aes(x, y, label = plot)) +\n  geom_point() +\n  labs(x = \"Engine displacement (l)\", y = \"Fuel use efficiency (MPG)\",\n       colour = \"Engine cylinders\\n(number)\") +\n  theme_bw()\nIsoquercitin_synthase <- magick::image_read(\"https://thebustalab.github.io/integrated_bioanalytics/images/homology2.png\")\ngrobs.tb <- tibble(x = c(0, 10, 20, 40), y = c(4, 5, 6, 9),\n                   width = c(0.05, 0.05, 0.01, 1),\n                   height =  c(0.05, 0.05, 0.01, 0.3),\n                   grob = list(grid::circleGrob(), \n                               grid::rectGrob(), \n                               grid::textGrob(\"I am a Grob\"),\n                               grid::rasterGrob(image = Isoquercitin_synthase)))\n\nggplot() +\n  geom_grob(data = grobs.tb, \n            aes(x, y, label = grob, vp.width = width, vp.height = height),\n            hjust = 0.7, vjust = 0.55) +\n  scale_y_continuous(expand = expansion(mult = 0.3, add = 0)) +\n  scale_x_continuous(expand = expansion(mult = 0.2, add = 0)) +\n  theme_bw(12)\n# ggplot() +\n#   annotate(\"grob\", x = 1, y = 3, vp.width = 0.5,\n#            label = grid::rasterGrob(image = Isoquercitin_synthase, width = 1)) +\n#   theme_bw(12)\n# bloom_example_pics <- ggplot(data = data.frame(x = c(0,1), y = c(0.5,0.5))) +\n#   geom_point(aes(x = x, y = y), color = \"white\") +\n#   theme_void() +\n#   annotation_custom(\n#       rasterGrob(\n#           png::readPNG(\n#               \"https://thebustalab.github.io/integrated_bioanalytics/images/homology2.png\"\n#           ), interpolate=TRUE\n#       ), xmin=0, xmax=1, ymin=0, ymax=1\n#   )"},{"path":"figures-captions.html","id":"composite-figures","chapter":"figures & captions","heading":"composite figures","text":"Many high quality figures composite figures one panel. simple way make figures R. First, make component composite figure send plot new object:Now, add together lay . Let’s look various ways lay :","code":"\ncolor_palette <- RColorBrewer::brewer.pal(11, \"Paired\")\nnames(color_palette) <- unique(alaska_lake_data$element)\n\nplot1 <- ggplot(\n  data = filter(alaska_lake_data, element_type == \"bound\"),\n  aes(y = lake, x = mg_per_L)\n) +\n  geom_col(\n    aes(fill = element), size = 0.5, position = \"dodge\",\n    color = \"black\"\n  ) +\n  facet_grid(park~., scales = \"free\", space = \"free\") +\n  theme_bw() + \n  scale_fill_manual(values = color_palette) +\n  scale_y_discrete(name = \"Lake Name\") +\n  scale_x_continuous(name = \"Abundance mg/L)\") +\n  theme(\n    text = element_text(size = 14)\n  )\n\nplot2 <- ggplot(\n  data = filter(alaska_lake_data, element_type == \"free\"),\n  aes(y = lake, x = mg_per_L)\n) +\n  geom_col(\n    aes(fill = element), size = 0.5, position = \"dodge\",\n    color = \"black\"\n  ) +\n  facet_grid(park~., scales = \"free\", space = \"free\") +\n  theme_bw() + \n  scale_fill_manual(values = color_palette) +\n  scale_y_discrete(name = \"Lake Name\") +\n  scale_x_continuous(name = \"Abundance mg/L)\") +\n  theme(\n    text = element_text(size = 14)\n  )\nplot_grid(plot1, plot2)\nplot_grid(plot1, plot2, ncol = 1)\nplot_grid(plot_grid(plot1,plot2), plot1, ncol = 1)"},{"path":"figures-captions.html","id":"exporting-graphics","chapter":"figures & captions","heading":"exporting graphics","text":"export graphics R, consider code .  something like: “C:\\Desktop\\the_file.png” (.e. path specific file .png suffix. file yet exist - already exist, overwritten. adjust height width get image look want, dialed , crank resolution 1200 2400 export final version.","code":"plot <- ggplot(data, aes(x = x, y = y)) + geom_point()\n\npng(filename = <path_to_file_you_want_to_create>, width = 8, height = 8, res = 600, units = \"in\")\n\nplot\n\ndev.off()plot <- ggplot(data, aes(x = x, y = y)) + geom_point()\n\npdf(filename = <path_to_file_you_want_to_create>, width = 8, height = 8)\n\nplot\n\ndev.off()"},{"path":"figures-captions.html","id":"captions","chapter":"figures & captions","heading":"captions","text":"Figures critical tools clearly effectively communicating scientific results. However, Reviewer 2 tell , figure good caption. Captions provide essential context, guiding reader significance, structure, details visual information presented. guidelines help craft informative captions. recommendations organized categories, covering essential components like figure titles, panel descriptions, variable definitions, data representation details, statistical analyses, data sources. example captions helpful interactive tool (buildCaption()) also included streamline caption construction ensure consistency scientific communication.","code":""},{"path":"figures-captions.html","id":"title-and-text","chapter":"figures & captions","heading":"title and text","text":"Figure Title:Figure Title:Provide concise, descriptive title summarizes overall message purpose figure.Provide concise, descriptive title summarizes overall message purpose figure.Ensure title quickly informs reader main topic, experimental system, hypothesis addressed figure.Ensure title quickly informs reader main topic, experimental system, hypothesis addressed figure.Caption Text:Caption Text:Avoid using unexplained abbreviations jargon. abbreviations necessary, provide definitions first use.Avoid using unexplained abbreviations jargon. abbreviations necessary, provide definitions first use.","code":""},{"path":"figures-captions.html","id":"panel-by-panel-descriptions","chapter":"figures & captions","heading":"panel-by-panel descriptions","text":"Panel Identification:\nLabel panel (e.g., , B, C, etc.) refer labels consistently caption.\nLabel panel (e.g., , B, C, etc.) refer labels consistently caption.Graph Type Layout:\nState type plot (line plot, bar chart, scatter plot, histogram, etc.). doubt, “plot” okay.\nDescribe special features insets, overlays, embedded plots (e.g., zoomed regions, additional mini-panels).\nState type plot (line plot, bar chart, scatter plot, histogram, etc.). doubt, “plot” okay.Describe special features insets, overlays, embedded plots (e.g., zoomed regions, additional mini-panels).Axes Variables:\nClearly define axis (x vs. y) descriptive terms, including units measurement (e.g., time seconds, concentration µM).\nExplain additional dimensions (color coding, marker sizes, symbols) used represent extra variables.\nClearly define axis (x vs. y) descriptive terms, including units measurement (e.g., time seconds, concentration µM).Explain additional dimensions (color coding, marker sizes, symbols) used represent extra variables.Data Representation Details:\nDescribe individual data points, bars, error bars represent. example:\nData Points/Bars: Explain whether indicate individual measurements, means, medians, summary statistics.\nError Bars: Specify whether indicate standard error, standard deviation, 95% confidence intervals, another metric.\n\nNote graphical elements like trend lines regression lines model fit applied.\nDescribe individual data points, bars, error bars represent. example:\nData Points/Bars: Explain whether indicate individual measurements, means, medians, summary statistics.\nError Bars: Specify whether indicate standard error, standard deviation, 95% confidence intervals, another metric.\nData Points/Bars: Explain whether indicate individual measurements, means, medians, summary statistics.Error Bars: Specify whether indicate standard error, standard deviation, 95% confidence intervals, another metric.Note graphical elements like trend lines regression lines model fit applied.Sample Size Replicates:\nIndicate number independent samples experimental replicates underlying element graph.\nIndicate number independent samples experimental replicates underlying element graph.Statistical Analysis Comparisons:\nDescribe control experiments baseline data presented figure, including used validate compare experimental results.\nState statistical tests used (e.g., t-test, ANOVA, regression analysis) significance level(s).\nDescribe statistical significance indicated figure (e.g., asterisks, brackets, p-value annotations).\nProvide necessary details data normalization, transformation, curve fitting influence data interpretation.\nfigures scale bars reference markers (e.g., microscopy images), specify scale explicitly.\nDescribe control experiments baseline data presented figure, including used validate compare experimental results.State statistical tests used (e.g., t-test, ANOVA, regression analysis) significance level(s).Describe statistical significance indicated figure (e.g., asterisks, brackets, p-value annotations).Provide necessary details data normalization, transformation, curve fitting influence data interpretation.figures scale bars reference markers (e.g., microscopy images), specify scale explicitly.","code":""},{"path":"figures-captions.html","id":"data-source-and-methodology","chapter":"figures & captions","heading":"data source and methodology","text":"Data Origins Methodology:\nClearly state data come (e.g., experimental assays, clinical samples, simulations, databases).\ndata derived previously published work public repository, include proper references accession numbers, reasonable / possible.\nConsider including summary methods used obtain generate data. standard practice fields - look articles field typically include.\nConsider including information experimental conditions (e.g., treatment concentrations, temperature, environmental conditions) computational parameters (e.g., algorithm settings), assuming weren’t already mentioned describing axes.\nMention image processing steps (e.g., brightness/contrast adjustments, background subtraction) steps critical understanding visual data.\nClearly state data come (e.g., experimental assays, clinical samples, simulations, databases).data derived previously published work public repository, include proper references accession numbers, reasonable / possible.Consider including summary methods used obtain generate data. standard practice fields - look articles field typically include.Consider including information experimental conditions (e.g., treatment concentrations, temperature, environmental conditions) computational parameters (e.g., algorithm settings), assuming weren’t already mentioned describing axes.Mention image processing steps (e.g., brightness/contrast adjustments, background subtraction) steps critical understanding visual data.","code":""},{"path":"figures-captions.html","id":"example-captions","chapter":"figures & captions","heading":"example captions","text":"\nFigure 4.1: Figure 1: Carbon, nitrogen, phosphorous Alaskan lakes. ) bar chart showing abundance (mg per L, x-axis) bound elements (C, N, P) various Alaskan lakes (lake names y-axis) located one three parks Alaska (park names right y groupings). B) bar chart showing abundance (mg per L, x-axis) free elements (Cl, S, F, Br, Na, K, Ca, Mg) various Alaskan lakes (lake names y-axis) located one three parks Alaska (park names right y groupings). data public chemistry data repository. bar represents result single measurement single analyte, identity coded using color shown color legend. Abbreviations: BELA - Bering Land Bridge National Preserve, GAAR - Gates Arctic National Park & Preserve, NOAT - Noatak National Preserve.\n","code":""},{"path":"figures-captions.html","id":"buildcaption","chapter":"figures & captions","heading":"buildCaption","text":"help manage suggestions , please consider using buildCaption() tool, can open using command . command open interactive window checklist help quickly build quality captions.","code":"\nbuildCaption()"},{"path":"figures-captions.html","id":"section-17","chapter":"figures & captions","heading":"","text":"","code":""},{"path":"figures-captions.html","id":"further-reading-9","chapter":"figures & captions","heading":"further reading","text":"Grammar extensions insets ggpp\narticle explains use ggpp extension add insets annotations ggplot2 graphics R. introduces grammar extensions allow insert subplots, highlight specific regions, incorporate custom graphical elements composable expressive way. Particularly useful emphasizing detail providing context within complex figures.Grammar extensions insets ggpp\narticle explains use ggpp extension add insets annotations ggplot2 graphics R. introduces grammar extensions allow insert subplots, highlight specific regions, incorporate custom graphical elements composable expressive way. Particularly useful emphasizing detail providing context within complex figures.Patchwork: Simple plot layout ggplot2Patchwork elegant intuitive package arranging multiple ggplot2 plots single composite figure. minimal syntax mirrors mathematical layout expressions, allows users combine plots vertically, horizontally, nested arrangements—ideal creating figure panels publications presentations.Patchwork: Simple plot layout ggplot2Patchwork elegant intuitive package arranging multiple ggplot2 plots single composite figure. minimal syntax mirrors mathematical layout expressions, allows users combine plots vertically, horizontally, nested arrangements—ideal creating figure panels publications presentations.Cowplot: Versatile plot compositionCowplot another popular package composing multiple ggplot2 plots. offers control customization patchwork, particularly aligning plots, adjusting spacing, embedding annotations. makes well-suited fine-tuned figure design preparing publication-quality graphics.Cowplot: Versatile plot compositionCowplot another popular package composing multiple ggplot2 plots. offers control customization patchwork, particularly aligning plots, adjusting spacing, embedding annotations. makes well-suited fine-tuned figure design preparing publication-quality graphics.","code":""},{"path":"question-driven-report.html","id":"question-driven-report","chapter":"question-driven report","heading":"question-driven report","text":"structured report following components:\nCover page: Title author, Introductory paragraph\nOne section three scientific question. section one figure + caption, well one paragraphs explaining figure conclusions follow .\nReferences\nCover page: Title author, Introductory paragraphOne section three scientific question. section one figure + caption, well one paragraphs explaining figure conclusions follow .References","code":""},{"path":"question-driven-report.html","id":"section-18","chapter":"question-driven report","heading":"","text":"","code":""},{"path":"question-driven-report.html","id":"brief-structural-example","chapter":"question-driven report","heading":"brief structural example","text":"Chemical Pollutants Minnesota SoilsIn order better understand pollution state Minnesota, study focused detailed analyses chemical measurements soil samples 300 sites around state. analyses consisted principal components analysis determine sites similar one another, thus answering question sites exhibited similar pollutant profiles (Section 2.1). first analysis followed statistical tests see whether differences detected sites’ chemistry, answering question whether significant differences pollutant profiles site (Section 2.2).2.1 Principal Components AnalysisTo understand relationships sites soil chemistry sampled, principal components analysis used. 20 different analytes, contained halogen atoms, included analysis. scatter plot showing position 300 samples space defined dimesions 1 2 (explain 54% 35% total variance dataset, respectively), revealed two major clusters present, small number outliers (Fig. 1). color coding two clusters according whether samples collected rural versus urban areas, possible see first cluster made almost exclusively samples urban areas, second cluster made almost entirely samples rural areas. suggested variance pollutant chemistry among samples collected assocaited urban versus rural environments.2.2 Statistical AnalysesUsing groupings identified via principal components analysis, statistical tests conducted determine chemical abundances differed groups. Tests normality homogeneity variance (Shapiro Levene tests) revealed data assessed using ANOVA instead required use non-parametric test. Accordingly, Kruskall-Wallis test followed post-hoc Dunn tests applied, showed abundances halogenated pollutants significantly higher urban versus rural areas (p = 0.0035, Fig. 2A). direct observations consistent conclusions drawn others recent literature reviews focused hydrocarbon compounds (Petrucci et al., 2018; Hendrix et al., 2019). Thus, new chemical analyses presented demonstrate discrepancy urban versus rural pollution true hydrocarbon compounds (found previously), also halogenated compounds. Together, findings strongly suggest either cities source pollution mechanism concentrates pollution cities.","code":""},{"path":"question-driven-report.html","id":"structure","chapter":"question-driven report","heading":"structure","text":"(key: number suggested sentences: purpose: “example”)Title Author\n1 Use 75-140 characters (ideally 125 characters). essentially two types titles: descriptive titles mechanistic titles.\nmanuscript exploratory research, consider using descriptive title. example: “Comparative analysis carbon, sulfur, phoshorous chemistry six Alaskan lakes.”\nmanuscript hypothesis-driven research, consider using mechanistic title. example: “Dissolved organic carbon Alaskan lakes heavily influenced water pH temperature.”\ngood title :\nindicative content paper\nAttract interest potential readers\nReflect whether article deascriptive mechanistic\nInclude important keywords\n\n\n1 Use 75-140 characters (ideally 125 characters). essentially two types titles: descriptive titles mechanistic titles.\nmanuscript exploratory research, consider using descriptive title. example: “Comparative analysis carbon, sulfur, phoshorous chemistry six Alaskan lakes.”\nmanuscript hypothesis-driven research, consider using mechanistic title. example: “Dissolved organic carbon Alaskan lakes heavily influenced water pH temperature.”\ngood title :\nindicative content paper\nAttract interest potential readers\nReflect whether article deascriptive mechanistic\nInclude important keywords\n\nmanuscript exploratory research, consider using descriptive title. example: “Comparative analysis carbon, sulfur, phoshorous chemistry six Alaskan lakes.”manuscript hypothesis-driven research, consider using mechanistic title. example: “Dissolved organic carbon Alaskan lakes heavily influenced water pH temperature.”good title :\nindicative content paper\nAttract interest potential readers\nReflect whether article deascriptive mechanistic\nInclude important keywords\nindicative content paperAttract interest potential readersReflect whether article deascriptive mechanisticInclude important keywordsIntroductory paragraph:\n1: State aim report: “objective report …”\n3-4: Call subsections report according methodology scientific questions: “used method X quantify property Y study subject address question”property Y vary based date sample collected?” (section 2.1).”\n1: State aim report: “objective report …”3-4: Call subsections report according methodology scientific questions: “used method X quantify property Y study subject address question”property Y vary based date sample collected?” (section 2.1).”subsection paragraph:\n\n1: Purpose work described paragraph: “order determine…”\n1: Review methods experimental design specific subsection (necessary)\n4-5: Results method experiment (.e. data features)\n1-2: Comparison new results literature (possible)\n1-2: Conclusion combined results concluding remark “Thus, analysis X revealed …”\n\n1: Purpose work described paragraph: “order determine…”1: Review methods experimental design specific subsection (necessary)4-5: Results method experiment (.e. data features)1-2: Comparison new results literature (possible)1-2: Conclusion combined results concluding remark “Thus, analysis X revealed …”\n","code":""},{"path":"question-driven-report.html","id":"suggestions","chapter":"question-driven report","heading":"suggestions","text":"efficient way write format outlined ? Yes. Follow step--step instructions :","code":""},{"path":"question-driven-report.html","id":"outline-then-draft-paragraphs","chapter":"question-driven report","heading":"outline then draft paragraphs","text":"Identify “data features” -> “conclusion” combinations. Using figures, make list potentially interesting features data, pair feature possible conclusiona lead . Example:“GC-MS data presented indicates cities higher levels pollution rural areas (Fig. 1),” (data feature)“suggesting either cities source pollution mechanism concentrates pollution cities.” (conclusion)Perform targeted literature searches. Expand “data feature” -> “conclusion” combinations “supplementary information” “literature information”. Example:“GC-MS data presented indicates cities higher levels pollution rural areas.” (data feature)“direct observations consistent conclusions drawn others recent literature reviews (Petrucci., 2018; Hendrix et al., 2019)” (literature information)“Overall, suggests either cities source pollution mechanism concentrates pollution cities.” (conclusion)Group “data feature” -> “supp/lit info” -> “conclusion” combinations paragraphs. Edit conclusion highlights new contribution data makes situation. Also consider whether parargaphs now suggest existence mechanisms. Example (note conclusion sentence italics highlights new findings):“GC-MS data presented indicates cities higher levels pollution rural areas (Fig. 1). direct observations consistent meta-analyses previously published observations (Supplemental Figure 1), well conclusions drawn others recent literature reviews (et al., 2018; person et al., 2019). new chemical analyses presented thus confirm true hydrocarbon compounds, extend observation halogenated compounds atmosphere. Together findings strongly suggest either cities source pollution mechanism concentrates pollution cities.","code":""},{"path":"question-driven-report.html","id":"order-then-edit-paragraphs","chapter":"question-driven report","heading":"order then edit paragraphs","text":"Identify paragraph characteristics groupConsider whether paragraphs prerequisites others whether paragraphs can grouped according topic.Group paragraphs according topic prerequisite dependencies (putting prereq dependencies close eachother possible.)Rearrange paragraph groups Create natural flow. Consider:Starting group paragraphs relevant overall pitch/goal paperEnding group paragraphs future perspectiveEnding strong suit (.e. something speculative)Consider putting orphaned paragraphs (shortened version ) conclusion section.Edit transitions groups. Edit paragraph, particularly first last sentences, connect paragraphs flowing document. Specifically, means several things:implicit cross-paragraph references (.e. new paragraph begin “compound described exhibited interesting properties”, rather, “3-hydroxycinnamic acid exhibited interesting properties.”).abrupt jumps subject paragraphs, consider breaking discussion subsections help reader identify logical resting points.discussion require reader go back read first half order understand second half.","code":""},{"path":"links.html","id":"links","chapter":"links","heading":"links","text":"","code":""},{"path":"links.html","id":"geoms","chapter":"links","heading":"geoms","text":"geoms ggplot2 cheatsheet","code":""},{"path":"links.html","id":"colors","chapter":"links","heading":"colors","text":"ColorBrewer2","code":""},{"path":"datasets.html","id":"datasets","chapter":"datasets","heading":"datasets","text":"","code":""},{"path":"datasets.html","id":"alaska_lake_data","chapter":"datasets","heading":"alaska_lake_data","text":"Alaska Lake Data collected part water quality monitoring initiative across various lakes protected national parks, aimed assessing chemical composition environmental conditions unique ecosystems. Researchers took water samples several lakes, measuring key environmental parameters like water temperature pH, along analyzing abundance different chemical elements, carbon, nitrogen, phosphorus. comparing concentrations bound free elements, study aimed understand health aquatic environments impact natural anthropogenic factors water chemistry. dataset used inform conservation strategies maintaining ecological balance sensitive regions.\n- lake (Categorical): name lake water sample collected. refers sample.\n- park (Categorical): park national park code lake located. part sample identification.\nwater_temp (Continuous): water temperature (degrees Celsius) sample time collection. analyte describing environmental condition sample.\n- pH (Continuous): pH value water, representing acidity alkalinity. analyte providing environmental characteristic sample.\n- element (Categorical): chemical element measured water (e.g., C carbon, N nitrogen). analyte.\n- mg_per_L (Continuous): concentration (milligrams per liter) corresponding analyte element column, indicating abundance analyte water sample.\n- element_type (Categorical): Describes whether element “bound” “free” state, providing context form analyte.","code":""},{"path":"datasets.html","id":"algae_data","chapter":"datasets","heading":"algae_data","text":"dataset generated part study investigating biochemical composition different algae strains varying harvesting conditions. goal research examine different algae strains harvesting regimes affect abundance various chemical species, particularly fatty acids amino acids, potential applications biofuel production nutritional supplements. Replicates performed ensure consistency, wide range chemical species measured provide insights algae’s metabolic profile response environmental harvesting changes.replicate (Categorical): replicate number sample experiment, indicating iteration algae sample analyzed.algae_strain (Categorical): specific strain algae used experiment (e.g., “Tsv1”). refers strain sample collected.harvesting_regime (Categorical): method condition algae sample harvested (e.g., “Heavy” regime).chemical_species (Categorical): type chemical species analyte measured algae sample, including various fatty acids (FAs) amino acids (Aas).abundance (Continuous): measured abundance chemical species analyte algae sample, expressed continuous quantitative form (e.g., mg/L similar units).","code":""},{"path":"datasets.html","id":"beer_components","chapter":"datasets","heading":"beer_components","text":"dataset captures volatile compounds released different ingredients like barley corn, likely part study food aroma profiles. Researchers measured abundance specific analytes (2-Methylpropanal) classified chemical group (e.g., Aldehydes). goal assess different ingredients contribute overall aroma linking analyte sensory descriptors, include odor characteristics “Green,” “Pungent,” “Malty.” dataset useful food science research.ingredient (Categorical): ingredient analytes measured (e.g., “barley,” “corn”). refers ingredient sample collected.replicate (Categorical): replicate number sample experiment, indicating repetition measurement consistency.analyte (Categorical): specific volatile compound chemical measured ingredient (e.g., “2-Methylpropanal”).analyte_class (Categorical): chemical classification analyte (e.g., “Aldehyde”).abundance (Continuous): concentration analyte measured sample, likely quantitative unit mg/L.analyte_odor (Categorical): sensory descriptor odors associated analyte, listed combination descriptors (e.g., “Green; Pungent; Burnt; Malty; Toasted”).","code":""},{"path":"datasets.html","id":"hawaii_aquifers","chapter":"datasets","heading":"hawaii_aquifers","text":"dataset represents water quality measurements various wells within aquifer system, collected part study groundwater composition. Researchers measured abundance different dissolved elements compounds, silica (SiO2) chloride (Cl), different wells aquifer. dataset used assess chemical profile groundwater monitor changes water quality time. Note absence geospatial data (latitude longitude) certain samples, note samples come well aquifer different latitude longitude coordinates.aquifer_code (Categorical): code assigned identify aquifer system (e.g., “aquifer_1”) sample came .well_name (Categorical): name well water sample collected (e.g., “Alewa_Heights_Spring”).longitude (Continuous): longitudinal coordinates well location sample take.latitude (Continuous): latitudinal coordinates well location sample take.analyte (Categorical): specific dissolved compound element measured water sample (e.g., “SiO2,” “Cl”).abundance (Continuous): concentration analyte water sample, expressed quantitative unit (mg/L).","code":""},{"path":"datasets.html","id":"hops_components","chapter":"datasets","heading":"hops_components","text":"dataset contains detailed information various hop varieties used brewing, including country origin, brewing usage (aroma bittering), chemical composition essential oils acids. dataset serves resource brewers select hop varieties based aroma profiles chemical content, alpha acids essential oils like humulene myrcene, influence bitterness, flavor, aroma beer. goal dataset help optimize hop selection process brewing specific flavor profiles brewing techniques like dry hopping bittering.hop_variety (Categorical): specific variety hop used (e.g., “Cascade,” “Chinook”).hop_origin (Categorical): country origin hop variety (e.g., “USA,” “England”).hop_brewing_usage (Categorical): primary use hop brewing, either aroma bittering, techniques like dry hopping.hop_aroma (Categorical): sensory description hop’s aroma profile, can include terms like “floral,” “citrus,” “spicy.”total_oil (Continuous): total essential oil content hop, typically measured milliliters per 100 grams.alpha_acids (Continuous): percentage alpha acids hop, contribute bitterness beer.beta_acids (Continuous): percentage beta acids hop, also contribute bitterness degrade slowly time.humulene (Continuous): compound contributing hop’s woody, earthy aroma.myrcene (Continuous): compound contributes hop’s citrus floral aroma.humulone (Continuous): Another compound found hop oils, related bitterness aroma.caryophyllene (Continuous): compound contributing spicy, peppery aromas.farnesene (Continuous): compound contributing green, woody, fruity aromas.","code":""},{"path":"r-faq.html","id":"r-faq","chapter":"r faq","heading":"r faq","text":"","code":""},{"path":"r-faq.html","id":"updating-r-and-r-packages","chapter":"r faq","heading":"Updating R and R Packages","text":"Close RStudio, open plain R GUI, run following:Mac:PC:","code":"\ninstall.packages('remotes') #assuming it is not remotes installed\nremotes::install_github('andreacirilloac/updateR')\nupdateR::updateR()\ninstall.packages(\"installr\")\ninstallr::updateR()"},{"path":"r-faq.html","id":"ordering-1","chapter":"r faq","heading":"ordering","text":"list numeric element inherent order : -inf -> +inf. list character element also inherent order : -> Z, ’s mixed number letter list (interpreted R character list): 0 -> 9 -> -> Z.However, cases want list character elements order -> Z. cases, want convert list character elements list factor elements. Factors lists character elements inherent order -> Z. example, plot , y axis , perhaps, “correct” order:fix ? need convert column group_number list factors correct order (see ). , use command factor, accept argument called levels can define order characters :Notice now look type data contained column group_number says “”. great! means converted column list factors, instead characters. Now happens make plot?VICTORY!","code":"\nggplot(periodic_table) +\n  geom_point(aes(y = group_number, x = atomic_mass_rounded))\nperiodic_table$group_number <- factor(\n  periodic_table$group_number,\n  levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"lanthanides\", \"actinides\")\n)\n\nperiodic_table\n## # A tibble: 118 × 41\n##    atomic_number element_name atomic_symbol group_number\n##            <dbl> <chr>        <chr>         <fct>       \n##  1             1 hydrogen     H             1           \n##  2             2 helium       He            18          \n##  3             3 lithium      Li            1           \n##  4             4 beryllium    Be            2           \n##  5             5 boron        B             13          \n##  6             6 carbon       C             14          \n##  7             7 nitrogen     N             15          \n##  8             8 oxygen       O             16          \n##  9             9 fluorine     F             17          \n## 10            10 neon         Ne            18          \n## # ℹ 108 more rows\n## # ℹ 37 more variables: period <dbl>,\n## #   atomic_mass_rounded <dbl>, melting_point_C <dbl>,\n## #   boiling_point_C <dbl>, state_at_RT <chr>,\n## #   density_g_per_mL <dbl>,\n## #   electronegativity_pauling <dbl>,\n## #   first_ionization_poten_eV <dbl>, …\nggplot(periodic_table) +\n  geom_point(aes(y = group_number, x = atomic_mass_rounded))"},{"path":"r-faq.html","id":"column-manipulation","chapter":"r faq","heading":"column manipulation","text":"select specific columns:remove certain columns:","code":"\nalaska_lake_data %>%\n  select(water_temp, pH)\n## # A tibble: 220 × 2\n##    water_temp    pH\n##         <dbl> <dbl>\n##  1       6.46  7.69\n##  2       6.46  7.69\n##  3       6.46  7.69\n##  4       6.46  7.69\n##  5       6.46  7.69\n##  6       6.46  7.69\n##  7       6.46  7.69\n##  8       6.46  7.69\n##  9       6.46  7.69\n## 10       6.46  7.69\n## # ℹ 210 more rows\nalaska_lake_data %>%\n  select(!water_temp)\n## # A tibble: 220 × 6\n##    lake            park     pH element mg_per_L element_type\n##    <chr>           <chr> <dbl> <chr>      <dbl> <chr>       \n##  1 Devil_Mountain… BELA   7.69 C          3.4   bound       \n##  2 Devil_Mountain… BELA   7.69 N          0.028 bound       \n##  3 Devil_Mountain… BELA   7.69 P          0     bound       \n##  4 Devil_Mountain… BELA   7.69 Cl        10.4   free        \n##  5 Devil_Mountain… BELA   7.69 S          0.62  free        \n##  6 Devil_Mountain… BELA   7.69 F          0.04  free        \n##  7 Devil_Mountain… BELA   7.69 Br         0.02  free        \n##  8 Devil_Mountain… BELA   7.69 Na         8.92  free        \n##  9 Devil_Mountain… BELA   7.69 K          1.2   free        \n## 10 Devil_Mountain… BELA   7.69 Ca         5.73  free        \n## # ℹ 210 more rows"},{"path":"r-faq.html","id":"user-color-palettes","chapter":"r faq","heading":"user color palettes","text":"Suppose want create specific color palette pack alaska_lake_data. three unique parks:First define colors want:name vector according park want color:Now feed object values argument scale_color_manual (scale_fill_manual, want fill):","code":"\nunique(alaska_lake_data$park)\n## [1] \"BELA\" \"GAAR\" \"NOAT\"\ncustom_colors_for_lakes <- c(\"#1a9850\", \"#ffffbf\", \"#d73027\")\ncustom_colors_for_lakes\n## [1] \"#1a9850\" \"#ffffbf\" \"#d73027\"\nnames(custom_colors_for_lakes) <- c(\"GAAR\", \"NOAT\", \"BELA\")\ncustom_colors_for_lakes\n##      GAAR      NOAT      BELA \n## \"#1a9850\" \"#ffffbf\" \"#d73027\"\nggplot(alaska_lake_data) + \n  geom_point(aes(x = pH, y = water_temp, fill = park), size = 5, shape = 21, color = \"black\") +\n  scale_fill_manual(values = custom_colors_for_lakes) +\n  theme_classic()"},{"path":"templates.html","id":"templates","chapter":"templates","heading":"templates","text":"","code":""},{"path":"templates.html","id":"matrix-analyses","chapter":"templates","heading":"4.2 matrix analyses","text":"","code":""},{"path":"templates.html","id":"basic-runmatrixanalysis-template","chapter":"templates","heading":"4.2.1 basic runMatrixAnalysis() template","text":"","code":"\n\nrunMatrixAnalysis(\n                \n  data = NULL,\n\n  analysis = c(\"hclust\", \"pca\", \"pca_ord\", \"pca_dim\"),\n\n  column_w_names_of_multiple_analytes = NULL,\n  column_w_values_for_multiple_analytes = NULL,\n    \n  columns_w_values_for_single_analyte = NULL,\n\n  columns_w_sample_ID_info = NULL\n\n)"},{"path":"templates.html","id":"advanced-runmatrixanalysis-template","chapter":"templates","heading":"4.2.2 advanced runMatrixAnalysis() template","text":"","code":"\n\nrunMatrixAnalysis(\n  data, # data to use for analysis\n  analysis = c(\n      \"pca\", \"pca_ord\", \"pca_dim\", # PCA\n      \"mca\", \"mca_ord\", \"mca_dim\", # MCA (PCA on categorical data)\n      \"mds\", \"mds_ord\", \"mds_dim\", # MDS\n      \"tsne\", \"dbscan\", \"kmeans\", # Clustering\n      \"hclust\", \"hclust_phylo\" # Hierarchical clustering\n  ),\n  parameters = NULL,\n  column_w_names_of_multiple_analytes = NULL,\n  column_w_values_for_multiple_analytes = NULL,\n  columns_w_values_for_single_analyte = NULL,\n  columns_w_additional_analyte_info = NULL,\n  columns_w_sample_ID_info = NULL,\n  transpose = FALSE, # default = FALSE, this chooses whether to transpose the data\n  distance_method = c( # the distance metric to use in computing a distance matrix\n    \"euclidean\", \"maximum\",\n    \"manhattan\", \"canberra\",\n    \"binary\", \"minkowski\",\n    \"coeff_unlike\"\n  ),\n  agglomeration_method = c( # the clustering method to use in heirarchical clustering\n      \"ward.D2\", \"ward.D\", \"single\", \"complete\",\n      \"average\", # (= UPGMA)\n      \"mcquitty\", # (= WPGMA)\n      \"median\", # (= WPGMC)\n      \"centroid\" # (= UPGMC)\n  ),\n  unknown_sample_ID_info = NULL,\n  components_to_return = 2, # how many principal components to return\n  scale_variance = NULL, ## default = TRUE, except for hclust, then default = FALSE\n  na_replacement = c(\"mean\", \"none\", \"zero\", \"drop\"), # default = \"mean\", this chooses what to do with missing values\n  output_format = c(\"wide\", \"long\"), # default = \"wide\", this chooses whether to output a wide or long format\n)"}]
