[["index.html", "R For Chemists Chapter 1 Introduction", " R For Chemists Lucas Busta 2020-11-03 Chapter 1 Introduction "],["installation.html", "Chapter 2 Installation 2.1 Installing R 2.2 Installing RStudio 2.3 Verifying installations 2.4 Installing the tidyverse", " Chapter 2 Installation 2.1 Installing R R is the computing language we will use to run our chemometric analyses and produce high quality plots. If you already have R installed, you can go straight to installing RStudio. If not, follow these steps to install R: Go to https://cran.r-project.org/ Click on “Download R for &lt;your operating system&gt;” (see footnote), depending on your operating system you will select “Download R for Linux”, “Download R for (Mac) OS X”, or “Download R for Windows”. footnote: We will use &lt;this notation&gt; quite a bit. It indicates a place where you should insert information, data, or something similar that corresponds to your particular situation. In this example it means insert “your operating system”, i.e. Linux, (Mac) OS X, or Windows. For Mac: download the .pkg file for the latest release. As of 8/31/2020, this is R-4.0.2.pkg. For PC: click “install R for the first time”, then click “Download R 4.0.2 for Windows”. After the executable finishes downloading (in Windows, it is a file with .exe extension; for Mac, it is a .dmg file or a .dmg inside a .pkg file), open the file as an administrator, and follow the installation instructions. R should install without any problems. You can click OK for all of the windows that pop-up during installation, and choose a “regular” installation (if given the choice). If you have trouble installing R please google “Install R Mac” or “Install R PC” and following one the many video tutorials out there. If you have tried this and are still having trouble, please contact me. 2.2 Installing RStudio Once we install R, we can install RStudio, which is essentially a convenient way of interacting with R. Some people do not like RStudio and prefer to interact with R directly. This is fine, but many beginning R users find RStudio helpful, so I recommend it. Follow these steps to install RStudio: Go to https://rstudio.com/ Click “DOWNLOAD” at the top of the page. Click the “DOWNLOAD” button that corresponds to RStudio Desktop with the free Open Source License. The page may automatically detect which operating system you are using and recommend a version for you. If it does, download that file (.exe for PC or .dmg for Mac). If not, scroll down to the “All Installers” section and download the file that is right for you. Open the file as an administrator, and follow the installation instructions. RStudio should install without any problems. You can click OK for all of the windows that pop-up during installation, and choose a “regular” installation (if given the choice). If you have trouble installing RStudio please google “Install RStudio Mac” or “Install RStudio PC” and following one the many video tutorials out there. If you have tried this and are still having trouble, please contact me. 2.3 Verifying installations Open RStudio by clicking on the appropriate file in your applications folder, or wherever it is saved on your computer. You will see several windows. One is the Code Editor, one is the R Console, one is the Workspace and History, and one is the Plots and Files window. The R Console window should have a &gt; in it. Type head(Indometh). This should display the first six lines of a data set describing the pharmacokinets of indomethacin. This is one of the built in datasets in R - you do not need any additional files to run this test. head(Indometh) ## # A tibble: 6 x 3 ## Subject time conc ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1.5 ## 2 1 0.5 0.94 ## 3 1 0.75 0.78 ## 4 1 1 0.48 ## 5 1 1.25 0.37 ## 6 1 2 0.19 Next, type plot(Indometh) into the R Console. This will plot the indomethacin dataset in a basic way. plot(Indometh) If both the above commands (head(Indometh) and plot(Indometh)) worked and there were no error messages during installation, then you should be ready to proceed. 2.4 Installing the tidyverse For us to run our analyses, we need to install a set of add-on functions that expand R’s capabilities. These functions are collected in something called the tidyverse, a very well-known and widely-used R package developed by Hadley Wickham. You do not need to manually download anything to complete this installation - R will do it for you. In the R Console, type install.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\") to install the tidyverse. RSudio might ask you: “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)”, for now, type no and press enter. install.packages(&quot;tidyverse&quot;, repos = &quot;http://cran.us.r-project.org&quot;) ## Error in install.packages : Updating loaded packages Let’s make sure your version of the tidyverse is installed correctly. To do this, we will load the tidyverse library/package inside of an R session. We can do this using library(tidyverse). Let’s try it: library(tidyverse) If the library load correctly - then you are set to go! If not, try updating your R / RStudio installations, the reinstalling the tidyverse. If this still fails, please contact me. "],["R-Basics.html", "Chapter 3 R Basics 3.1 Before we start… 3.2 Functions 3.3 Objects", " Chapter 3 R Basics Now that we’ve got R, RStudio, and the tidyverse installed, we’re going to look at a few core concepts in R. 3.1 Before we start… 3.1.1 Help Throughout your time with R, you will probably want help. I ceratinly do. You can use a question mark ? to get help with many different concepts. You can just put it in front of the thing you want help with. We’ll see an example in just a minute… 3.1.2 Syntax Throughout this book, we’ll use this &lt;notation&gt; to indicate a place where the requested item that corresponds to your situation. For example, if I am instructed to run this command: print(\"&lt;your_name_here&gt;\"), I would type: print(&quot;Luke&quot;) ## [1] &quot;Luke&quot; Computers are very powerful but can be dumb at times. They are not very good with unexpected characters - they are often particularly sensitive when it comes to spaces , slashes \\ /, equal signs = (vs. ==), and quotes \". This book will try to warn you when syntax issues may arise. To try and prevent issues, this book will also use snake case (see the image below) - consider doing the same to avoid problems! 3.1.3 Paths To analyze data on your own computer, you will need to know the “path” to your data. This is essentially the street address of your data on your computer’s hard drive. Paths look different on Mac and PC. On Mac: /Users/lucasbusta/Documents/sample_data_set.csv (note the forward slashes!) On PC: C:\\My Computer\\Documents\\sample_data_set.csv (note the backward slashes!) You can quickly find paths to files via the following: On Mac: Locate the file in Finder. Right-click on the file, hold the Option key, then click “Copy as Pathname” On PC: Locate the file in Windows Explorer. Hold down the Shift key then right-click on the file. Click “Copy As Path” On either operating system, if you don’t want to type paths into your command line, another option is to define the following function within your R Session. You can do that by pasting the line below into your R Console and pressing enter. readCSV &lt;- function() { return(readr::read_csv(file.choose())) } Once that is done, you can use the command readCSV() to open up a navigation window and select your file that way. Cool! 3.2 Functions Ok, we’ve got some bookkeeping out of the way. Let’s get down to working with data! For this we need functions: A function is a command that tells R to perform an action! A function begins and ends with parentheses: this_is_a_function() The stuff inside the parentheses are the details of how you want the function to perform its action: run_this_analysis(on_this_data) Let’s illustrate this with an example. We’re going to use a function from the tidyverse called read_csv. This means we need to first load the tidyverse. We’ll use it to read some data from a path on our computer. This is a link to the original, untidied version of the data, but you can download a tidied version of the data by clicking here. We’re going to import that tidied version (not the original, untidied version) using the read_csv command. We’ll run read_csv(\"&lt;path_to_your_data&gt;\"). Note the use of QUOTES \"\"! Those are necessary. Also make sure your path uses the appropriate direction of slashes for your operating system. library(tidyverse) read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/algae_data.csv&quot;) ## # A tibble: 180 x 5 ## replicate algae_strain harvesting_regime chemical_species abundance ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Tsv1 Heavy FAs 520. ## 2 1 Tsv1 Heavy saturated_Fas 123. ## 3 1 Tsv1 Heavy omega_3_polyunsatura… 186. ## 4 1 Tsv1 Heavy monounsaturated_Fas 28.4 ## 5 1 Tsv1 Heavy polyunsaturated_Fas 369. ## 6 1 Tsv1 Heavy omega_6_polyunsatura… 183. ## 7 1 Tsv1 Heavy lysine 84.1 ## 8 1 Tsv1 Heavy methionine 24.1 ## 9 1 Tsv1 Heavy essential_Aas 692. ## 10 1 Tsv1 Heavy non_essential_Aas 919. ## # … with 170 more rows ** Note, we also could have imported this data by using readCSV() and then navigating to and selecting our file. 3.3 Objects You can think of objects as if they were “files” inside an R session where information can be stored. Let’s try making an object. All we have to do is use &lt;- to send the information from our read_csv command into a new object. This will create the object. See the example below. algae_chemistry &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/algae_data.csv&quot;) # Alternative: # algae_chemistry &lt;- readCSV() Now we have an object called algae_chemistry. We can examine the contents of that object by typing its name. For example: algae_chemistry ## # A tibble: 180 x 5 ## replicate algae_strain harvesting_regime chemical_species abundance ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Tsv1 Heavy FAs 520. ## 2 1 Tsv1 Heavy saturated_Fas 123. ## 3 1 Tsv1 Heavy omega_3_polyunsatura… 186. ## 4 1 Tsv1 Heavy monounsaturated_Fas 28.4 ## 5 1 Tsv1 Heavy polyunsaturated_Fas 369. ## 6 1 Tsv1 Heavy omega_6_polyunsatura… 183. ## 7 1 Tsv1 Heavy lysine 84.1 ## 8 1 Tsv1 Heavy methionine 24.1 ## 9 1 Tsv1 Heavy essential_Aas 692. ## 10 1 Tsv1 Heavy non_essential_Aas 919. ## # … with 170 more rows Cool! However, this is a pretty big object. For our next chapter on visualization, it would be nice to have a smaller dataset object to work with. Let’s use another tidyverse command called filter to filter the algae_chemistry object. We will need to tell the filter command what to filter out using “logical predicates” (things like equal to: ==, less than: &lt;, greater than: &gt;, greater-than-or-equal-to: &lt;=, etc.). Let’s filter algae_chemistry so that only rows where the chemical_species is equal to FAs (fatty acids) is preserved. This will look like chemical_species == \"FAs\". Here we go: filter(algae_chemistry, chemical_species == &quot;FAs&quot;) ## # A tibble: 18 x 5 ## replicate algae_strain harvesting_regime chemical_species abundance ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Tsv1 Heavy FAs 520. ## 2 2 Tsv1 Heavy FAs 450. ## 3 3 Tsv1 Heavy FAs 514. ## 4 1 Tsv1 Light FAs 580. ## 5 2 Tsv1 Light FAs 535. ## 6 3 Tsv1 Light FAs 513. ## 7 1 Tsv2 Heavy FAs 373. ## 8 2 Tsv2 Heavy FAs 409. ## 9 3 Tsv2 Heavy FAs 390. ## 10 1 Tsv2 Light FAs 490. ## 11 2 Tsv2 Light FAs 541. ## 12 3 Tsv2 Light FAs 461. ## 13 1 Tsv11 Heavy FAs 474. ## 14 2 Tsv11 Heavy FAs 331. ## 15 3 Tsv11 Heavy FAs 530. ## 16 1 Tsv11 Light FAs 526. ## 17 2 Tsv11 Light FAs 514. ## 18 3 Tsv11 Light FAs 544. Cool! Now it’s just showing us the 18 rows where the chemical_species is fatty acids (FAs). Let’s write this new, smaller dataset into a new object. For that we use &lt;-, remember? algae_data_small &lt;- filter(algae_chemistry, chemical_species == &quot;FAs&quot;) Now we have a nice, small table that we can use to practice data visualization. We’ll do that in the next chapter. "],["ggplot2.html", "Chapter 4 ggplot2 4.1 Setting up a ggplot 4.2 Geoms", " Chapter 4 ggplot2 For visualization, we’re going to use ggplot2 - a powerful set of commands for plot generation. Let’s make sure we’ve got our data from the last chapter active in our current session: library(tidyverse) algae_chemistry &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/algae_data.csv&quot;) algae_chemistry_small &lt;- filter(algae_chemistry, chemical_species == &quot;FAs&quot;) algae_chemistry_small ## # A tibble: 18 x 5 ## replicate algae_strain harvesting_regime chemical_species abundance ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Tsv1 Heavy FAs 520. ## 2 2 Tsv1 Heavy FAs 450. ## 3 3 Tsv1 Heavy FAs 514. ## 4 1 Tsv1 Light FAs 580. ## 5 2 Tsv1 Light FAs 535. ## 6 3 Tsv1 Light FAs 513. ## 7 1 Tsv2 Heavy FAs 373. ## 8 2 Tsv2 Heavy FAs 409. ## 9 3 Tsv2 Heavy FAs 390. ## 10 1 Tsv2 Light FAs 490. ## 11 2 Tsv2 Light FAs 541. ## 12 3 Tsv2 Light FAs 461. ## 13 1 Tsv11 Heavy FAs 474. ## 14 2 Tsv11 Heavy FAs 331. ## 15 3 Tsv11 Heavy FAs 530. ## 16 1 Tsv11 Light FAs 526. ## 17 2 Tsv11 Light FAs 514. ## 18 3 Tsv11 Light FAs 544. Great! Looks like we’re ready to go. 4.1 Setting up a ggplot There are three steps to setting up a ggplot: 4.1.1 Step 1: Define the data you want to use. We do this using the ggplot function’s data argument. When we run that line, it just shows a grey plot space. Why is this? It’s because all we’ve done is told ggplot that (i) we want to make a plot and (ii) what data should be used. We haven’t explained how to represent features of the data using ink. ggplot(data = algae_chemistry_small) 4.1.2 Step 2: Define how your variables map onto the axes. This is called aesthetic mapping and is done with the aes() function. aes() should be placed inside the ggplot command. Now when we run it, we get our axes! ggplot(data = algae_chemistry_small, aes(x = algae_strain, y = abundance)) 4.1.3 Step 3: Use geometric shapes to represent other variables in your data. Map your variables onto the geometric features of the shapes. To define which shape should be used, use a geom_* command. Some options are, for example, geom_point(), geom_boxplot(), and geom_violin(). These functions should be added to your plot using the + sign. We can use a new line to keep the code from getting too wide, just make sure the + sign is at the end fo the top line. Again, use aes() to map your variables onto the geometric features of the shapes. Let’s try it: ggplot(data = algae_chemistry_small, aes(x = algae_strain, y = abundance)) + geom_point(aes(color = harvesting_regime)) 4.2 Geoms 4.2.1 Modifying geoms In the last plot in the previous section, the points were a bit small, how could we fix that? We can modify the features of the shapes by adding additional arguments to the geom_*() functions. To change the size of the points created by the geom_point() function, this means that we need to add the size = argument. Here’s an example: ggplot(data = algae_chemistry_small, aes(x = algae_strain, y = abundance)) + geom_point(aes(color = harvesting_regime), size = 5) One powerful aspect of ggplot is the ability to quickly change mappings to see if alternative plots are more effective at bringing out the trends in the data. For example, we could modify the plot above by switching how harvesting_regime is mapped: ggplot(data = algae_chemistry_small, aes(x = algae_strain, y = abundance)) + geom_point(aes(size = harvesting_regime), color = &quot;black&quot;) ** Important note: Inside the aes() function, map aesthetics (the features of the geom’s shape) to a variable. Outside the aes() function, map aesthetics to constants. You can see this in the above two plots - in the first one, color is inside aes() and mapped to the variable called harvesting_regime, while size is outside the aes() call and is set to the constant 5. In the second plot, the situation is reversed, with size being inside the aes() function and mapped to the variable harvesting_regime, while color is outside the aes() call and is mapped to the constant “black”. 4.2.2 Using multiple geoms We can also stack geoms on top of one another by using multiple + signs. We also don’t have to assign the same mappings to each geom. ggplot(data = algae_chemistry_small, aes(x = algae_strain, y = abundance)) + geom_violin() + geom_point(aes(color = harvesting_regime), size = 5) As you can probably guess right now, there are lots of mappings that can be done, and lots of different ways to look at the same data! ggplot(data = algae_chemistry_small, aes(x = algae_strain, y = abundance)) + geom_violin(aes(fill = algae_strain)) + geom_point(aes(color = harvesting_regime, size = replicate)) "],["Exercises-1.html", "Chapter 5 Exercises 1 5.1 Part 1: Algae Chemistry Dataset 5.2 Part 2: Alaska Lakes Dataset", " Chapter 5 Exercises 1 In this set of exercises we’re going to practice importing, filtering, and plotting data. We’re going to work with two datasets: (i) algae_chemistry_data.csv and (ii) alaska_lake_data.csv. By clicking on those links you can download each dataset. For these exercises, you will write your code and answers to any questions in the Script Editor window of your RStudio. Then you will save that file and send it to me. That file comprises your submission for this assignment. I should be able to open and run the file on my computer (after changing the pathnames, if any - so don’t worry about compatibility for those). The file should contain both the code that can perform the actions described below and text that answers the questions asked below. You can download an example of what this file might look like here. If you have any questions please let me know 5.1 Part 1: Algae Chemistry Dataset 5.1.1 Question 1: Importing data Import the algae chemistry data. Remember that read_csv() is part of the tidyverse, so that library needs to be loaded into your R session. Also remember that another option is to paste and run readCSV &lt;- function() { return(readr::read_csv(file.choose())) } in your R Console, which then gives you access to the function readCSV(). That command doesn’t require an input path, so you don’t need to mess around with slashes and quotes. If you need examples of how to import data, please see the R Basics section of this book. 5.1.2 Question 2: Dataset dimensions How many rows and columns does the algae chemistry dataset have? (hint: when you display the dataset on your screen by typing its name into the console, dimensions are also displayed). Write the answer to this question in your R Script right below the code you use to find the answer. 5.1.3 Question 3: Objects Import the algae chemistry data and send it into a new object called algae_chemistry_data. Remember about &lt;-. See the R Basics section of this book if you need help. 5.1.4 Question 4: Filtering 5.1.4.1 A Now that you have the algae data imported and stored in an object called algae_chemistry_data, filter the data so that only entries are shown for which the chemical_species is “FAs”. What are the dimensions (i.e. number of rows and columns) of the resulting dataset? 5.1.4.2 B Now filter the dataset so that only entries for the algae_strain “Tsv1” are shown. What are the dimensions of the resulting dataset? 5.1.4.3 C Now filter the dataset so that only entries with an abundance greater than 250 are shown. Note that &gt; can be used in the filter command instead of ==, and that numbers inside a filter command do not require quotes around them. What are the dimensions of the resulting dataset? 5.1.5 Question 5: Plotting Make a ggplot that has algae_strain on the x axis and abundance on the y axis. Remember about aes(). Use points (geom_point()) to represent each compound. You don’t need to color the points. If you need a refresher on how to make a ggplot, please refer to the chapter on ggplot2. Which algae strain has the most abundant compound out of all the compounds in the dataset? 5.1.6 Question 6: Plotting Make a ggplot that has abundance on the x axis and chemical_species on the y axis. Use points to represent each compound. You don’t need to color the points. Generally speaking, what are the two most abundant classes of chemical species in these algae strains? (FAs/Fas stand for fatty acids, AAs/Aas stand for amino acids.) 5.1.7 Question 7: Filtering and plotting I am going to show you an example of how you can filter and plot at the same time: library(tidyverse) algae_chemistry_data &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/algae_data.csv&quot;) ## Parsed with column specification: ## cols( ## replicate = col_double(), ## algae_strain = col_character(), ## harvesting_regime = col_character(), ## chemical_species = col_character(), ## abundance = col_double() ## ) ggplot(data = filter(algae_chemistry_data, chemical_species == &quot;essential_Aas&quot;), aes(x = algae_strain, y = abundance)) + geom_point() Using the above as a template, make a plot that shows just omega_3_polyunsaturated_Fas, with algae_strain on the x axis, and abundance on the y axis. Color the points so that they correspond to harvesting_regime. Remember that mapping a feature of a shape onto a variable must be done inside aes(). Change the plot so that all the points are size = 5. Remember that mapping features of a shape to a constant needs to be done outside aes(). Which harvesting regime leads to higher levels of omega_3_polyunsaturated_Fas? 5.1.8 Question 8: Filtering and plotting Use a combination of filtering and plotting to show the abundance of the different chemical species in just the algae_strain called “Tsv1”. Use an x and y axis, as well as points to represent the measurements. Make point size correspond to the replicate, and color the points according to harvesting regime. 5.1.9 Question 9: Open-ended plotting Make a plot that checks to see which chemical_species were more abundant under light as opposed to heavy harvesting_regime in all three replicates. Use filtered data so that just one algae_strain is shown, an x and a y axis, and points to represent the measurements. Make the points size = 5 and also set the point’s alpha = 0.6. The points should be colored according to harvesting_regime. Make 3 plots, one for each strain of algae. 5.1.10 Question 10: A peek at what’s to come… Take the code that you made for Question 9. Remove the filtering. Add the following line to the end of the plot: facet_grid(.~algae_strain). Remember that adding things to plots is done with the + sign, so your code should look something like: ggplot(data = algae_chemistry_data, aes(x = &lt;something&gt;, y = &lt;something else&gt;)) + geom_point(aes(&lt;some things&gt;), &lt;some here too&gt;) + facet_grid(.~algae_strain) Also try, instead of facet_grid(.~algae_strain), facet_grid(algae_strain~.) at the end of you plot command. (note the swap in the position of the .~ relative to algae_strain). This means your code should look something like: ggplot(data = algae_chemistry_data, aes(x = &lt;something&gt;, y = &lt;something else&gt;)) + geom_point(aes(&lt;some things&gt;), &lt;some here too&gt;) + facet_grid(algae_strain~.) What advantages does this one extra line provide over what you had to do in question 9? 5.2 Part 2: Alaska Lakes Dataset 5.2.1 Question 1: Importing Data Import the Alaska lakes dataset into R and store it in an object. You can download the dataset from the link at the top of this page of exercises. 5.2.2 Question 2: Objects How many variables are in the Alaska lakes dataset? 5.2.3 Question 3: Filtering Filter the data set so only meausurements of free elements are shown. Remember, it’s ==, not =. What are the dimensions of the resulting dataset? 5.2.4 Question 4: Plotting Make a plot that shows the water temperatures of each lake. Don’t worry if you get a warning message from R about “missing values”. Which is the hottest lake? The coolest? 5.2.5 Question 5: Plotting Make a plot that shows the water temperature of each lake. The x axis should be park, the y axis water temp. Add geom_violin() to the plot first, then geom_point(). Make the points size = 5. Color the points according to water_temp. Which park has four lakes with very similar temperatures? 5.2.6 Question 6: Filtering and Plotting From the plot you made for question 5, it should be apparent that there is one lake in NOAT that is much warmer than the others. Filter the data so that only entries from park == \"NOAT\" are shown (note the double equals sign and the quotes around NOAT…). Combine this filtering with plotting and use geom_point() to make a plot that shows which specific lake that is. 5.2.7 Question 7: Filtering and Plotting Make a plot that shows which lake has the highest abundance of sulfur. 5.2.8 Question 8: Open-ended Plotting Make a plot that uses geom_point(). Set the “shape” aesthetic of the points to 21, i.e. geom_point(aes(...), shape = 21). This gives you access to a new aesthetics: fill. It also changes the behaviour of the color aesthetic slightly. Here is an example: library(tidyverse) lake_data &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/alaska_lake_data.csv&quot;) ## Parsed with column specification: ## cols( ## lake = col_character(), ## park = col_character(), ## water_temp = col_double(), ## pH = col_double(), ## element = col_character(), ## mg_per_L = col_double(), ## element_type = col_character() ## ) ggplot(data = filter(lake_data, lake == &quot;Lake_Narvakrak&quot;), aes(x = lake, y = mg_per_L)) + geom_point(shape = 21, size = 10, color = &quot;black&quot;, fill = &quot;green&quot;) Now we have lots of aesthetics we can map to: x, y, size, color, and fill (leave shape set to 21 for now). Make a plot of your own design. It should include filtering, and all the aesthetics listed above, though whether you map them to a variable or a constant is up to you. When you are done with this plot, take a screen shot of it. Go to THIS GOOGLE SHEET, make a slide for yourself (you don’t have to include your name), and paste your screen shot there. Add a small caption that explains how your variables are mapped. "],["geoms-facets-scales-themes.html", "Chapter 6 geoms, facets, scales, themes 6.1 Geoms 6.2 Facets 6.3 Scales 6.4 Themes", " Chapter 6 geoms, facets, scales, themes We’ve looked at how to import data, filter data, and map variables in our data to geometric shapes to make plots. Let’s have a look at a few more things. For these examples, we’re going to use this solvents dataset. 6.1 Geoms I’d like to introduce you to two new geoms. The first geom_smooth() is used when there are two continuous variables. It is particularly nice when geom_point() is stacked on top of it. library(tidyverse) solvents &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/solvents.csv&quot;) ## Parsed with column specification: ## cols( ## solvent = col_character(), ## formula = col_character(), ## boiling_point = col_double(), ## melting_point = col_double(), ## density = col_double(), ## miscible_with_water = col_logical(), ## solubility_in_water = col_double(), ## relative_polarity = col_double(), ## vapor_pressure = col_double(), ## CAS_number = col_character(), ## formula_weight = col_double(), ## refractive_index = col_double(), ## specific_gravity = col_double(), ## category = col_character() ## ) ggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + geom_smooth() + geom_point() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Also, please be aware of geom_tile(), which is nice for situations with two discrete variables and one continuous variable. geom_tile() makes what are often referred to as heat maps. Note that geom_tile() is somewhat similar to geom_point(shape = 21), in that it has both fill and color aesthetics that control the center color and the border color, respectively. library(tidyverse) algae &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/algae_data.csv&quot;) ## Parsed with column specification: ## cols( ## replicate = col_double(), ## algae_strain = col_character(), ## harvesting_regime = col_character(), ## chemical_species = col_character(), ## abundance = col_double() ## ) ggplot(data = filter(algae, harvesting_regime == &quot;Heavy&quot;), aes(x = algae_strain, y = chemical_species)) + geom_tile(aes(fill = abundance), color = &quot;black&quot;, size = 1) These examples should illustrate that there is, to some degree, correspondence between the type of data you are interested in plotting (number of discrete and continuous variables) and the types of geoms that can effectively be used to represent the data. There is a handy cheat sheet that can help you identify the right geom for your situation. Please keep this cheat sheet in mind for your future plotting needs… 6.2 Facets As alluded to in Exercises 1, it is possible to map variables in your dataset to more than the geometric features of shapes (i.e. geoms). One very common way of doing this is with facets. Faceting creates small multiples of your plot, each of which shows a different subset of your data based on a categorical variable of your choice. Let’s check it out. Here, we can facet in the horizontal direction: ggplot(data = algae, aes(x = algae_strain, y = chemical_species)) + geom_tile(aes(fill = abundance), color = &quot;black&quot;) + facet_grid(.~replicate) We can facet in the vertical direction: ggplot(data = algae, aes(x = algae_strain, y = chemical_species)) + geom_tile(aes(fill = abundance), color = &quot;black&quot;) + facet_grid(replicate~.) And we can do both at the same time: ggplot(data = algae, aes(x = algae_strain, y = chemical_species)) + geom_tile(aes(fill = abundance), color = &quot;black&quot;) + facet_grid(harvesting_regime~replicate) Faceting is a great way to describe more variation in your plot without having to make your geoms more complicated. For situations where you need to generate lots and lots of facets, consider facet_wrap instead of facet_grid. 6.3 Scales Every time you define an aesthetic mapping (e.g. aes(x = algae_strain)), you are defining a new scale that is added to your plot. You can control these scales using the scale_* family of commands. Consider our faceting example above. In it, we use geom_tile(aes(fill = abundance)) to map the abundance variable to the fill aesthetic of the tiles. This creates a scale called fill that we can adjust using scale_fill_*. In this case, fill is mapped to a continuous variable and so the fill scale is a color gradient. Therefore, scale_fill_gradient() is the command we need to change it. Remember that you could always type ?scale_fill_ into the console and it will help you find relevant help topics that will provide more detail. Another option is to google: “How to modify color scale ggplot geom_tile”, which will undoubtedly turn up a wealth of help. ggplot(data = algae, aes(x = algae_strain, y = chemical_species)) + geom_tile(aes(fill = abundance), color = &quot;black&quot;) + facet_grid(harvesting_regime~replicate) + scale_fill_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + theme_classic() 6.4 Themes So far we’ve just looked at how to control the means by which your data is represented on the plot. There are also components of the plot that are, strictly speaking, not data per se, but rather non-data ink. These are controlled using the theme() family of commands. There are two ways to go about this. 6.4.1 Complete themes ggplot comes with a handful of built in “complete themes”. These will change the appearance of your plots with respect to the non-data ink. Compare the following plots: ggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + geom_smooth() + geom_point() + theme_classic() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + geom_smooth() + geom_point() + theme_dark() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + geom_smooth() + geom_point() + theme_void() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 6.4.2 Theme components You can also change individual components of themes. This can be a bit tricky, but it’s all explained if you run ?theme(). Hare is an example (and google will provide many, many more). ggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + geom_smooth() + geom_point() + theme( text = element_text(size = 20, color = &quot;black&quot;) ) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Last, here is an example of combining scale_* and theme* with previous commands to really get a plot looking sharp. ggplot(data = solvents, aes(x = boiling_point, y = vapor_pressure)) + geom_smooth(color = &quot;#4daf4a&quot;) + scale_x_continuous(name = &quot;Boiling Point&quot;, breaks = seq(0,200,25), limits = c(30,210)) + scale_y_continuous(name = &quot;Vapor Pressure&quot;, breaks = seq(0,600,50)) + geom_point(color = &quot;#377eb8&quot;, size = 4, alpha = 0.6) + theme_bw() + theme( axis.text = element_text(color = &quot;black&quot;), text = element_text(size = 20, color = &quot;black&quot;) ) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; "],["Exercises-2.html", "Chapter 7 Exercises 2 7.1 Question 1 7.2 Question 2 7.3 Question 3 7.4 Question 4 7.5 Question 5", " Chapter 7 Exercises 2 In this set of exercises we’re going to practice making more plots using the solvents dataset. Since you are now familiar with importing, filtering, and plotting data, the prompts are going to be relatively open ended - I do not care what variables you map to x, y, fill, color, etc. Rather, I expect your submission to demonstrate to me that you have explored each of the new topics covered in R Basics 2. This includes geoms beyond geom_point() and geom_violin(), facets, scale modifications, and theme adjustments. Be creative! Explore the solvents dataset. Find something interesting! Show me that you have mastered this materal. Don’t forget about the ggplot cheat sheet. As before, for these exercises, you will write your code and answers to any questions in the Script Editor window of your RStudio. Then you will save that file and send it to me. That file comprises your submission for this assignment. I should be able to open and run the file on my computer (after changing the pathnames, if any - so don’t worry about compatibility for those). The file should contain both the code that can perform the actions described below and text that answers the questions asked below. You can download an example of what this file might look like here. If you have any questions please let me know. 7.1 Question 1 Identify a relationship between two variables in the dataset. Create a plot that is optimized (see note) to highlight the features of this relationship. Write a short caption that describes the plot and the trend you’ve identified and highlighted. note: I realize that the word “optimize” is not clearly defined here. That’s ok! You are the judge of what is optimized and what is not. Use your caption to make a case for why your plot is optimized. Defend your ideas with argument! 7.2 Question 2 Create two plots that are identical except that one uses the scales = \"free\" feature of facet_grid while the other does not (i.e. one should use facet_grid(&lt;things&gt;), whiel the other uses facet_grid(&lt;things&gt;, scales = \"free\")). Write a single caption that describes both plots, highlighting the advantages provided by each plot over the other. 7.3 Question 3 Create two plots that are identical except that one uses geom_point(), while the other uses geom_jitter(). Write a single caption that describes both plots. The caption should highlight the differences bewteen these two plots and it should describe case(s) in which you think it would be appropriate to use geom_jitter() over geom_point(). 7.4 Question 4 Make a plot that has five aesthetic mappings (x and y mappings count). Use the scales_* family of commands to modify some aspect of each scale create by the five mappings. Hint: some scales are somewhat tricky to modify (alpha, linetype, …), and some scales are easier to modify (x, y, color, fill, shape). 7.5 Question 5 Make a plot and manually modify at least three aspects of its theme (i.e. do not use one of the build in complete themes such as theme_classic(), rather, manually modify components of the theme using theme()). This means that inside your theme() command, there should be three arguments separated by commas. "],["summary-statistics.html", "Chapter 8 Summary Statistics 8.1 Summary statistics 8.2 The pipe (%&gt;%) 8.3 Tidy data", " Chapter 8 Summary Statistics 8.1 Summary statistics So far, we have been importing and plotting raw data. This is well and good, but it is not always suitable. Often we have scientific questions that cannot be answered by looking at raw data alone, or sometimes there is too much raw data to plot. For this, we need summary statistics - things like averages, standard deviations, and so on. While these metrics can be computed in Excel, programming such can be time consuming, especially for group statistics. Consider the example below, which uses the NY Trees dataset. The NY Trees dataset contains information on nearly half a million trees in New York City (this is after considerable filtering and simplification): library(tidyverse) ny_trees &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/NY_trees.csv&quot;) ## Parsed with column specification: ## cols( ## tree_height = col_double(), ## tree_diameter = col_double(), ## address = col_character(), ## tree_loc = col_character(), ## pit_type = col_character(), ## soil_lvl = col_character(), ## status = col_character(), ## spc_latin = col_character(), ## spc_common = col_character(), ## trunk_dmg = col_character(), ## zipcode = col_double(), ## boroname = col_character(), ## latitude = col_double(), ## longitude = col_double() ## ) ny_trees ## # A tibble: 378,762 x 14 ## tree_height tree_diameter address tree_loc pit_type soil_lvl status ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 21.1 6 1139 5… Front Sidewal… Level Good ## 2 59.0 6 2220 B… Across Sidewal… Level Good ## 3 92.4 13 2254 B… Across Sidewal… Level Good ## 4 50.2 15 2332 B… Across Sidewal… Level Good ## 5 95.0 21 2361 E… Front Sidewal… Level Poor ## 6 67.5 19 2409 E… Front Continu… Level Good ## 7 75.3 11 1481 E… Front Lawn Level Excel… ## 8 27.9 7 1129 5… Front Sidewal… Level Good ## 9 111. 26 2076 E… Across Sidewal… Level Excel… ## 10 83.9 20 2025 E… Front Sidewal… Level Excel… ## # … with 378,752 more rows, and 7 more variables: spc_latin &lt;chr&gt;, ## # spc_common &lt;chr&gt;, trunk_dmg &lt;chr&gt;, zipcode &lt;dbl&gt;, boroname &lt;chr&gt;, ## # latitude &lt;dbl&gt;, longitude &lt;dbl&gt; More than 300,000 observations of 14 variables! That’s 4.2M datapoints! Now, what is the average and standard deviation of the height and diameter of each tree species within each NY borough? Do those values change for trees that are in parks versus sidewalk pits?? I don’t even know how one would begin to approach such questions using traditional spreadsheets. Here, we will answer these questions with ease using two new commands: group_by() and summarize(). Let’s get to it. Say that we want to know (and of course, visualize) the mean and standard deviation of the heights of each tree species in NYC. We can see that data in first few columns of the NY trees dataset above, but how to calculate these statistics? In R, mean can be computed with mean() and standard deviation can be calculated with sd(). Also, keep in mind that a single column of a dataset can be accessed using $. So, we can calculate the average and standard deviation of all the trees in the data set as follows: mean(ny_trees$tree_height) ## [1] 72.57976 sd(ny_trees$tree_height) ## [1] 28.65391 Great! But how to do this for each species? We need to divide up the data by species, then compute the mean and standard deviation, then recombine the results into a new table. First, we use group_by(). Note that in ny_trees, species are indicated in the column called spc_latin. Once the data is grouped, we can use summarize() to compute statistics. Please note that both group_by() and summarize() require .data = as an argument, as opposed to data =, which we have been using up to this point. ny_trees_by_spc &lt;- group_by(.data = ny_trees, spc_latin) summarize(.data = ny_trees_by_spc, mean_height = mean(tree_height)) ## # A tibble: 12 x 2 ## spc_latin mean_height ## &lt;chr&gt; &lt;dbl&gt; ## 1 ACER PLATANOIDES 82.6 ## 2 ACER RUBRUM 106. ## 3 ACER SACCHARINUM 65.6 ## 4 FRAXINUS PENNSYLVANICA 60.6 ## 5 GINKGO BILOBA 90.4 ## 6 GLEDITSIA TRIACANTHOS 53.0 ## 7 PLATANUS ACERIFOLIA 82.0 ## 8 PYRUS CALLERYANA 21.0 ## 9 QUERCUS PALUSTRIS 65.5 ## 10 QUERCUS RUBRA 111. ## 11 TILIA CORDATA 98.8 ## 12 ZELKOVA SERRATA 101. Bam. Mean height of each tree species. summarize() is more powerful though, we can do many summary statistics at once: ny_trees_by_spc &lt;- group_by(.data = ny_trees, spc_latin) ny_trees_by_spc_summ &lt;- summarize(.data = ny_trees_by_spc, mean_height = mean(tree_height), stdev_height = sd(tree_height)) ny_trees_by_spc_summ ## # A tibble: 12 x 3 ## spc_latin mean_height stdev_height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACER PLATANOIDES 82.6 17.6 ## 2 ACER RUBRUM 106. 15.7 ## 3 ACER SACCHARINUM 65.6 16.6 ## 4 FRAXINUS PENNSYLVANICA 60.6 21.3 ## 5 GINKGO BILOBA 90.4 24.5 ## 6 GLEDITSIA TRIACANTHOS 53.0 13.0 ## 7 PLATANUS ACERIFOLIA 82.0 16.0 ## 8 PYRUS CALLERYANA 21.0 5.00 ## 9 QUERCUS PALUSTRIS 65.5 6.48 ## 10 QUERCUS RUBRA 111. 20.7 ## 11 TILIA CORDATA 98.8 32.6 ## 12 ZELKOVA SERRATA 101. 10.7 Now we can use this data in plotting. For this, we will use a new geom, geom_pointrange, which takes x and y aesthetics, as usual, but also requires two additional y-ish aesthetics ymin and ymax. Note that in this case, if we map spc_latin to the x axis, the x axis tick labels will have long names that will overlap in our plot. In the past we have avoided this by swapping the mappings of x and y in out plots. Here, this gets a bit tedious since we will have to change ymin and ymax as well. Instead of all this, we can just add coord_flip() to our plot, which will swap x and y axes automatically. Also, note that in the aesthetic mappings for ymin and ymax, we can use a mathematical expression: mean-stdev and mean+stdev, respectivey. In our case, these are mean_height - stdev_height and mean_height + stdev_height. Let’s see it in action: ggplot(data = ny_trees_by_spc_summ) + geom_pointrange( aes( x = spc_latin, y = mean_height, ymin = mean_height - stdev_height, ymax = mean_height + stdev_height ) ) + coord_flip() Cool! Just like that we’ve found (and visualized) the average and standard deviation of tree heights, by species, in NYC. But it doesn’t stop there. We can use group_by() and summarize() on multiple variables (i.e. more groups). We can do this to examine the properties of each tree species in each NYC borough. Let’s check it out. ny_trees_by_spc_boro &lt;- group_by(.data = ny_trees, spc_latin, boroname) ny_trees_by_spc_boro_summ &lt;- summarize(.data = ny_trees_by_spc_boro, mean_diam = mean(tree_diameter), stdev_diam = sd(tree_diameter)) ny_trees_by_spc_boro_summ ## # A tibble: 48 x 4 ## spc_latin boroname mean_diam stdev_diam ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ACER PLATANOIDES Bronx 13.9 6.74 ## 2 ACER PLATANOIDES Brooklyn 15.4 14.9 ## 3 ACER PLATANOIDES Manhattan 11.6 8.45 ## 4 ACER PLATANOIDES Queens 15.1 12.9 ## 5 ACER RUBRUM Bronx 11.4 7.88 ## 6 ACER RUBRUM Brooklyn 10.5 7.41 ## 7 ACER RUBRUM Manhattan 6.63 4.23 ## 8 ACER RUBRUM Queens 14.1 8.36 ## 9 ACER SACCHARINUM Bronx 19.7 10.5 ## 10 ACER SACCHARINUM Brooklyn 22.2 10.1 ## # … with 38 more rows Now we have summary statistics for each tree species within each borough. This is different from the previous plot in that we now have an additional variable (boroname) in our summarized dataset. This additional variable needs to be encoded in our plot. Let’s map boroname to x and facet over tree species, which used to be on x. We’ll also manually modify the theme element strip.text.y to get the species names in a readable position. ggplot(data = ny_trees_by_spc_boro_summ) + geom_pointrange( aes( x = boroname, y = mean_diam, ymin = mean_diam-stdev_diam, ymax = mean_diam+stdev_diam ) ) + facet_grid(spc_latin~.) + coord_flip() + theme( strip.text.y = element_text(angle = 0) ) Excellent! And if we really want to go for something pretty: library(RColorBrewer) ggplot(data = ny_trees_by_spc_boro_summ) + geom_pointrange( aes( x = boroname, y = mean_diam, ymin = mean_diam-stdev_diam, ymax = mean_diam+stdev_diam, fill = spc_latin ), color = &quot;black&quot;, shape = 21 ) + labs( x = &quot;Borough&quot;, y = &quot;Trunk diameter&quot;, caption = str_wrap(&quot;Figure 1: Diameters of trees in New York City. Points correspond to average diameters of each tree species in each borough. Horizontal lines indicate the standard deviation of tree diameters. Points are colored according to tree species.&quot;, width = 80) ) + facet_grid(spc_latin~.) + guides(fill = &quot;none&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + coord_flip() + theme_bw() + theme( strip.text.y = element_text(angle = 0), plot.caption = element_text(hjust = 0.5) ) Now we are getting somewhere. It looks like there are some really big maple trees (Acer) in Queens. 8.2 The pipe (%&gt;%) When we want to get summary statistics for a dataset, we often end up creating lots of new objects, sometimes with convoluted names. For example, what if we want to know and visualize the average temperature across the lakes in each of the three Alaska parks? We might do something like the below: library(tidyverse) AK_lakes &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/alaska_lake_data.csv&quot;) ## Parsed with column specification: ## cols( ## lake = col_character(), ## park = col_character(), ## water_temp = col_double(), ## pH = col_double(), ## element = col_character(), ## mg_per_L = col_double(), ## element_type = col_character() ## ) head(AK_lakes) ## # A tibble: 6 x 7 ## lake park water_temp pH element mg_per_L element_type ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Devil_Mountain_Lake BELA 6.46 7.69 C 3.4 bound ## 2 Devil_Mountain_Lake BELA 6.46 7.69 N 0.028 bound ## 3 Devil_Mountain_Lake BELA 6.46 7.69 P 0 bound ## 4 Devil_Mountain_Lake BELA 6.46 7.69 Cl 10.4 free ## 5 Devil_Mountain_Lake BELA 6.46 7.69 S 0.62 free ## 6 Devil_Mountain_Lake BELA 6.46 7.69 F 0.04 free AK_lakes_by_park &lt;- group_by(.data = AK_lakes, park) AK_lakes_by_park_summ &lt;- summarize(.data = AK_lakes_by_park, mean_temperature = mean(water_temp)) ggplot(AK_lakes_by_park_summ) + geom_point(aes(x = park, y = mean_temperature)) In order to do that, we created two new objects AK_lakes_by_park and AK_lakes_by_park_summ. In big analysis chains, we can end up creating lots of objects with complicated names. There must be a better way! There is. Meet the pipe: %&gt;%. It sends the output from one command directly to the next, so we neither need to have each command create a new object, nor do we need to start each command by telling it what data to use. With the pipe, the same analysis as above can be done with the text below. Neat! Easier! library(tidyverse) read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/alaska_lake_data.csv&quot;) %&gt;% group_by(park) %&gt;% summarize(mean_temperature = mean(water_temp)) %&gt;% ggplot() + geom_point(aes(x = park, y = mean_temperature)) ## Parsed with column specification: ## cols( ## lake = col_character(), ## park = col_character(), ## water_temp = col_double(), ## pH = col_double(), ## element = col_character(), ## mg_per_L = col_double(), ## element_type = col_character() ## ) 8.3 Tidy data When we make data tables by hand, it’s often easy to make a wide-style table like the following. In it, the abundances of 7 different fatty acids in 10 different species are tabulated. Each fatty acid gets its own row, each species, its own column. library(tidyverse) FAs &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/fadb_sample.csv&quot;) ## Parsed with column specification: ## cols( ## fatty_acid = col_character(), ## Agonandra_brasiliensis = col_double(), ## Agonandra_silvatica = col_double(), ## Agonandra_excelsa = col_double(), ## Heisteria_silvianii = col_double(), ## Malania_oleifera = col_double(), ## Ximenia_americana = col_double(), ## Ongokea_gore = col_double(), ## Comandra_pallida = col_double(), ## Buckleya_distichophylla = col_double(), ## Nuytsia_floribunda = col_double() ## ) FAs ## # A tibble: 7 x 11 ## fatty_acid Agonandra_brasi… Agonandra_silva… Agonandra_excel… ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hexadecan… 3.4 1 1.2 ## 2 Octadecan… 6.2 0.1 0.4 ## 3 Eicosanoi… 4.7 3.5 1.7 ## 4 Docosanoi… 77.4 0.4 1 ## 5 Tetracosa… 1.4 1 1.4 ## 6 Hexacosan… 1.9 12.6 23.1 ## 7 Octacosan… 5 81.4 71.3 ## # … with 7 more variables: Heisteria_silvianii &lt;dbl&gt;, ## # Malania_oleifera &lt;dbl&gt;, Ximenia_americana &lt;dbl&gt;, Ongokea_gore &lt;dbl&gt;, ## # Comandra_pallida &lt;dbl&gt;, Buckleya_distichophylla &lt;dbl&gt;, ## # Nuytsia_floribunda &lt;dbl&gt; While this format is very nice for filling in my hand (such as in a lab notebook or similar), it does not groove with ggplot and other tidyverse functions very well. We need to convert it into a long-style table. This is done using pivot_longer(). You can think of this function as transforming both your data’s column names (or some of the column names) and your data matrix’s values (in this case, the measurements) each into their own variables (i.e. columns). We can do this for our fatty acid dataset using the command below. In it, we specify what data we want to transform (data = FAs), we need to tell it what columns we want to transform (cols = 2:11), what we want the new variable that contains column names to be called (names_to = \"plant_species\") and what we want the new variable that contains matrix values to be called (values_to = \"relative_abundance\"). All together now: pivot_longer(data = FAs, cols = 2:11, names_to = &quot;plant_species&quot;, values_to = &quot;relative_abundance&quot;) ## # A tibble: 70 x 3 ## fatty_acid plant_species relative_abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Hexadecanoic acid Agonandra_brasiliensis 3.4 ## 2 Hexadecanoic acid Agonandra_silvatica 1 ## 3 Hexadecanoic acid Agonandra_excelsa 1.2 ## 4 Hexadecanoic acid Heisteria_silvianii 2.9 ## 5 Hexadecanoic acid Malania_oleifera 0.7 ## 6 Hexadecanoic acid Ximenia_americana 3.3 ## 7 Hexadecanoic acid Ongokea_gore 1 ## 8 Hexadecanoic acid Comandra_pallida 2.3 ## 9 Hexadecanoic acid Buckleya_distichophylla 1.6 ## 10 Hexadecanoic acid Nuytsia_floribunda 3.8 ## # … with 60 more rows Brilliant! Now we have a tidy, long-style table that can be used with ggplot. "],["Exercises-3.html", "Chapter 9 Exercises 3 9.1 Question 1 9.2 Question 2 9.3 Question 3 9.4 Question 4 9.5 Question 5 9.6 Question 6 9.7 Question 7", " Chapter 9 Exercises 3 Isn’t seven the most powerfully magical number? Isn’t seven the most powerfully magical number? Yes… I think the idea of a seven-part assignment would greatly appeal to an alchemist. In this set of exercises we are going to use the Periodic Table dataset. Please download and import that dataset into R. Then, open a new powerpoint presentation and make seven empty slides. There are seven questions below. Each will direct you to make a particular type of plot. Copy and paste those plots, in order, into the seven slides in your presentation. Paste the code underlying your figure into the “notes” section for that slide (the little space under the slide where you can write notes). If the question also requires some text to answer properly, put that text in the “notes” section as well. When you are done, email that presentation to me, which will comprise your submission for this assignment. Please let me know if you have any questions. Good luck, and have fun! 9.1 Question 1 Make a plot using geom_point() that shows the average atomic weight of the elements discovered in each year spanned by the dataset (i.e. what was the average weight of the elements discovered in 1900? 1901? 1902? etc.). You should see a trend, particularly after 1950. What do you think has caused this trend? 9.2 Question 2 The column state_at_RT indicates the state of each element at room temperate. Make a plot that shows the average first ionization potential of all the elements belonging to each state group indicated in state_at_RT (i.e. what is the average 1st ionization potential of all elements that are solid at room temp? liquid? etc.). Which is the highest? 9.3 Question 3 Filter the dataset so that only elements with atomic number less than 85 are included. Considering only these elements, what is the average and standard deviation of boiling points for each type of crystal_structure? Make a plot using geom_pointrange() that shows the mean and standard deviation of each of these groups. What’s up with elements that have a cubic crystal structure? 9.4 Question 4 Now filter the original dataset so that only elements with atomic number less than 37 are considered. The elements in this dataset belong to the first four periods. What is the average abundance of each of these four periods in seawater? i.e. what is the average abundance of all elements from period 1? period 2? etc. Which period is the most abundant? In this context what does “CHON” mean? (not the rock band, though they are also excellent, especially that song that features GoYama) 9.5 Question 5 Now filter the original dataset so that only elements with atomic number less than 103 are considered. Filter it further so that elements from group number 18 are excluded. Using this twice-filtered dataset, compute the average, minimum, and maximum values for electronegativiy for each group_number. Use geom_point() and geom_errorbar() to illustrate the average, minimum, and maximum values for each group number. 9.6 Question 6 Filter the dataset so that only elements with atomic number less than 85 are considered. Group these by color. Now filter out those that have color == \"colorless\". Of the remaining elements, which has the widest range of specific heats? Use geom_point() and geom_errorbar() to illustrate the mean and standard deviation of each color’s specific heats. 9.7 Question 7 You have learned many things in this course so far. filter(), ggplot(), and now group_by() and summarize(). Using all these commands, create a graphic to illustrate what you consider to be an interesting periodic trend. Use theme elements and scales to enhance your plot: impress me! "],["Clustering.html", "Chapter 10 Clustering", " Chapter 10 Clustering So far we have been looking at how to plot raw data, as well as data that has been summarize across samples. This is important stuff and very useful. However, we often have questions about how samples in our datasets relate to one another. For example: in the Alaska lakes dataset, which lake is most similar, chemically speaking, to Lake Narvakrak? Answering this requires calculating numeric distances between samples based on their chemical properties. For this, we will use runMatrixAnalysis(), a function that you can load into your R Session by running the following command: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) In order for runMatrixAnalysis() to work, you will also need to install ape and ggtree. Use the code below to do that: install.packages(&quot;ape&quot;, repos = &quot;https://cloud.r-project.org&quot;, quiet = FALSE) if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;ggtree&quot;) With the requisite packages installed, we can use the template for runMatrixAnalysis() to begin our command. In order to use the template, it is critical that we think about our data in terms of samples and analytes. Let’s consider our Alaksa lakes data set: library(tidyverse) AK_lakes &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/alaska_lake_data.csv&quot;) AK_lakes ## # A tibble: 220 x 7 ## lake park water_temp pH element mg_per_L element_type ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Devil_Mountain_Lake BELA 6.46 7.69 C 3.4 bound ## 2 Devil_Mountain_Lake BELA 6.46 7.69 N 0.028 bound ## 3 Devil_Mountain_Lake BELA 6.46 7.69 P 0 bound ## 4 Devil_Mountain_Lake BELA 6.46 7.69 Cl 10.4 free ## 5 Devil_Mountain_Lake BELA 6.46 7.69 S 0.62 free ## 6 Devil_Mountain_Lake BELA 6.46 7.69 F 0.04 free ## 7 Devil_Mountain_Lake BELA 6.46 7.69 Br 0.02 free ## 8 Devil_Mountain_Lake BELA 6.46 7.69 Na 8.92 free ## 9 Devil_Mountain_Lake BELA 6.46 7.69 K 1.2 free ## 10 Devil_Mountain_Lake BELA 6.46 7.69 Ca 5.73 free ## # … with 210 more rows We can see that this dataset is comprised of measuring various analytes (i.e. several chemical elements, as well as water_temp, and pH), in different samples (i.e. lakes). We need to tell the runMatrixAnalysis() function how each column relates to this samples and analytes structure. See the image below for an explanation. With this in mind, let’s try out our template: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;, local = knitr:::knit_global()) AK_lakes_clustered &lt;- runMatrixAnalysis( data = AK_lakes, analysis = &quot;hclust&quot;, column_w_names_of_multiple_analytes = &quot;element&quot;, column_w_values_for_multiple_analytes = &quot;mg_per_L&quot;, columns_w_values_for_single_analyte = c(&quot;water_temp&quot;, &quot;pH&quot;), columns_w_additional_analyte_info = &quot;element_type&quot;, columns_w_sample_ID_info = c(&quot;lake&quot;, &quot;park&quot;) ) ## Not replacing any NAs in your data set AK_lakes_clustered ## # A tibble: 39 x 25 ## lake park sample_unique_ID parent node branch.length label isTip ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 Devi… BELA Devil_Mountain_… 33 1 7.25 Devi… TRUE ## 2 Imur… BELA Imuruk_Lake_BELA 32 2 4.91 Imur… TRUE ## 3 Kuzi… BELA Kuzitrin_Lake_B… 36 3 3.27 Kuzi… TRUE ## 4 Lava… BELA Lava_Lake_BELA 35 4 3.02 Lava… TRUE ## 5 Nort… BELA North_Killeak_L… 21 5 204. Nort… TRUE ## 6 Whit… BELA White_Fish_Lake… 22 6 65.2 Whit… TRUE ## 7 Inia… GAAR Iniakuk_Lake_GA… 29 7 3.60 Inia… TRUE ## 8 Kuru… GAAR Kurupa_Lake_GAAR 31 8 8.57 Kuru… TRUE ## 9 Lake… GAAR Lake_Matcharak_… 29 9 3.60 Lake… TRUE ## 10 Lake… GAAR Lake_Selby_GAAR 30 10 5.24 Lake… TRUE ## # … with 29 more rows, and 17 more variables: x &lt;dbl&gt;, y &lt;dbl&gt;, ## # branch &lt;dbl&gt;, angle &lt;dbl&gt;, water_temp &lt;dbl&gt;, pH &lt;dbl&gt;, C &lt;dbl&gt;, ## # N &lt;dbl&gt;, P &lt;dbl&gt;, Cl &lt;dbl&gt;, S &lt;dbl&gt;, F &lt;dbl&gt;, Br &lt;dbl&gt;, Na &lt;dbl&gt;, ## # K &lt;dbl&gt;, Ca &lt;dbl&gt;, Mg &lt;dbl&gt; It works! Now we can plot our cluster diagram with a ggplot add-on called ggtree. We’ve seen that ggplot takes a “data” argument (i.e. ggplot(data = &lt;some_data&gt;) + geom_*() etc.). In contrast, ggtree takes an argument called tr, though if you’re using the runMatrixAnalysis() function, you can treat these two (data and tr) the same, so, use: ggtree(tr = &lt;output_from_runMatrixAnalysis&gt;) + geom_*() etc. Note that ggtree also comes with several great new geoms: geom_tiplab() and geom_tippoint(). Let’s try those out: library(ggtree) ggtree(tr = AK_lakes_clustered) + geom_tiplab() + geom_tippoint() + theme_classic() Cool! Though that plot could use some tweaking… let’s try: ggtree(tr = AK_lakes_clustered) + geom_tiplab(aes(label = lake), offset = 10) + geom_tippoint(shape = 21, aes(fill = park), size = 4) + coord_cartesian(xlim = c(0,350)) Very nice! "],["Exercises-4.html", "Chapter 11 Exercises 4", " Chapter 11 Exercises 4 For this set of exercises, please use runMatrixAnalysis() to run and visualize a hierarchical cluster analysis with each of the main datasets that we have worked with so far, except for NY_trees. This means the algae data, the Alaska lakes data, the solvents data, and THIS SUBSET of the periodic table. You do not need to use the entire periodic table dataset. Please note that each one of these datasets will be loaded into your R session when you run: To complete this assignment, put the figure from each cluster analysis in its own slide of a powerpoint presentation, put the underlying code in the corresponding notes section, and send the pptx to me. Let me know if you have any questions! For this assignment, you may find two things very helpful: Chapter 10. It explains how to use runMatrixAnalysis(). Please note that I used some of your feedback in class to make runMatrixAnalysis() simpler. Accordingly, in the lecture recording I am explaining a version of that command that is slightly out of date. The book chapter, however, is completely up-to-date. You may find the colnames() function useful for this assignment. It will list all the column names in a dataset for you. For example: mini_per_table &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/per_table_small.csv&quot;) colnames(mini_per_table) ## [1] &quot;atomic_number&quot; ## [2] &quot;atomic_symbol&quot; ## [3] &quot;group_number&quot; ## [4] &quot;period&quot; ## [5] &quot;atomic_mass_rounded&quot; ## [6] &quot;melting_point_C&quot; ## [7] &quot;boiling_point_C&quot; ## [8] &quot;state_at_RT&quot; ## [9] &quot;density_g_per_mL&quot; ## [10] &quot;electronegativity_pauling&quot; ## [11] &quot;first_ionization_poten_eV&quot; ## [12] &quot;second_ionization_poten_eV&quot; ## [13] &quot;third_ionization_poten_eV&quot; ## [14] &quot;electron_affinity_eV&quot; ## [15] &quot;atomic_radius_ang&quot; ## [16] &quot;ionic_radius_ang&quot; ## [17] &quot;covalent_radius_ang&quot; ## [18] &quot;atomic_volume_cm3_per_mol&quot; ## [19] &quot;electrical_conductivity_mho_per_cm&quot; ## [20] &quot;specific_heat_J_per_g_K&quot; ## [21] &quot;heat_of_fusion_kJ_per_mol&quot; ## [22] &quot;heat_of_vaporization_kJ_per_mol&quot; ## [23] &quot;thermal_conductivity_W_per_m_K&quot; ## [24] &quot;polarizability_A_cubed&quot; ## [25] &quot;heat_atomization_kJ_per_mol&quot; "],["PCA.html", "Chapter 12 PCA 12.1 PCA 12.2 Drivers of PCA dimensions 12.3 Comparing principal components", " Chapter 12 PCA There is another way to look at our data in a cluster context - i.e. another way to identify clusters of samples that have similar properties based on the analytes in the data set. This method is called k-means, which we will look at later, because for it we first need to have a look at dimensionality reduction techniques, particularly principal components analysis (PCA). 12.1 PCA PCA looks at all the variance in a high dimensional data set and chooses new axes within that data set that align with the directions containing highest variance. These new axes are called principal components. Let’s look at an example: In the example above, the three dimensional space can be reduced to a two dimensional space with the principal components analysis. New axes (principal components) are selected (bold arrows on left) that become the x and y axes in the principal components space (right). We can run and visualize principal components analyses using the runMatrixAnalysis() function as in the example below: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) AK_lakes &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/alaska_lake_data.csv&quot;) head(AK_lakes) ## # A tibble: 6 x 7 ## lake park water_temp pH element mg_per_L element_type ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Devil_Mountain_Lake BELA 6.46 7.69 C 3.4 bound ## 2 Devil_Mountain_Lake BELA 6.46 7.69 N 0.028 bound ## 3 Devil_Mountain_Lake BELA 6.46 7.69 P 0 bound ## 4 Devil_Mountain_Lake BELA 6.46 7.69 Cl 10.4 free ## 5 Devil_Mountain_Lake BELA 6.46 7.69 S 0.62 free ## 6 Devil_Mountain_Lake BELA 6.46 7.69 F 0.04 free AK_lakes_pca &lt;- runMatrixAnalysis( data = AK_lakes, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = &quot;element&quot;, column_w_values_for_multiple_analytes = &quot;mg_per_L&quot;, columns_w_values_for_single_analyte = c(&quot;water_temp&quot;, &quot;pH&quot;), columns_w_additional_analyte_info = &quot;element_type&quot;, columns_w_sample_ID_info = c(&quot;lake&quot;, &quot;park&quot;) ) ## Not replacing any NAs in your data set library(ggrepel) ggplot(data = AK_lakes_pca, aes(x = Dim.1, y = Dim.2)) + geom_point(aes(fill = park), shape = 21, size = 4, alpha = 0.8) + geom_label_repel(aes(label = lake), alpha = 0.5) + theme_classic() Great! In this plot we can see that White Fish Lake and North Killeak Lake, both in BELA park, are quite different from the other parks (they are separated from the others along dimension 1, i.e. the first principal component). At the same time, Wild Lake, Iniakuk Lake, Walker Lake, and several other lakes in GAAR park are different from all the others (they are separated from the others along dimension 2, i.e. the second principal component). Important question: what makes the lakes listed above different from the others? Certainly some aspect of their chemistry, since that’s the data that this analysis is built upon, but how do we determine which analyte(s) are driving the differences among the lakes that we see in the PCA plot? 12.2 Drivers of PCA dimensions Let’s look at how to access the information about which analytes are major contributors to each principal component. This is important because it will tell you which analytes are associated with particular dimensions, and by extension, which analytes are associated with (and are markers for) particular groups in the PCA plot. This can be determined using an ordination plot. Let’s look at an example. We can obtain the ordination plot information using runMatrixAnalysis() with analysis = \"pca-ord\": ## Not replacing any NAs in your data set ## # A tibble: 6 x 3 ## analyte Dim.1 Dim.2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 water_temp 0.0769 -0.267 ## 2 pH 0.704 0.0190 ## 3 C 0.297 -0.248 ## 4 N 0.00446 0.732 ## 5 P 0.485 -0.0817 ## 6 Cl 0.978 0.0152 We can now visualize the ordination plot using our standard ggplot plotting techniques. Note the use of geom_label_repel() and filter() to label certain segments in the ordination plot. You do not need to use geom_label_repel(), you could use the built in geom_label(), but geom_label_repel() can make labelling your segments easier. source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) AK_lakes_pca_ord &lt;- runMatrixAnalysis( data = AK_lakes, analysis = c(&quot;pca-ord&quot;), column_w_names_of_multiple_analytes = &quot;element&quot;, column_w_values_for_multiple_analytes = &quot;mg_per_L&quot;, columns_w_values_for_single_analyte = c(&quot;water_temp&quot;, &quot;pH&quot;), columns_w_additional_analyte_info = &quot;element_type&quot;, columns_w_sample_ID_info = c(&quot;lake&quot;, &quot;park&quot;) ) library(ggforce) # Gives access to geom_circle library(ggrepel) # Gives access to geom_label_repel head(AK_lakes_pca_ord) ## # A tibble: 6 x 3 ## analyte Dim.1 Dim.2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 water_temp 0.0769 -0.267 ## 2 pH 0.704 0.0190 ## 3 C 0.297 -0.248 ## 4 N 0.00446 0.732 ## 5 P 0.485 -0.0817 ## 6 Cl 0.978 0.0152 ggplot(AK_lakes_pca_ord) + geom_segment(aes(x = 0, y = 0, xend = Dim.1, yend = Dim.2, color = analyte), size = 1) + geom_circle(aes(x0 = 0, y0 = 0, r = 1)) + geom_label_repel( data = filter(AK_lakes_pca_ord, Dim.1 &gt; 0.9, Dim.2 &lt; 0.1, Dim.2 &gt; -0.1), aes(x = Dim.1, y = Dim.2, label = analyte), xlim = c(1,1.5) ) + geom_label_repel( data = filter(AK_lakes_pca_ord, Dim.2 &gt; 0.5), aes(x = Dim.1, y = Dim.2, label = analyte), direction = &quot;y&quot;, ylim = c(1,1.5) ) + coord_cartesian(xlim = c(-1,1.5), ylim = c(-1,1.5)) + theme_bw() Great! With this ordination plot we can now see that the abundances of K, Cl, Br, and Na are the major contributors of variance to the first principal component (or the first dimension). The abundances of these elements are what make White Fish Lake and North Killeak Lake different from the other lakes. We can also see that the abundances of N, S, and Ca are the major contributors to variance in teh second dimension, whic means that these elements ar what set Wild Lake, Iniakuk Lake, Walker Lake, and several other lakes in GAAR park apart from the rest of the lakes in the data set. 12.3 Comparing principal components We also can access information about the how much of the variance in the data set is explained by each principal component: AK_lakes_pca_ord &lt;- runMatrixAnalysis( data = AK_lakes, analysis = c(&quot;pca-dim&quot;), column_w_names_of_multiple_analytes = &quot;element&quot;, column_w_values_for_multiple_analytes = &quot;mg_per_L&quot;, columns_w_values_for_single_analyte = c(&quot;water_temp&quot;, &quot;pH&quot;), columns_w_additional_analyte_info = &quot;element_type&quot;, columns_w_sample_ID_info = c(&quot;lake&quot;, &quot;park&quot;) ) ## Not replacing any NAs in your data set head(AK_lakes_pca_ord) ## # A tibble: 6 x 2 ## principal_component percent_variance_explained ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 48.8 ## 2 2 18.6 ## 3 3 11.6 ## 4 4 7.88 ## 5 5 4.68 ## 6 6 3.33 And we can now plot that using ggplot: ggplot( data = AK_lakes_pca_ord, aes(x = principal_component, y = percent_variance_explained) ) + geom_line() + geom_point() + theme_bw() Cool! We can see that the first principal component retains nearly 50% of the variance in the original dataset, while the second dimension contains only about 20%. "],["Exercises-5.html", "Chapter 13 Exercises 5 13.1 Option 1: Human metabolomics. 13.2 Option 2: Grape vine varieties", " Chapter 13 Exercises 5 In this set of exercises you will choose to complete one of the following options. For either option please note the following: Refer to Chapter 12 for help with PCA and ordination plots. The data sets and the runMatrixAnalysis() function can all be loaded into your R Session by running the command below: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) When you are filling out the runMatrixAnalysis() template, you can use the colnames() function to help you specify a long list of column names rather than typing them out by hand. For example, in the periodic table data set, we can refer to a set of columns (columns 10 through 20) with the following command: pts &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/per_table_small.csv&quot;) colnames(pts)[10:20] ## [1] &quot;electronegativity_pauling&quot; ## [2] &quot;first_ionization_poten_eV&quot; ## [3] &quot;second_ionization_poten_eV&quot; ## [4] &quot;third_ionization_poten_eV&quot; ## [5] &quot;electron_affinity_eV&quot; ## [6] &quot;atomic_radius_ang&quot; ## [7] &quot;ionic_radius_ang&quot; ## [8] &quot;covalent_radius_ang&quot; ## [9] &quot;atomic_volume_cm3_per_mol&quot; ## [10] &quot;electrical_conductivity_mho_per_cm&quot; ## [11] &quot;specific_heat_J_per_g_K&quot; We can use that command in the template, as in the example below. With the notation colnames(pts)[c(5:7,9:25)], we can mark columns 5 - 7 and 9 - 25 as columns_w_values_for_single_analyte (note what happens when you run c(5:7,9:25) in the console, and what happens when you run colnames(pts)[c(5:7,9:25)] in the console). With the notation colnames(pts)[c(1:4, 8)] we can mark columns 1 - 4 and column 8 as columns_w_sample_ID_info (note what happens when you run c(1:4, 8) in the console, and what happens when you run colnames(pts)[c(1:4, 8)] in the console). pca &lt;- runMatrixAnalysis( data = pts, analysis = &quot;pca&quot;, column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = colnames(pts)[c(5:7,9:25)], columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = colnames(pts)[c(1:4, 8)] ) Use the two suggestions above to help you complete one of the two options below. 13.1 Option 1: Human metabolomics. This first option is to work with a dataset describing metabolomics data (i.e. abundances of &gt; 100 different biochemicals) from each of 93 human patients, some of which have Chronic Kidney Disease. If you choose this option, your task is to discover a biomarker for Chronic Kidney Disease. This means that you will need to determine a metabolite whose abundance is strongly associated with the disease. To do this you should complete the following: Obtain the data and the latest version of runMatrixAnalysis() by running the following in your R console. Once you do that, the data set will be available as ckd_data. source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) Run a PCA analysis of the data (i.e. runMatrixAnalysis() with analysis = \"pca\") Plot the results of the analysis to determine which principal component (i.e. dimension) separates the healthy and kidney_disease samples. Obtain the ordination plot coordinates for the analytes in the PCA analysis (i.e. runMatrixAnalysis() with analysis = \"pca-ord\"). Visualize the ordination plot and determine which of the analytes are strongly associated with the principal component (i.e. dimension) separates the healthy and kidney_disease samples. Bingo! These analytes are associated with Chronic Kidney Disease and could be biomarkers for such. 13.2 Option 2: Grape vine varieties This second option is to work with a dataset describing metabolomics data (i.e. abundances of &gt; 100 different biochemicals) from 5 different wine grape varieties. If you choose this option, your task is to discover a biomarker for Chardonnay and a biomarker for Cabernet Sauvignon. This means that you will need to identify two metabolites, each of which are associated with one of those two grape varieties. To do this you should complete the following: Obtain the data and the latest version of runMatrixAnalysis() by running the following in your R console. Once you do that, the dataset will be available as wine_grape_data. source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) Run a PCA analysis of the data (i.e. runMatrixAnalysis() with analysis = \"pca\") Plot the results of the analysis to determine which principal component (i.e. dimension) separates the Chardonnay samples from the other varieties and the Cabernet Sauvignon samples from the other varieties. Obtain the ordination plot coordinates for the analytes in the PCA analysis (i.e. runMatrixAnalysis() with analysis = \"pca-ord\"). Visualize the ordination plot and determine which of the analytes are strongly associated with the principal component (i.e. dimension) separates the Chardonnay samples from the other varieties and the Cabernet Sauvignon samples from the other varieties. Bingo! These analytes are associated with those varieites and could be biomarkers for such. "],["k-means.html", "Chapter 14 k-means", " Chapter 14 k-means k-means can help us decide how to assign our data into clusters. It is generally desirable to have a small number of clusters, however, this must be balanced by not having the variance within each cluster be too big. To strike this balance point, the elbow method is used. For it, we must first determine the maximum within-group variance at each possible number of clusters. An illustration of this is shown in A below: One we know within-group variances, we find the “elbow” point - the point with minimum angle theta - thus picking the outcome with a good balance of cluster number and within-cluster variance (illustrated above in B and C.) Let’s try k-means using runMatrixAnalysis. We can use it in conjunction with analysis = \"pca\" or analysis = \"hclust\". Let’s do PCA first. To include k-means, we can just set kmeans = \"auto\". It’s important to note that kmeans cannot handle NAs. We must set something for the na_replacement argument. One solution is to ignore variables that have NAs for some values, which can be done by setting na_replacement = \"drop\". With kmeans = \"auto\" and na_replacement = \"drop\", we can now run our analyssis. The output now has an additional column called kmeans_cluster, which indicates what cluster each sample is in: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) solvents_pca_kmeans &lt;- runMatrixAnalysis( data = solvents, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = colnames(solvents)[c(3:5, 7:9, 11:12)], columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = c(&quot;solvent&quot;, &quot;formula&quot;, &quot;miscible_with_water&quot;, &quot;CAS_number&quot;, &quot;category&quot;), transpose = FALSE, kmeans = &quot;auto&quot;, na_replacement = &quot;drop&quot; ) ## Dropping any variables in your dataset that have NA as a value. ## Variables dropped: ## solubility_in_water vapor_pressure solvents_pca_kmeans ## # A tibble: 32 x 15 ## solvent formula miscible_with_w… CAS_number category sample_unique_ID ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 acetic… C2H4O2 TRUE 64-19-7 oxygen-… acetic_acid_C2H… ## 2 acetone C3H6O TRUE 67-64-1 oxygen-… acetone_C3H6O_ … ## 3 aceton… C2H3N TRUE 75-05-8 nitroge… acetonitrile_C2… ## 4 benzene C6H6 FALSE 71-43-2 hydroca… benzene_C6H6_FA… ## 5 benzon… C7H5N FALSE 100-47-0 nitroge… benzonitrile_C7… ## 6 1-buta… C4H10O FALSE 71-36-3 alcohol 1-butanol_C4H10… ## 7 2-buta… C4H8O FALSE 78-93-3 oxygen-… 2-butanone_C4H8… ## 8 carbon… CS2 FALSE 75-15-0 sulfide carbon_disulfid… ## 9 carbon… CCl4 FALSE 56-23-5 chlorin… carbon_tetrachl… ## 10 chloro… C6H5Cl FALSE 108-90-7 chlorin… chlorobenzene_C… ## # … with 22 more rows, and 9 more variables: Dim.1 &lt;dbl&gt;, Dim.2 &lt;dbl&gt;, ## # kmeans_cluster &lt;fct&gt;, boiling_point &lt;dbl&gt;, melting_point &lt;dbl&gt;, ## # density &lt;dbl&gt;, relative_polarity &lt;dbl&gt;, formula_weight &lt;dbl&gt;, ## # refractive_index &lt;dbl&gt; We can plot the results and color them according to the group that kmeans suggested: ggplot(solvents_pca_kmeans) + geom_point(aes(x = Dim.1, y = Dim.2, fill = kmeans_cluster), shape = 21, size = 5, alpha = 0.6) Hmmm, it looks like the elbow algorithm is suggesting lots of clusters. Why is this? Let’s look at the elbow plot itself. For this, we can just set kmeans = \"elbow\": solvents_pca_kmeans_elbow &lt;- runMatrixAnalysis( data = solvents, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = colnames(solvents)[c(3:5, 7:9, 11:12)], columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = c(&quot;solvent&quot;, &quot;formula&quot;, &quot;miscible_with_water&quot;, &quot;CAS_number&quot;, &quot;category&quot;), transpose = FALSE, kmeans = &quot;elbow&quot;, na_replacement = &quot;drop&quot; ) ## Dropping any variables in your dataset that have NA as a value. ## Variables dropped: ## solubility_in_water vapor_pressure solvents_pca_kmeans_elbow ## # A tibble: 31 x 2 ## cluster_number variance_within_cluster ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 142804. ## 2 2 67355. ## 3 3 49545. ## 4 4 38964. ## 5 5 30702. ## 6 6 25212. ## 7 7 20188. ## 8 8 16508. ## 9 9 14428. ## 10 10 12265. ## # … with 21 more rows This gives us the maximum variance within a cluster for each number of clusters. Let’s plot that: ggplot( solvents_pca_kmeans_elbow, aes(x = cluster_number, y = variance_within_cluster) ) + geom_col() + geom_point() + geom_line() Hmm, it looks like there aren’t any strong elbows in this plot - probably the reason that the elbow method chooses such a high number of clusters. Suppose we want to manually set the number of clusters? We can set kmeans = 3 if we want three clusters in the output. Below, let’s do just that. Let’s also plot the results and use geom_mark_ellipse from the ggforce package. library(ggforce) runMatrixAnalysis( data = solvents, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = colnames(solvents)[c(3:5, 7:9, 11:12)], columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = c(&quot;solvent&quot;, &quot;formula&quot;, &quot;miscible_with_water&quot;, &quot;CAS_number&quot;, &quot;category&quot;), transpose = FALSE, kmeans = 3, na_replacement = &quot;drop&quot; ) %&gt;% ggplot(aes(x = Dim.1, y = Dim.2, fill = kmeans_cluster)) + geom_point(shape = 21, size = 5) + geom_mark_ellipse(aes(label = kmeans_cluster), alpha = 0.2) + theme_classic() + coord_cartesian(xlim = c(-4,4), ylim = c(-4,4)) ## Dropping any variables in your dataset that have NA as a value. ## Variables dropped: ## solubility_in_water vapor_pressure Cool! One more important point: when using kmeans, the output of runMatrixAnalysis (specifically the kmeans_cluster column) can be used to create groupings for summary statistics. For example, suppose we want two groups of solvents and we want to calculate the mean and standard deviation in boiling points for each of those groups: solvents_clustered &lt;- runMatrixAnalysis( data = solvents, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = colnames(solvents)[c(3:5, 7:9, 11:12)], columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = c(&quot;solvent&quot;, &quot;formula&quot;, &quot;miscible_with_water&quot;, &quot;CAS_number&quot;, &quot;category&quot;), transpose = FALSE, kmeans = 2, na_replacement = &quot;drop&quot; ) ## Dropping any variables in your dataset that have NA as a value. ## Variables dropped: ## solubility_in_water vapor_pressure solvents_clustered_summary &lt;- solvents_clustered %&gt;% group_by(kmeans_cluster) %&gt;% summarize(mean_bp = mean(boiling_point)) ggplot() + geom_col( data = solvents_clustered_summary, aes(x = kmeans_cluster, y = mean_bp), color = &quot;black&quot;, fill = &quot;white&quot; ) + geom_point( data = solvents_clustered, aes(x = kmeans_cluster, y = boiling_point) ) Very good! Since we can use the outputs of our k-means analyses to run and visualize summary statistics, it’s possible that we’ll want to see the cluster plot (dendrogram or pca plot) alongside the summary stats plot. For this we can use the plot_grid function from the cowplot package. Let’s check it out: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) solvents_clustered &lt;- runMatrixAnalysis( data = solvents, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = colnames(solvents)[c(3:5, 7:9, 11:12)], columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = c(&quot;solvent&quot;, &quot;formula&quot;, &quot;miscible_with_water&quot;, &quot;CAS_number&quot;, &quot;category&quot;), transpose = FALSE, kmeans = 4, na_replacement = &quot;drop&quot; ) ## Dropping any variables in your dataset that have NA as a value. ## Variables dropped: ## solubility_in_water vapor_pressure colors &lt;- c(&quot;maroon&quot;, &quot;gold&quot;, &quot;grey&quot;, &quot;white&quot;) pca_plot &lt;- ggplot( data = solvents_clustered, aes(x = Dim.1, y = Dim.2, fill = kmeans_cluster) ) + geom_mark_ellipse( aes(label = kmeans_cluster), alpha = 0.5, label.lineheight = 0.2, size = 0.5) + geom_point(shape = 21, size = 2) + theme_classic() + guides(fill = &quot;none&quot;) + scale_x_continuous(name = &quot;PCA dimension 1&quot;, breaks = seq(-8,8,1)) + scale_y_continuous(name = &quot;PCA dimension 2&quot;, breaks = seq(-7,7,1)) + scale_fill_manual(values = colors) + coord_cartesian(xlim = c(-8,8), ylim = c(-7,7)) solvents_clustered_summary &lt;- solvents_clustered %&gt;% group_by(kmeans_cluster) %&gt;% summarize(mean_bp = mean(boiling_point)) bar_plot &lt;- ggplot() + geom_violin( data = solvents_clustered, aes(x = kmeans_cluster, y = boiling_point, fill = kmeans_cluster), size = 0.5, color = &quot;black&quot;, alpha = 0.6, width = 0.5 ) + geom_crossbar( data = solvents_clustered_summary, aes(x = kmeans_cluster, y = mean_bp, ymin = mean_bp, ymax = mean_bp), color = &quot;black&quot;, width = 0.5 ) + geom_point( data = solvents_clustered, aes(x = kmeans_cluster, y = boiling_point), size = 2, color = &quot;black&quot;, alpha = 0.6 ) + scale_y_continuous(name = &quot;Boiling point&quot;, breaks = seq(0,250,20)) + scale_x_discrete(name = &quot;Cluster&quot;) + scale_fill_manual(values = colors) + theme_classic() + coord_flip() + guides(fill = &quot;none&quot;) + theme(legend.position = &quot;bottom&quot;) cowplot::plot_grid(pca_plot, bar_plot, align = &quot;h&quot;, axis = &quot;b&quot;, labels = &quot;AUTO&quot;) Now we are really rockin!! "],["Exercises-6.html", "Chapter 15 Exercises 6 15.1 Question 1 15.2 Question 2 15.3 Question 3 15.4 Question 4 15.5 Question 5 15.6 Question 6 15.7 Question 7 15.8 Challenge (optional)", " Chapter 15 Exercises 6 Use the wine grapes dataset (it’s stored as wine_grape_data after you run the source(...) command). Put your answers to the following questions into a powerpoint file - the plots in the slide, and the code in the notes section. Please email me the powerpoint file and let me know if you have any questions. Good luck and have fun! 15.1 Question 1 Run a principal components analysis on the dataset. Use na_replacement = \"drop\" (so that variables with NA values are not included in the analysis) and generate clusters automatically using kmeans by setting kmeans = \"auto\". Make scatter plot of the results. How many clusters does kmeans recommend? 15.2 Question 2 Modify your code from Question 1 so that only two clusters are generated. Plot the results. Install ggforce (install.packages(ggforce)), then load that package (library(ggforce)), and use geom_mark_ellipse to highlight each cluster in your plot (note that the fill aesthetic is required to mark groups). Which varieties are put into each of the two clusters? 15.3 Question 3 Use an ordination plot to determine what chemicals makes Chardonnay so different from the other varieties. To what class of compounds do these chemical belong? 15.4 Question 4 Modify your code from Question 2 so that three clusters are generated. Plot the results. Install ggforce (install.packages(ggforce)), then load that package (library(ggforce)), and use geom_mark_ellipse to highlight each cluster in your plot (note that the fill aesthetic is required to mark groups). Based on this plot, which grape variety undergoes the least amount of change, chemically speaking, bewteen dry and well-watered conditions? 15.5 Question 5 Run a heirarchical clustering analysis on the wine grapes data set, using kmeans to create three groups, and also continue using na_replacement = \"drop\". Plot the results. Which grape variety undergoes the most change in terms of its chemistry between well-watered and dry conditions? (hint: remember that the x-axis shows the distances bewteen nodes and tips, the y-axis is meaningless). 15.6 Question 6 Google “Quercetin”. What kind of compound is it? Use the clusters created by the heirarchical clustering analysis in question 5 as groups for which to calculate summary statistics. Calculate the mean and standard deviation of the concentration of Quercetin in each group. Plot the result using geom_pointrange and adjust axis font sizes so that they are in good proportion with the size of the plot. Also specify a theme (for example, theme_classic()). Does one cluster have a large amount of variation in Quercetin abundance? How can this be? Aren’t all the samples (i.e. grape varieties) in that cluster are closely related in terms of their chemistry? 15.7 Question 7 Use cowplot::plot_grid to display your plots from questions 4 and 5 next to each other. 15.8 Challenge (optional) Use cowplot to display your plots from questions 4, 5, and 6 alongside each other. Make your combined plot as attractive as possible! Use each of the following: align = TRUE inside geom_tiplab() nrow = 1 inside plot_grid() rel_widths = &lt;your_choice&gt; inside plot_grid() name = &lt;your_choice&gt; inside scale_*_* label = kmeans_cluster inside geom_mark_ellipse() breaks = &lt;your_choice&gt; inside scale_x_continuous() or scale_y_continuous() (as an example, breaks = seq(0,10,1)) Also, consider using: guides(fill = \"none\", color = \"none\") Install the RColorBrewer package, and use one of its color schemes. As an example with the color scheme Set1: scale_fill_brewer(palette = \"Set1\", na.value = \"grey\") scale_color_brewer(palette = \"Set1\", na.value = \"grey\") Save your plot as a png using ggsave(). Maybe something like this: "],["comparing-means.html", "Chapter 16 comparing means 16.1 The p value 16.2 Two means 16.3 More than two means 16.4 Further reading", " Chapter 16 comparing means Often, we want to know if our study subjects contain different amounts of certain analytes. For example, “Does this lake over here contain more potassium than that lake over there?” For this, we need statistical tests. Here, we will have a look at comparing mean values for analyte abundance in situations with two samples and in situations with more than two samples. To run these statistical analyses, we will need several new R packages: rstatix, agricolae, and multcompView. Please install these with install.packages(\"rstatix\"), install.packages(\"agricolae\"), and install.packages(\"multcompView\"). Load them into your R session using library(rstatix), library(agricolae), and library(multcompView). 16.1 The p value “The p value represents the probability of getting data as extreme as our results if the null hypothesis is true.” In other words - the p value is the probability that we would observe the differences we did, if in fact there were no differences in the means at all. 16.2 Two means When we are comparing two means, we need to first determine what kind of statistical test we can use on our data. If (i) our data can be modelled by a normal distribution and (ii) the variances about the two means are similar, then we can use a more powerful test (i.e. a test that will be more likely to detect a difference in means, assuming one exists). We can check our data for normality and similar variances using the Shapiro test and the Levene test. Let’s use the hawaii_aquifers data as an example, and let’s consider only the element potassium: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) K_data &lt;- hawaii_aquifers %&gt;% dplyr::filter(analyte == &quot;K&quot;) K_data ## # A tibble: 110 x 6 ## aquifer_code well_name longitude latitude analyte abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aquifer_1 Alewa_Heights_Spring NA NA K 0.6 ## 2 aquifer_1 Beretania_High_Service NA NA K 3.1 ## 3 aquifer_1 Beretania_Low_Service NA NA K 3.6 ## 4 aquifer_1 Kuliouou_Well -158. 21.3 K 3.9 ## 5 aquifer_1 Manoa_Well_II -158. 21.3 K 0.6 ## 6 aquifer_1 Moanalua_Wells_Pump_1 -158. 21.4 K 2.9 ## 7 aquifer_1 Moanalua_Wells_Pump_2 -158. 21.4 K 2.8 ## 8 aquifer_1 Moanalua_Wells_Pump_3 -158. 21.4 K 3.4 ## 9 aquifer_1 Nuuanu_Aerator_Well -158. 21.4 K 0.4 ## 10 aquifer_1 Palolo_Tunnel -158. 21.3 K 0.7 ## # … with 100 more rows To work with two means, let’s just look at aquifers 1 and 6: K_data_1_2 &lt;- K_data %&gt;% dplyr::filter(aquifer_code %in% c(&quot;aquifer_1&quot;, &quot;aquifer_6&quot;)) K_data_1_2 ## # A tibble: 24 x 6 ## aquifer_code well_name longitude latitude analyte abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aquifer_1 Alewa_Heights_Spring NA NA K 0.6 ## 2 aquifer_1 Beretania_High_Service NA NA K 3.1 ## 3 aquifer_1 Beretania_Low_Service NA NA K 3.6 ## 4 aquifer_1 Kuliouou_Well -158. 21.3 K 3.9 ## 5 aquifer_1 Manoa_Well_II -158. 21.3 K 0.6 ## 6 aquifer_1 Moanalua_Wells_Pump_1 -158. 21.4 K 2.9 ## 7 aquifer_1 Moanalua_Wells_Pump_2 -158. 21.4 K 2.8 ## 8 aquifer_1 Moanalua_Wells_Pump_3 -158. 21.4 K 3.4 ## 9 aquifer_1 Nuuanu_Aerator_Well -158. 21.4 K 0.4 ## 10 aquifer_1 Palolo_Tunnel -158. 21.3 K 0.7 ## # … with 14 more rows ggplot(K_data_1_2, aes(x = aquifer_code, y = abundance)) + geom_boxplot() + geom_point() Are these data normally distributed? Do they have similar variance? Let’s get a first approximation by looking at a plot: K_data_1_2 %&gt;% ggplot(aes(x = abundance)) + geom_histogram(bins = 30) + facet_wrap(~aquifer_code) + geom_density(aes(y = ..density..*10), colour = &quot;blue&quot;) Based on this graphic, it’s hard to say! Let’s use a statistical test to help. When we want to run the Shaprio test, we are looking to see if each group has normally distributed here (here group is “aquifer_code”, i.e. aquifer_1 and aquifer_6). This means we need to group_by(aquifer_code) before we run the test: K_data_1_2 %&gt;% group_by(aquifer_code) %&gt;% shapiro_test(abundance) ## # A tibble: 2 x 4 ## aquifer_code variable statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aquifer_1 abundance 0.885 0.102 ## 2 aquifer_6 abundance 0.914 0.239 Both p-values are above 0.05! This means that the distributions are not significantly different from a normal distribution. What about the variances about the two means? Are they similar? For this we need a Levene test. With that test, we are not looking within each group, but rather across groups - this means we do NOT need to group_by(aquifer_code) and should specify a y ~ x formula instead: K_data_1_2 %&gt;% levene_test(abundance ~ aquifer_code) ## # A tibble: 1 x 4 ## df1 df2 statistic p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 22 0.289 0.596 The p-value from this test is 0.596! This means that their variances are not significantly different. Now, since our data passed both test, this means we can use a normal t-test. A t-test is a parametric test. This means that it relies on modelling the data using a normal distribution in order to make comparisons. It is also a powerful test. This means that it is likely to detect a difference in means, assuming one is present. Let’s try it out: K_data_1_2 %&gt;% t_test(abundance ~ aquifer_code) ## # A tibble: 1 x 8 ## .y. group1 group2 n1 n2 statistic df p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abundance aquifer_1 aquifer_6 12 12 -2.75 20.5 0.0121 A p-value of 0.012! This is below 0.05, meaning that there is a 95% chance that the two means are different. Suppose that our data had not passed the Shapiro and/or Levene tests. We would then need to use a Wilcox test. The Wilcox test is a non-parametric test, which means that it does not use a normal distribution to model the data in order to make comparisons. This means that is a less powerful test than the t-test, which means that it is less likely to detect a difference in the means, assuming there is one. For fun, let’s try that one out and compare the p-values from the two methods: K_data_1_2 %&gt;% wilcox_test(abundance ~ aquifer_code) ## # A tibble: 1 x 7 ## .y. group1 group2 n1 n2 statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 abundance aquifer_1 aquifer_6 12 12 33.5 0.0282 A p-value of 0.028! This is higher than the value given by the t-test (0.012). That is because the Wilcox test is a less powerful test: it is less likely to detect differences in means, assuming they exist. 16.3 More than two means In the previous section we compared two means. What if we want to compare means from more than two study subjects? The first step is again to determine which tests to use. Let’s consider our hawaii aquifer data again, though this time let’s use all the aquifers, not just two: K_data &lt;- hawaii_aquifers %&gt;% dplyr::filter(analyte == &quot;K&quot;) K_data ## # A tibble: 110 x 6 ## aquifer_code well_name longitude latitude analyte abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aquifer_1 Alewa_Heights_Spring NA NA K 0.6 ## 2 aquifer_1 Beretania_High_Service NA NA K 3.1 ## 3 aquifer_1 Beretania_Low_Service NA NA K 3.6 ## 4 aquifer_1 Kuliouou_Well -158. 21.3 K 3.9 ## 5 aquifer_1 Manoa_Well_II -158. 21.3 K 0.6 ## 6 aquifer_1 Moanalua_Wells_Pump_1 -158. 21.4 K 2.9 ## 7 aquifer_1 Moanalua_Wells_Pump_2 -158. 21.4 K 2.8 ## 8 aquifer_1 Moanalua_Wells_Pump_3 -158. 21.4 K 3.4 ## 9 aquifer_1 Nuuanu_Aerator_Well -158. 21.4 K 0.4 ## 10 aquifer_1 Palolo_Tunnel -158. 21.3 K 0.7 ## # … with 100 more rows ggplot(data = K_data, aes(x = aquifer_code, y = abundance)) + geom_boxplot() + geom_point(color = &quot;maroon&quot;, alpha = 0.6, size = 3) Let’s check visually to see if each group is normally distributed and to see if they have roughly equal variance: K_data %&gt;% group_by(aquifer_code) %&gt;% ggplot(aes(x = abundance)) + geom_histogram(bins = 30) + facet_wrap(~aquifer_code) + geom_density(aes(y = ..density..*10), colour = &quot;blue&quot;) Again, it is somewhat hard to tell visually if these data are normally distributed. It seems pretty likely that they have different variances about the means, but let’s check using the Shapiro and Levene tests. Don’t forget: with the Shaprio test, we are looking within each group and so need to group_by(), with the Levene test, we are looking across groups, and so need to provide a y~x formula: K_data %&gt;% group_by(aquifer_code) %&gt;% shapiro_test(abundance) ## # A tibble: 10 x 4 ## aquifer_code variable statistic p ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aquifer_1 abundance 0.885 0.102 ## 2 aquifer_10 abundance 0.864 0.163 ## 3 aquifer_2 abundance 0.913 0.459 ## 4 aquifer_3 abundance 0.893 0.363 ## 5 aquifer_4 abundance 0.948 0.421 ## 6 aquifer_5 abundance 0.902 0.421 ## 7 aquifer_6 abundance 0.914 0.239 ## 8 aquifer_7 abundance 0.915 0.355 ## 9 aquifer_8 abundance 0.842 0.220 ## 10 aquifer_9 abundance 0.786 0.00000866 K_data %&gt;% levene_test(abundance ~ aquifer_code) ## # A tibble: 1 x 4 ## df1 df2 statistic p ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 100 3.12 0.00239 Based on these tests, it looks like the data for aquifer 9 is significantly different from a normal distribution (Shaprio test p = 0.000008), and the means are certainly different from one another (Levene test p = 0.002). Let’s assume for a second that our data passed these tests. This means that we could reasonably model our data with normal distributions and use a parametric test to compare means. This means that we can use an ANOVA to test for differences in means. 16.3.1 ANOVA, Tukey tests We will use the anova_test function from the package rstatix. It will tell us if any of the means in the data are statistically different from one another. However, if there are differences between the means, it will not tell us which of them are different. K_data %&gt;% anova_test(abundance ~ aquifer_code) ## Coefficient covariances computed by hccm() ## # A tibble: 1 x 7 ## Effect DFn DFd F p `p&lt;.05` ges ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aquifer_code 9 100 10.0 7.72e-11 * 0.474 A p-value of 7.7e-11! There are definitely some significant differences among this group. But, WHICH are different from one another though? For this, we need to run Tukey’s Honest Significant Difference test (implemented using tukey_hsd). This will essentially run t-test on all the pairs of study subjects that we can derive from our data set (in this example, aquifer_1 vs. aquifer_2, aquifer_1 vs. aquifer_3, etc.). After that, it will correct the p-values according to the number of comparisons that it performed. This controls the rate of type I error that we can expect from the test. These corrected values are provided to us in the p.adj column. K_data %&gt;% tukey_hsd(abundance ~ aquifer_code) ## # A tibble: 45 x 9 ## term group1 group2 null.value estimate conf.low conf.high p.adj ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aqui… aquif… aquif… 0 0.00357 -2.00 2.01 1.00e+0 ## 2 aqui… aquif… aquif… 0 1.44 -0.668 3.55 4.56e-1 ## 3 aqui… aquif… aquif… 0 0.375 -2.35 3.10 1.00e+0 ## 4 aqui… aquif… aquif… 0 -1.15 -2.75 0.437 3.68e-1 ## 5 aqui… aquif… aquif… 0 -0.845 -3.09 1.40 9.68e-1 ## 6 aqui… aquif… aquif… 0 1.98 0.261 3.71 1.15e-2 ## 7 aqui… aquif… aquif… 0 2.70 0.837 4.56 3.56e-4 ## 8 aqui… aquif… aquif… 0 -0.125 -2.85 2.60 1.00e+0 ## 9 aqui… aquif… aquif… 0 -0.378 -1.78 1.03 9.97e-1 ## 10 aqui… aquif… aquif… 0 1.44 -0.910 3.79 6.13e-1 ## # … with 35 more rows, and 1 more variable: p.adj.signif &lt;chr&gt; Using the output from our tukey test, we can determine which means are similar. We can do this using the p_groups function: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) groups_based_on_tukey &lt;- K_data %&gt;% tukey_hsd(abundance ~ aquifer_code) %&gt;% p_groups() groups_based_on_tukey ## # A tibble: 10 x 3 ## treatment group spaced_group ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 aquifer_1 ab &quot;ab &quot; ## 2 aquifer_10 abc &quot;abc &quot; ## 3 aquifer_2 acd &quot;a cd&quot; ## 4 aquifer_3 abcd &quot;abcd&quot; ## 5 aquifer_4 b &quot; b &quot; ## 6 aquifer_5 ab &quot;ab &quot; ## 7 aquifer_6 cd &quot; cd&quot; ## 8 aquifer_7 d &quot; d&quot; ## 9 aquifer_8 abc &quot;abc &quot; ## 10 aquifer_9 ab &quot;ab &quot; We can use the output from p_groups to annotate our plot: ggplot(data = K_data, aes(x = aquifer_code, y = abundance)) + geom_boxplot() + geom_point(color = &quot;maroon&quot;, alpha = 0.6, size = 3) + geom_text(data = groups_based_on_tukey, aes(x = treatment, y = 9, label = group)) Excellent! This plot shows us, using the letters above each aquifer, which means are the same and which are different. If a letter is shared among the labels above two aquifers, it means that their means do not differ significantly. For example, aquifer 2 and aquifer 6 both have “b” in their labels, so their means are not different - and are the same as those of aquifers 3 and 10. 16.3.2 Kruskal, Dunn tests The above ANOVA example is great, but remember - our data did not pass the Shapiro or Levene tests. This means not all our data can be modelled by a normal distribution and taht we need to use a non-parametric test. The non-parametric alternative to the ANOVA is called the Kruskal test. Like the Wilcox test, it is less powerful that its parametric relative, meaning that it is less likely to detected differences, should they exist. However, since our data do not pass the Shapiro/Levene tests, we have to resort to the Kruskal test. Let’s try it out: K_data %&gt;% kruskal_test(abundance ~ aquifer_code) ## # A tibble: 1 x 6 ## .y. n statistic df p method ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 abundance 110 57.7 9 0.0000000037 Kruskal-Wallis A p-value of 3.9e-9! This is higher than the p-value from running ANOVA on the same data (remember, the Kruskal test is less powerful). Never the less, the value is still well below 0.05, meaning that some of the means are different. So, how do we determine WHICH are different from one another? When we ran ANOVA the follow-up test (the post hoc test) was Tukey’s HSD. After the Kruskal test, the post hoc test we use is the Dunn test. Let’s try: K_data %&gt;% dunn_test(abundance ~ aquifer_code) ## # A tibble: 45 x 9 ## .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 abundan… aquifer_1 aquifer… 12 7 -0.205 0.838 1 ns ## 2 abundan… aquifer_1 aquifer… 12 6 2.25 0.0242 0.702 ns ## 3 abundan… aquifer_1 aquifer… 12 3 0.911 0.362 1 ns ## 4 abundan… aquifer_1 aquifer… 12 17 -2.70 0.00702 0.232 ns ## 5 abundan… aquifer_1 aquifer… 12 5 -1.15 0.252 1 ns ## 6 abundan… aquifer_1 aquifer… 12 12 2.53 0.0113 0.351 ns ## 7 abundan… aquifer_1 aquifer… 12 9 3.02 0.00254 0.0967 ns ## 8 abundan… aquifer_1 aquifer… 12 3 0.182 0.855 1 ns ## 9 abundan… aquifer_1 aquifer… 12 36 -0.518 0.605 1 ns ## 10 abundan… aquifer_… aquifer… 7 6 2.20 0.0278 0.777 ns ## # … with 35 more rows This gives us adjusted p-values for all pairwise comparisons. Once again, we can use p_groups() to give us a compact letter display for each group, which can then be used to annotate the plot: groups_based_on_tukey &lt;- K_data %&gt;% dunn_test(abundance ~ aquifer_code) %&gt;% p_groups() groups_based_on_tukey ## # A tibble: 10 x 3 ## treatment group spaced_group ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 aquifer_1 abcd &quot;abcd&quot; ## 2 aquifer_10 abcd &quot;abcd&quot; ## 3 aquifer_2 abc &quot;abc &quot; ## 4 aquifer_3 abcd &quot;abcd&quot; ## 5 aquifer_4 d &quot; d&quot; ## 6 aquifer_5 acd &quot;a cd&quot; ## 7 aquifer_6 ab &quot;ab &quot; ## 8 aquifer_7 b &quot; b &quot; ## 9 aquifer_8 abcd &quot;abcd&quot; ## 10 aquifer_9 cd &quot; cd&quot; ggplot(data = K_data, aes(x = aquifer_code, y = abundance)) + geom_boxplot() + geom_point(color = &quot;maroon&quot;, alpha = 0.6, size = 3) + geom_text(data = groups_based_on_tukey, aes(x = treatment, y = 9, label = group)) Note that these groupings are different from those generated by ANOVA/Tukey. 16.4 Further reading For more on comparing multiple means in R: www.datanovia.com For more on parametric versus non-parametric tests: Statistics by Jim "],["exam.html", "Chapter 17 Exam 17.1 Data set 1: barley, corn, hops 17.2 Data set 2: hops, hops, hops!", " Chapter 17 Exam For this examination, imagine that you are an analytical chemist that was just hired by Fireside Brewing, a large beer brewing company in Montana. The person that previously worked in your postion, Mr. Porter, recently completed a chemical analysis of the major ingredients your Fireside Brewing uses. However, Mr. Porter didn’t have time to finish analyzing the data that was collected. At Fireside Brewing, your boss is Mrs. Pilsner, a friendly woman who has worked in the brewing industry for decades and is a certified cicerone (a professional beer taster). As an expert taster, she is familiar with all the classes of compounds in beer ingredients and their corresponding flavors, but she is not an expert in chemistry or data analysis. For your first task in this new position, Mrs. Pilsner wants you to finish analyzing Mr. Porter’s data. Mrs. Pilsner has not given you specific instructions on how to analyze the data, rather, she has a set of specific questions that she wants answered to improve Fireside Brewing. Mr. Porter left some notes about how to perform the analyses, but, since he was close to retirement and spent too much time riding his bicycle and drinking beer, the notes are not complete. Use Mr. Porter’s incomplete notes to help you answer the questions below. Answer each question with both (i) at least one high quality plot that provides a visual answer and (ii) a caption that provides a written answer. If a question necessitates an answer that is difficult to show with a plot (for example, a single p value), include that answer in the caption. note: a high quality plot is one in which, for example, axes tick labels do not overlap but also fill the space available to them, colors are used, raw data is plotted (if possible), axes labels are customized, an appropriate theme is chosen, and geoms are chosen carefully. The plots should be visually attractive and professional. You are a new employee - impress people! 17.1 Data set 1: barley, corn, hops 17.1.1 Question 1: Mrs Pilsner: We are having problems with our Pale Ale. Its flavor is not consistent from batch to bath. We need to identify the ingredient that is causing this problem. What is the variability of each analyte class (ketone, sesquiterpene, etc.) in each of the ingredients we use (barley, corn, hop extract, hop oil, and hops), and particuarly, which ingredient and which class are most highly variable? 17.1.2 Question 2: Mrs. Pilsner: We are experimenting with a new beer that uses a 1:1:1 mixture of all the hop products that we use (hops, hop oil, and hop extract). I want to try to predict what type of flavor profile this beer might have. Using Mr. Porter’s data for those three ingredients (hops, hop oil, and hop extract), can you predict what the abundance of each analyte class will be in the mixture? 17.1.3 Question 3: Mrs. Pilsner: We are trying to reduce the price of our India Session Ale by replacing its hops with either hop extract or hop oil. Chemically speaking, which is more similar to hops - hop extract, or hop oil? 17.1.4 Question 4: Mrs. Pilsner: I recently heard a rumor from other cicerones that barley containing significantly more 3-methylbutanal than all other aldehydes leads to a maltier flavor, compared to other types of barley. Statistically speaking, does our barley currently contain significantly more 3-methylbutanal than all other aldehydes? note from Mr. Porter: Reminder to self: filter out analytes with abundance == 0 for all replicates before conducting statistical tests, otherwise they will be biased. 17.1.5 Question 5: Mrs. Pilsner: When I taste our beers made with hop extract, they don’t taste very hoppy compared to our beers made with hop oil or hops. Why is this - how do our different ingredients cluster according to their average chemical composition? 17.2 Data set 2: hops, hops, hops! After you answer all Mrs. Pilsner’s questions about Mr. Porter’s data, Mrs. Pilsner tells you about another data set that was collected by Mr. Porter’s intern, an undergraduate named Ms. Amber Stout. Amber analyzed the chemical composition of many different hop varieties. Please answer Mrs. Pilsner’s questions about this data in the same way you did for the questions above. 17.2.1 Question 1 Mrs. Pilsner: There are three major types of hops - aroma hops, dry hopping hops, and bittering hops. We had an emergency scenario last month where we ran out of bittering hops. We needed to substitute either aroma hops or dry hopping hops into the recipe. At the time, we used aroma hops. Was that a good decision? If that emergency were to arise again, should we substitute aroma hops again, or should we use dry hopping hops instead? 17.2.2 Question 2: Mrs. Pilsner: We buy our hops from many different countries of origin. I’ve always wondered which country has the most unique hop varieties. Can you make a plot that shows me which country has the most hop varieties that have no overlap with the flavor profiles of other countries hops? What chemical(s) largely drive this difference? 17.2.3 Question 3: Mrs. Pilsner: If we wanted to prepare our own hop oil, which type of hops should we use - aroma hops, bittering hops, or dry hopping hops? In other words, is there a statistical difference in the amount of total oil that each type of hops has? Mr. Porter: Note to self: here we are interested not in comparing total_oil from different hop_varieties, but instead in comparing total_oil from hops with different hop_brewing_usage. 17.2.4 Question 4: Mrs. Pilsner: We are interested in producing a new type of beer with the lowest amount of acids possible. Which variety of hops has the lowest total amount of acids? 17.2.5 Question 5: Mrs. Pilsner: Cascade hops are a very popular variety, and also a variety we use in producing our flagship IPA. However, due to COVID-19, cascade hops are on back order for six months. What style of hop is most similar to cascade? I would like to order it to use in place of cascade hops for the batch of IPA we will brew next week. "],["datasets.html", "A Datasets A.1 hawaii_aquifers", " A Datasets A.1 hawaii_aquifers hawaii_aquifers ## # A tibble: 990 x 6 ## aquifer_code well_name longitude latitude analyte abundance ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 aquifer_1 Alewa_Heights_Spring NA NA SiO2 11 ## 2 aquifer_1 Alewa_Heights_Spring NA NA Cl 20 ## 3 aquifer_1 Alewa_Heights_Spring NA NA Mg 7.6 ## 4 aquifer_1 Alewa_Heights_Spring NA NA Na 14 ## 5 aquifer_1 Alewa_Heights_Spring NA NA K 0.6 ## 6 aquifer_1 Alewa_Heights_Spring NA NA SO4 7 ## 7 aquifer_1 Alewa_Heights_Spring NA NA HCO3 51 ## 8 aquifer_1 Alewa_Heights_Spring NA NA dissolved_sol… 118 ## 9 aquifer_1 Alewa_Heights_Spring NA NA Ca 6.3 ## 10 aquifer_1 Beretania_High_Serv… NA NA SiO2 30 ## # … with 980 more rows This data set contains six columns and 990 rows. Each row provides the abundance of a particular analyte in a particular well (well_name). Each well draws on a particular aquifer (aquifer_code), and is located at a particular latitude and longitude. Format: long (i.e. tidy) Contains : TRUE "],["functions-1.html", "B Functions B.1 Libraries and packages B.2 Operators B.3 Data wrangling B.4 Plotting B.5 Statistics B.6 t.test() B.7 anova()", " B Functions Here are functions that we will cover in this class, though you should definitely use the internet as a source of other functions to meet your needs! B.1 Libraries and packages B.1.1 install.packages() (base function) Use it to install a package! For example: install.packages(&quot;tidyverse&quot;) B.2 Operators B.2.1 %in% (base function). Use it to find An example: # Find cars in the horsepower mid-range mid_strength &lt;- filter(mtcars,hp %in% 100:200) B.2.2 &lt; “less than” B.2.3 &lt;= “less than or equal to” B.2.4 &gt; “greater than” B.2.5 &gt;= “greater than or equal to” B.2.6 == “equal to” B.2.7 != “not equal to” B.2.8 | “or” B.3 Data wrangling B.3.1 read_csv() (a readr function - part of the tidyverse) Use it to read .csv files. B.3.2 read.csv() (base function) Use it to read .csv files (though please consider the newer read_csv instead). B.3.3 readCSV() (custom function) Use it to interactively read in files. B.3.4 head() (base function) Use it to see just the first few lines of a dataframe: head(algae_chemistry_data) ## # A tibble: 6 x 5 ## replicate algae_strain harvesting_regime chemical_species abundance ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Tsv1 Heavy FAs 520. ## 2 1 Tsv1 Heavy saturated_Fas 123. ## 3 1 Tsv1 Heavy omega_3_polyunsaturated_Fas 186. ## 4 1 Tsv1 Heavy monounsaturated_Fas 28.4 ## 5 1 Tsv1 Heavy polyunsaturated_Fas 369. ## 6 1 Tsv1 Heavy omega_6_polyunsaturated_Fas 183. B.3.5 filter() (a dplyr function - part of the tidyverse) Use it to filter your data. B.3.6 mutate() (a dplyr function - part of the tidyverse). An example: # Add a column &quot;kilometers per liter (klp)&quot; and calculate using mpg EUROPE_mtcars &lt;- mutate(mtcars,kpl = mpg*0.4251) B.3.7 case_when() (a dplyr function - part of the tidyverse) # Classify the fuel efficiency of the cars in the mtcars data set efficiency &lt;- mutate( mtcars, fuel_efficient = case_when( mpg &lt;= 15 ~ &quot;Bad&quot;, mpg &gt; 15 &amp; mpg &lt; 25 ~ &quot;ok&quot;, mpg &gt;= 25 ~ &quot;Good&quot; ) ) efficiency ## # A tibble: 32 x 12 ## mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows, and 1 more variable: fuel_efficient &lt;chr&gt; B.3.8 pivot_longer() (a tidyr function - part of the tidyverse) B.3.9 group_by() (a dplyr function - part of the tidyverse) B.3.10 summarize() (a dplyr function - part of the tidyverse) B.3.11 %&gt;% (a magrittr function - part of the tidyverse) Use it to send the output of one command directly to the next. # How many cars with 8 cylinders have automatic transmissions? mtcars %&gt;% filter(cyl &gt; 6) %&gt;% group_by(am) %&gt;% summarize(n = n()) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## am n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 12 ## 2 1 2 B.4 Plotting B.4.1 plot() (base function) Use it to create rudimentary plots of some data objects. B.4.2 ggplot2::ggplot() (a ggplot2 function - part of the tidyverse) Use it to plot your data. B.4.3 ggplot2::aes() (a ggplot2 function - part of the tidyverse) B.4.4 ggplot2::geom_*() (a ggplot2 function - part of the tidyverse) B.4.5 ggplot2::facet_grid() (a ggplot2 function - part of the tidyverse) B.4.6 ggplot2::scale_*_*() (a ggplot2 function - part of the tidyverse) B.4.7 ggplot2::theme_*() (a ggplot2 function - part of the tidyverse) B.4.8 ggplot2::coord_flip() (a ggplot2 function - part of the tidyverse) B.5 Statistics B.5.1 mean() (base function) B.5.2 sd() (base function) B.5.3 table() (base function) Use it to calculate the number of items in a category For example, how many bound versus unbound elements are there in the alaksa_lakes_data? ## Let&#39;s load our CHEM5725 environment source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) ## Let&#39;s just look at one lake so that there aren&#39;t duplicate elements DML &lt;- dplyr::filter(alaska_lake_data, lake == &quot;Devil_Mountain_Lake&quot;) table(DML$element_type) ## ## bound free ## 3 8 We can access each element separately: table(DML$element_type)[1] ## bound ## 3 table(DML$element_type)[2] ## free ## 8 So, if we want to calculate percent free elements: table(DML$element_type)[2] / sum(table(DML$element_type)) * 100 ## free ## 72.72727 B.5.4 weighted.mean() (from the stats package) Suppose we want to calculate mean abundance of pyruvate in patients with kidney disease versus patients that are healthy. First, let’s just make the data set smaller and easier to see: ckd_data_pyruvate &lt;- ckd_data[,1:3] ckd_data_pyruvate ## # A tibble: 93 x 3 ## patient_number patient_status Pyruvate ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 20 healthy 14.2 ## 2 24 healthy 14.2 ## 3 29 healthy 13.9 ## 4 52 healthy 13.8 ## 5 54 healthy 13.7 ## 6 56 healthy 13.4 ## 7 57 healthy 13.6 ## 8 58 healthy 13.7 ## 9 59 healthy 14.5 ## 10 60 healthy 13.6 ## # … with 83 more rows Now, what is the number of healthy and sick patients in the dataset? table(ckd_data_pyruvate$patient_status) ## ## healthy kidney_disease ## 37 56 This means that the weights should be 37/(37+56) and 56/(37+56) Now, let’s assign those weights to each column: ckd_data_pyruvate &lt;- mutate( ckd_data_pyruvate, weight = case_when( patient_status == &quot;healthy&quot; ~ 56/(37+56), patient_status == &quot;kidney_disease&quot; ~ 37/(37+56) ) ) ckd_data_pyruvate ## # A tibble: 93 x 4 ## patient_number patient_status Pyruvate weight ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20 healthy 14.2 0.602 ## 2 24 healthy 14.2 0.602 ## 3 29 healthy 13.9 0.602 ## 4 52 healthy 13.8 0.602 ## 5 54 healthy 13.7 0.602 ## 6 56 healthy 13.4 0.602 ## 7 57 healthy 13.6 0.602 ## 8 58 healthy 13.7 0.602 ## 9 59 healthy 14.5 0.602 ## 10 60 healthy 13.6 0.602 ## # … with 83 more rows Now we can calculate the weighted mean: group_by(ckd_data_pyruvate, patient_status) %&gt;% summarize( weighted_mean_pyruvate = weighted.mean(Pyruvate, weight) ) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## patient_status weighted_mean_pyruvate ## &lt;chr&gt; &lt;dbl&gt; ## 1 healthy 13.9 ## 2 kidney_disease 13.8 B.6 t.test() algae_data_filtered &lt;- filter( algae_data, algae_strain == &quot;Tsv1&quot; &amp; chemical_species == &quot;omega_3_polyunsaturated_Fas&quot; ) t.test( algae_data_filtered$abundance ~ algae_data_filtered$harvesting_regime ) ## ## Welch Two Sample t-test ## ## data: algae_data_filtered$abundance by algae_data_filtered$harvesting_regime ## t = -4.1635, df = 2.5212, p-value = 0.03511 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -95.377472 -7.542528 ## sample estimates: ## mean in group Heavy mean in group Light ## 185.9367 237.3967 B.7 anova() algae_data_filtered &lt;- filter( algae_data, harvesting_regime == &quot;Heavy&quot; &amp; chemical_species == &quot;omega_3_polyunsaturated_Fas&quot; ) aov_results &lt;- aov( algae_data_filtered$abundance ~ algae_data_filtered$algae_strain ) summary(aov_results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## algae_data_filtered$algae_strain 2 4486 2243.0 7.077 0.0264 * ## Residuals 6 1902 316.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(aov_results) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = algae_data_filtered$abundance ~ algae_data_filtered$algae_strain) ## ## $`algae_data_filtered$algae_strain` ## diff lwr upr p adj ## Tsv11-Tsv1 -25.16333 -69.76378 19.43711 0.2696625 ## Tsv2-Tsv1 -54.63000 -99.23045 -10.02955 0.0219944 ## Tsv2-Tsv11 -29.46667 -74.06711 15.13378 0.1864469 "],["custom-functions.html", "C Custom functions C.1 Activating custom functions C.2 Basic runMatrixAnalysis() template C.3 Advanced runMatrixAnalysis() template", " C Custom functions C.1 Activating custom functions To activate in your session, run the following in your console: source(&quot;https://thebustalab.github.io/R_For_Chemists/custom_functions/chem.R&quot;) Once this is run the following will available in your R environment: algae_data alaska_lake_data solvents periodic_table periodic_table_small ckd_data wine_grape_data -note that this does not give access to NY_trees. readCSV runMatrixAnalysis If you want to look at any of the datasets in Excel, please consider writing the file to your hard drive using write_csv. Use ?write_csv to see how the command works. C.2 Basic runMatrixAnalysis() template Once you have activated runMatrixAnalysis in your R session (see above), you can use the function by filling out and executing the basic template below. There is also an advanced template further down the page. runMatrixAnalysis( data = NULL, analysis = c(&quot;hclust&quot;, &quot;pca&quot;, &quot;pca-ord&quot;, &quot;pca-dim&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = NULL, columns_w_sample_ID_info = NULL ) C.3 Advanced runMatrixAnalysis() template runMatrixAnalysis( data = NULL, # the data set to work on analysis = c(&quot;hclust&quot;, &quot;pca&quot;, &quot;pca-ord&quot;, &quot;pca-dim&quot;), # the analysis to conduct column_w_names_of_multiple_analytes = NULL, # a column with names of multiple analytes column_w_values_for_multiple_analytes = NULL, # a column with quantities measured for multiple analytes columns_w_values_for_single_analyte = NULL, # a column with quantities measured for a single analyte columns_w_additional_analyte_info = NULL, # a column with character or numeric information about analytes that was not &quot;measured&quot; as part of the experiment. columns_w_sample_ID_info = NULL, # a column with information about the sample (i.e. contents from the test tube&#39;s label) transpose = FALSE, kmeans = c(&quot;none&quot;, &quot;auto&quot;, &quot;elbow&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;etc.&quot;), na_replacement = c(&quot;none&quot;, &quot;mean&quot;, &quot;zero&quot;, &quot;drop&quot;) ) "],["faq.html", "D FAQ D.1 Filtering D.2 Order categorical axes", " D FAQ D.1 Filtering dplyr::filter(&lt;data&gt;, &lt;variable&gt; &lt; 18) ## less than 18 dplyr::filter(&lt;data&gt;, &lt;variable&gt; &lt;= 18) ## less than or equal to 18 dplyr::filter(&lt;data&gt;, &lt;variable&gt; &gt; 18) ## greater than 18 dplyr::filter(&lt;data&gt;, &lt;variable&gt; &gt;= 18) ## greater than or equal to 18 dplyr::filter(&lt;data&gt;, &lt;variable&gt; == 18) ## equals than 18 dplyr::filter(&lt;data&gt;, &lt;variable&gt; != 18) ## not equal to 18 dplyr::filter(&lt;data&gt;, &lt;variable&gt; == 18 | &lt;variable&gt; == 19) ## equal to 18 or 19 D.2 Order categorical axes Under normal plotting scenario: library(tidyverse) NY_trees &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/NY_trees.csv&quot;) NY_trees ## # A tibble: 378,762 x 14 ## tree_height tree_diameter address tree_loc pit_type soil_lvl status spc_latin ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 21.1 6 1139 5… Front Sidewal… Level Good PYRUS CA… ## 2 59.0 6 2220 B… Across Sidewal… Level Good PLATANUS… ## 3 92.4 13 2254 B… Across Sidewal… Level Good PLATANUS… ## 4 50.2 15 2332 B… Across Sidewal… Level Good PLATANUS… ## 5 95.0 21 2361 E… Front Sidewal… Level Poor PLATANUS… ## 6 67.5 19 2409 E… Front Continu… Level Good PLATANUS… ## 7 75.3 11 1481 E… Front Lawn Level Excel… PLATANUS… ## 8 27.9 7 1129 5… Front Sidewal… Level Good PYRUS CA… ## 9 111. 26 2076 E… Across Sidewal… Level Excel… PLATANUS… ## 10 83.9 20 2025 E… Front Sidewal… Level Excel… PLATANUS… ## # … with 378,752 more rows, and 6 more variables: spc_common &lt;chr&gt;, ## # trunk_dmg &lt;chr&gt;, zipcode &lt;dbl&gt;, boroname &lt;chr&gt;, latitude &lt;dbl&gt;, ## # longitude &lt;dbl&gt; tree_data_status &lt;- group_by(.data = NY_trees, status) tree_data_status ## # A tibble: 378,762 x 14 ## tree_height tree_diameter address tree_loc pit_type soil_lvl status spc_latin ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 21.1 6 1139 5… Front Sidewal… Level Good PYRUS CA… ## 2 59.0 6 2220 B… Across Sidewal… Level Good PLATANUS… ## 3 92.4 13 2254 B… Across Sidewal… Level Good PLATANUS… ## 4 50.2 15 2332 B… Across Sidewal… Level Good PLATANUS… ## 5 95.0 21 2361 E… Front Sidewal… Level Poor PLATANUS… ## 6 67.5 19 2409 E… Front Continu… Level Good PLATANUS… ## 7 75.3 11 1481 E… Front Lawn Level Excel… PLATANUS… ## 8 27.9 7 1129 5… Front Sidewal… Level Good PYRUS CA… ## 9 111. 26 2076 E… Across Sidewal… Level Excel… PLATANUS… ## 10 83.9 20 2025 E… Front Sidewal… Level Excel… PLATANUS… ## # … with 378,752 more rows, and 6 more variables: spc_common &lt;chr&gt;, ## # trunk_dmg &lt;chr&gt;, zipcode &lt;dbl&gt;, boroname &lt;chr&gt;, latitude &lt;dbl&gt;, ## # longitude &lt;dbl&gt; tree_data_status_summary &lt;- summarize(.data = tree_data_status, mean_height = mean(tree_height), stdev_height = sd(tree_height)) status_color &lt;- c(&quot;red&quot;,&quot;darkorange&quot;,&quot;gold&quot;,&quot;darkgreen&quot;) names(status_color) &lt;- c(&quot;Dead&quot;, &quot;Poor&quot;, &quot;Good&quot;, &quot;Excellent&quot;) ggplot(data = tree_data_status_summary) + geom_pointrange(aes(x = status, y = mean_height, ymin = mean_height -stdev_height, ymax = mean_height + stdev_height), color = &quot;navy&quot;) + geom_point(data = filter(NY_trees, tree_height &gt; 150), aes(x = status, y = tree_height, fill = status), stroke = 1.5, shape = 21, size = 5) + theme_bw() + scale_fill_manual(values = status_color) But what if we want the order on the x-axis to be from worst status to best? We need to make the status column into factors (a list of characters that has an order other than alphabetical). Here’s how: tree_data_status_summary$status &lt;- factor(tree_data_status_summary$status, levels = c(&quot;Dead&quot;, &quot;Poor&quot;, &quot;Good&quot;, &quot;Excellent&quot;)) # note that we specify the order we want right here in &quot;levels&quot;... ggplot(data = tree_data_status_summary) + geom_pointrange(aes(x = status, y = mean_height, ymin = mean_height -stdev_height, ymax = mean_height + stdev_height), color = &quot;navy&quot;) + geom_point(data = filter(NY_trees, tree_height &gt; 150), aes(x = status, y = tree_height, fill = status), stroke = 1.5, shape = 21, size = 5) + theme_bw() + scale_fill_manual(values = status_color) "],["pca-and-big-data.html", "E PCA and Big Data", " E PCA and Big Data In this course it is possible that you will want to run a clustering or PCA analysis on a data set with hundreds of thousands of observations (like NY_trees, for example). This is not always straightforward, since looking at a dendrogram or a scatter plot with hundreds of thousands of points is not always fruitful. Consider the meteorological dataset below. mdata &lt;- read_csv(&quot;https://thebustalab.github.io/R_For_Chemists/sample_data/meteorological_data.csv&quot;) dim(mdata) ## [1] 234528 15 Nearly a quarter million observations! However, that does not mean that you cannot use those analyses on such a dataset. Consider just running the PCA or cluster analysis on a summary of the dataset. You can even create your own groups by which to summarize directly from large continuous variables in your dataset. In the example below, a new categorical variable WIND_GROUP is created by binning the observations according to WINDSPEED into 20 groups. This is accomplished using the cut() command. mdata &lt;- mdata[!is.na(mdata$WINDSPEED),] mdata$WIND_GROUP &lt;- as.numeric(cut(mdata$WINDSPEED, breaks = 20)) Cool! Now we can group_by() and summarize() on WIND_GROUP: mdata_windgroup &lt;- group_by(mdata, WIND_GROUP) %&gt;% summarize( TEMPERATURE = mean(TEMPERATURE, na.rm = TRUE), TEMPERATURE_DELTA = mean(TEMPERATURE_DELTA, na.rm = TRUE), RELATIVE_HUMIDITY = mean(RELATIVE_HUMIDITY, na.rm = TRUE), SOLAR_RADIATION = mean(SOLAR_RADIATION, na.rm = TRUE), OZONE = mean(OZONE, na.rm = TRUE), PRECIPITATION = mean(PRECIPITATION, na.rm = TRUE), WINDSPEED = mean(WINDSPEED, na.rm = TRUE), SHELTER_TEMPERATURE = mean(SHELTER_TEMPERATURE, na.rm = TRUE), WIND_DIRECTION = mean(WIND_DIRECTION, na.rm = TRUE), WINDSPEED_SCALAR = mean(WINDSPEED_SCALAR, na.rm = TRUE), FLOW_RATE = mean(FLOW_RATE, na.rm = TRUE), WETNESS = mean(WETNESS, na.rm = TRUE), ) ## `summarise()` ungrouping output (override with `.groups` argument) mdata_windgroup ## # A tibble: 20 x 13 ## WIND_GROUP TEMPERATURE TEMPERATURE_DEL… RELATIVE_HUMIDI… SOLAR_RADIATION ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 6.05 0.251 75.6 138. ## 2 2 4.72 0.124 75.2 143. ## 3 3 4.35 0.146 74.1 131. ## 4 4 4.09 0.175 74.5 118. ## 5 5 3.22 0.103 74.0 113. ## 6 6 3.28 0.0127 73.1 120. ## 7 7 4.10 -0.0685 72.0 138. ## 8 8 4.76 -0.137 70.5 155. ## 9 9 5.16 -0.180 69.0 167. ## 10 10 5.71 -0.199 67.0 184. ## 11 11 6.15 -0.222 64.9 199. ## 12 12 6.32 -0.176 65.9 184. ## 13 13 6.07 -0.176 65.6 181. ## 14 14 7.63 -0.168 66.2 169. ## 15 15 8.57 -0.132 67.6 176. ## 16 16 6.14 -0.193 66.8 218. ## 17 17 12.7 -0.3 51.1 420. ## 18 18 8.23 NaN 47.3 520. ## 19 19 -1.9 0 70 3 ## 20 20 -5.9 NaN 60 249 ## # … with 8 more variables: OZONE &lt;dbl&gt;, PRECIPITATION &lt;dbl&gt;, WINDSPEED &lt;dbl&gt;, ## # SHELTER_TEMPERATURE &lt;dbl&gt;, WIND_DIRECTION &lt;dbl&gt;, WINDSPEED_SCALAR &lt;dbl&gt;, ## # FLOW_RATE &lt;dbl&gt;, WETNESS &lt;dbl&gt; With that done, we can run matrix analyses on the summarized data, using our new WIND_GROUP variable as the sample ID, and plot the results! mdata_windgroup_analyzed &lt;- runMatrixAnalysis( data = mdata_windgroup, analysis = c(&quot;pca&quot;), column_w_names_of_multiple_analytes = NULL, column_w_values_for_multiple_analytes = NULL, columns_w_values_for_single_analyte = c(&quot;TEMPERATURE&quot;, &quot;TEMPERATURE_DELTA&quot;, &quot;RELATIVE_HUMIDITY&quot;, &quot;SOLAR_RADIATION&quot;, &quot;OZONE&quot;, &quot;PRECIPITATION&quot;, &quot;WINDSPEED&quot;, &quot;SHELTER_TEMPERATURE&quot;, &quot;WIND_DIRECTION&quot;, &quot;WINDSPEED_SCALAR&quot;, &quot;FLOW_RATE&quot;, &quot;WETNESS&quot;), columns_w_additional_analyte_info = NULL, columns_w_sample_ID_info = c(&quot;WIND_GROUP&quot;) ) ## Not replacing any NAs in your data set ggplot(mdata_windgroup_analyzed) + geom_point(aes(x = Dim.1, y = Dim.2, fill = WINDSPEED, size = TEMPERATURE), shape = 21) + theme_classic() "]]
